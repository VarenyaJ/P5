{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Output Evaluation Notebook\n",
    "\n",
    "This notebook runs LLM inference to predict HPO terms, compares them to ground truth phenopackets, and produces a summary report.\n"
   ],
   "id": "187dea139446c775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 0. Imports, Path Discovery & Sanity Checks\n",
    "\n",
    "Load all dependencies, discover the dataset CSV automatically, and validate critical directories.\n"
   ],
   "id": "298b82d55c7dad0e"
  },
  {
   "cell_type": "code",
   "id": "98ccd760-19a5-48d0-b2c4-64063ecf29e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:12:58.602346Z",
     "start_time": "2025-07-21T17:12:19.471725Z"
    }
   },
   "source": [
    "# Basic Setup\n",
    "import sys, os, glob, json, subprocess, pickle, datetime, hashlib, warnings, random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from ollama import chat\n",
    "from docling.document_converter import DocumentConverter, ConversionError\n",
    "from pypdfium2._helpers.misc import PdfiumError\n",
    "from google.protobuf.json_format import ParseDict, ParseError\n",
    "from phenopackets import Phenopacket as ProtoPhenopacket\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# Need this at least once for some reason:\n",
    "# import .autonotebook\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "# Make sure our utils folder is on PYTHONPATH\n",
    "project_root        = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder          = os.path.join(project_root, \"src\")\n",
    "utils_folder        = os.path.join(project_root, \"notebooks\", \"utils\")\n",
    "\n",
    "print(\"Project Start:       %s\" % project_root)\n",
    "print(\"Source Folder:       %s\" % src_folder)\n",
    "print(\"Utilities Folder:    %s\" % utils_folder)\n",
    "\n",
    "for path in (src_folder, utils_folder):\n",
    "    if not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"Expected folder on PYTHOPATH : {path}\")\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "print(\"PYTHONPATH patched with:\", src_folder, utils_folder)\n",
    "\n",
    "try:\n",
    "    from phenopacket import Phenopacket\n",
    "    from report import Report\n",
    "    from evaluation import PhenotypeEvaluator\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Could not import project utils: {e}\")\n",
    "\n",
    "# define all key paths\n",
    "pdf_input_directory                 = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"pmid_pdfs\")            # scripts/data/tmp/phenopacket_store/pmid_pdfs/\n",
    "ground_truth_notebooks_directory    = os.path.join(src_folder, \"P5\", \"scripts\", \"data\",\"tmp\", \"phenopacket_store\",\"notebooks\")              # scripts/data/tmp/phenopacket_store/notebooks/\n",
    "dataset_csv_path                    = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\")\n",
    "\n",
    "# All experimental outputs go under here\n",
    "experimental_data_root              = os.path.join(project_root, \"experimental-data\")\n",
    "llm_output_directory                = os.path.join(experimental_data_root, \"llm_output_dir\")                                                # intermediate .txt + raw JSON from LLM\n",
    "validated_jsons_directory           = os.path.join(experimental_data_root, \"validated_jsons\")                                               # validated_jsons, the final validated LLM phenopackets\n",
    "evaluation_report_output_path       = os.path.join(project_root, \"reports\", \"first_report.json\")                                            # the evaluation metrics report\n",
    "\n",
    "# Create any missing output folders\n",
    "os.makedirs(pdf_input_directory, exist_ok=True)\n",
    "os.makedirs(ground_truth_notebooks_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(dataset_csv_path), exist_ok=True)\n",
    "os.makedirs(llm_output_directory, exist_ok=True)\n",
    "os.makedirs(validated_jsons_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(evaluation_report_output_path), exist_ok=True)\n",
    "\n",
    "# Create the PMIDs pickle file path\n",
    "pmid_pkl_path = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"pmids.pkl\")\n",
    "\n",
    "# TODO: Figure out why deleting the `ground_truth_notebooks_directory` after creating it works. Maybe because git doesn't let me just overwrite a directory with a clone request\n",
    "# Before the git pull operation\n",
    "import shutil\n",
    "\n",
    "# Clean up existing directory if it exists\n",
    "target_dir = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\")\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "\n",
    "# 1. Now run the git pull to clone the \"phenopacket-store\" GitHub repo into scripts/data/tmp/phenopacket_store\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.pull_git_files\",\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\"),\n",
    "    \"https://github.com/monarch-initiative/phenopacket-store.git\",\n",
    "    \"notebooks\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 1 Complete, Produced %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "# 2. Scan the just-pulled notebooks for PMID_##### files\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.create_pmid_pkl\",\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\"),\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"pmids.pkl\"),\n",
    "    \"--recursive_dir_search\",\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 2 Complete\")\n",
    "\n",
    "# 3. Download *all* PDFs for those PMIDs (0 = unlimited)\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.pmid_downloader\", pmid_pkl_path, pdf_input_directory, \"10\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 3 Complete\")\n",
    "\n",
    "# 4. Finally, build THE CSV mapping PDFs to the ground-truth JSONs\n",
    "if not os.path.isfile(dataset_csv_path):\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"P5.scripts.create_phenopacket_dataset\",\n",
    "        pdf_input_directory,\n",
    "        ground_truth_notebooks_directory,\n",
    "        dataset_csv_path,\n",
    "        \"--recursive_ground_truth_dir\", \"True\"\n",
    "    ], check=True)\n",
    "    print(f\"Created dataset CSV at {dataset_csv_path}\")\n",
    "\n",
    "    print(\"Stage 4 Complete\")\n",
    "\n",
    "    if not os.path.isdir(pdf_input_directory):\n",
    "        raise FileNotFoundError(\"PDF input directory not found: %s\" % pdf_input_directory)\n",
    "    if not os.path.isdir(ground_truth_notebooks_directory):\n",
    "        raise FileNotFoundError(\"Ground truth notebooks directory not found: %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "\n",
    "print(\"hello0\")  # print hello 0 as a sanity check"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Start:       /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5\n",
      "Source Folder:       /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src\n",
      "Utilities Folder:    /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/notebooks/utils\n",
      "PYTHONPATH patched with: /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/notebooks/utils\n",
      "Stage 1 Complete, Produced /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "1234 PMIDs found within /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Stage 2 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_22305531:   0%|                         | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_22305531 was successfully downloaded. PMCID=3276655.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_19949040:  10%|█▋               | 1/10 [00:08<01:14,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PMCID found for PMID_19949040.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_35638551:  20%|███▍             | 2/10 [00:08<00:29,  3.63s/it]An error occurred when downloading PMID_35638551 = PMC_9973481: No connection adapters were found for 'data:,'\n",
      "Processing PMID_24998929:  30%|█████            | 3/10 [00:14<00:32,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_24998929 was successfully downloaded. PMCID=4136921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_10534505:  40%|██████▊          | 4/10 [00:20<00:31,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PMCID found for PMID_10534505.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_25511235:  50%|████████▌        | 5/10 [00:20<00:17,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PMCID found for PMID_25511235.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_26426912:  60%|██████████▏      | 6/10 [00:21<00:09,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PMCID found for PMID_26426912.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_23665959:  70%|███████████▉     | 7/10 [00:21<00:05,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_23665959 was successfully downloaded. PMCID=3706629.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_26763879:  80%|█████████████▌   | 8/10 [00:28<00:06,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PMCID found for PMID_26763879.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_29283439:  90%|███████████████▎ | 9/10 [00:28<00:02,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_29283439 was successfully downloaded. PMCID=5876123.\n",
      "Stage 3 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing of 10 PMIDs complete. 4 PDFs successfully downloaded.: 100%|█| 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset CSV at /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Stage 4 Complete\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n",
      "hello0\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1. Load Dataset\n",
    "\n",
    "Read the CSV of PMIDs, input paths, and truth paths\n"
   ],
   "id": "dabf5f01d8e6a670"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:20:30.813703Z",
     "start_time": "2025-07-21T17:20:30.787015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "dataframe_cases = pd.read_csv(dataset_csv_path)\n",
    "print(f\"Loaded {len(dataframe_cases)} rows from dataset CSV\")\n",
    "# Load cases & deduplicate PMIDs, with start/end counts\n",
    "orig_count = len(dataframe_cases)\n",
    "print(f\"Before deduplication: {orig_count} total cases\")\n",
    "\n",
    "# Debug: verify that every `input` path actually exists\n",
    "print(\"Checking existence of input PDFs:\")\n",
    "for pdf_path in dataframe_cases[\"input\"]:\n",
    "    status = \"FOUND\" if os.path.isfile(pdf_path) else \"MISSING\"\n",
    "    print(f\"  o {pdf_path}: {status}\")\n",
    "\n",
    "# Drop duplicate PMIDs\n",
    "dataframe_cases = dataframe_cases.drop_duplicates(subset=\"pmid\", keep=\"first\").reset_index(drop=True) # This may be too aggressive and I need to check if this is a good approach\n",
    "removed = orig_count - len(dataframe_cases)\n",
    "print(f\"{removed} duplicates removed (now {len(dataframe_cases)} unique PMIDs)\")\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = {\"pmid\", \"input\", \"truth\"}\n",
    "missing_columns = required_columns - set(dataframe_cases.columns)\n",
    "if missing_columns:\n",
    "    raise KeyError(\"Missing required columns: %s\" % missing_columns)\n",
    "\n",
    "# Preview first few rows\n",
    "dataframe_cases.head()\n",
    "\n",
    "print(\"hello1\")  # print hello 1 as a sanity check\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)"
   ],
   "id": "598b33092ab85352",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27 rows from dataset CSV\n",
      "Before deduplication: 27 total cases\n",
      "Checking existence of input PDFs:\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_22305531.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_22305531.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_29283439.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_29283439.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_29283439.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_29283439.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_29283439.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_29283439.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_29283439.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_29283439.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_23665959.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_23665959.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24998929.pdf: FOUND\n",
      "23 duplicates removed (now 4 unique PMIDs)\n",
      "hello1\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2. Discover Phenopacket-Store Files\n",
    "\n",
    "Locate all ground-truth Phenopacket JSON files under the `phenopacket_store/notebooks/` directory."
   ],
   "id": "2a1755b2eb294368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:32:07.607487Z",
     "start_time": "2025-07-21T17:32:07.458056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cache Integrity & Versioning\n",
    "CACHE_DIR = Path(experimental_data_root) / \"text_cache\"\n",
    "INDEX_FILE = CACHE_DIR / \"cache_index.json\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _hash_pdf(pdf_path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    h.update(pdf_path.read_bytes())\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_or_update_cache(pmids: list[str], pmid_to_pdf: dict[str, Path]):\n",
    "    if INDEX_FILE.exists():\n",
    "        index = json.loads(INDEX_FILE.read_text())\n",
    "    else:\n",
    "        index = {}\n",
    "    for pmid in pmids:\n",
    "        pdf = pmid_to_pdf[pmid]\n",
    "        digest = _hash_pdf(pdf)\n",
    "        if pmid not in index or index[pmid][\"hash\"] != digest:\n",
    "            text = extract_text_from_pdf(pdf)\n",
    "            with open(CACHE_DIR / f\"{pmid}.pkl\", \"wb\") as fh:\n",
    "                pickle.dump(text, fh)\n",
    "            index[pmid] = {\n",
    "                \"hash\": digest,\n",
    "                \"cached_at\": datetime.datetime.utcnow().isoformat()\n",
    "            }\n",
    "    INDEX_FILE.write_text(json.dumps(index, indent=2))\n",
    "\n",
    "\n",
    "# Finding all ground-truth phenopacket JSON files\n",
    "search_pattern = os.path.join(ground_truth_notebooks_directory, \"*\", \"phenopackets\", \"*.json\")\n",
    "truth_json_filepaths = glob.glob(search_pattern, recursive=True)\n",
    "if not truth_json_filepaths:\n",
    "    raise FileNotFoundError(f\"No ground-truth JSONs found at {search_pattern}\")\n",
    "\n",
    "print(\"Discovered %d ground-truth JSON files\" %len(truth_json_filepaths))\n",
    "\n",
    "print(\"hello2\")  # print hello 2 as a sanity check\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)"
   ],
   "id": "8118b55fc2741c2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 7969 ground-truth JSON files\n",
      "hello2\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3. Prepare PDF-to-Text Converter\n",
    "\n",
    "- Randomly pick N unique PMIDs from the CSV (from Step 1) to keep runs fast and reproducible.\n",
    "- Instantiate DocumentConverter and define a helper function to load or convert the clinical PDFs for LLM input.\n",
    "- Setup Persistent PDF-to-Text Cache"
   ],
   "id": "9a9faff43b491b26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:37:03.491929Z",
     "start_time": "2025-07-21T17:35:54.919503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload the deduplicated CSV from Step 1\n",
    "full_df = pd.read_csv(dataset_csv_path).drop_duplicates(subset=\"pmid\").reset_index(drop=True)\n",
    "\n",
    "# Choose how many cases to sample\n",
    "# N = 10\n",
    "# Don’t ask for more than exist\n",
    "N = min(N, len(full_df))\n",
    "subset_df = full_df.sample(n=N, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Sampling {N} PMIDs:\", subset_df[\"pmid\"].tolist())\n",
    "\n",
    "\n",
    "# Setup conversion for input material to LLM-compatible txt now\n",
    "\n",
    "# Initialize converter once\n",
    "pdf_to_text_converter = DocumentConverter()\n",
    "\n",
    "# Path to persistent cache of PDF text\n",
    "text_cache_path = os.path.join(experimental_data_root, \"text_cache.pkl\")\n",
    "# Load or initialize cache\n",
    "if os.path.exists(text_cache_path):\n",
    "    with open(text_cache_path, \"rb\") as f:\n",
    "        text_cache = pickle.load(f)\n",
    "    print(f\"Loaded text cache with {len(text_cache)} entries\")\n",
    "else:\n",
    "    text_cache = {}\n",
    "    print(\"Initialized empty text cache\")\n",
    "\n",
    "def load_clinical_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert .txt or .pdf file at \"pdf_path\" into a plain text string.\n",
    "    Use the in-memory cache first; write new text back to the cache only when the cache is explicitly saved at the end of the pipeline.\n",
    "    Raise FileNotFoundError or ConversionError if the file does not exist.\n",
    "    \"\"\"\n",
    "    # Return cache if it exists in memory\n",
    "    if pdf_path in text_cache:\n",
    "        return text_cache[pdf_path]\n",
    "\n",
    "    # Ensure the files exists before we continue\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        raise FileNotFoundError(\"Input file not found: %s\" % pdf_path)\n",
    "\n",
    "    # If it's already plain text, read and strip any header\n",
    "    if pdf_path.lower().endswith(\".txt\"):\n",
    "        content = open(pdf_path, encoding=\"utf-8\").read()\n",
    "        # Remove any leading markers\n",
    "        return content.split(\"[text]\")[-1]\n",
    "    else:\n",
    "        try:\n",
    "            # Convert PDF to text and handle conversion failures\n",
    "            doc = pdf_to_text_converter.convert(pdf_path)\n",
    "            content = doc.document.export_to_text()\n",
    "        except ConversionError as e:\n",
    "            raise ConversionError(f\"Could not convert {os.path.basename(pdf_path)}: {e}\")\n",
    "\n",
    "\n",
    "    # Save new text in memory and write updated cache to disk later\n",
    "    text_cache[pdf_path] = content\n",
    "    return content\n",
    "\n",
    "# Convert all PDFs in our sampled subset\n",
    "for pdf_path in subset_df[\"input\"]:\n",
    "    pdf_text = load_clinical_pdf(pdf_path)\n",
    "# Persist updated cache to disk\n",
    "with open(text_cache_path, \"wb\") as f:\n",
    "    pickle.dump(text_cache, f)\n",
    "\n",
    "print(f\"Saved text cache now with {len(text_cache)} entries\")\n",
    "\n",
    "print(\"hello3\")  # print hello 3 as a sanity check\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)"
   ],
   "id": "231c1c375a6ec410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 4 PMIDs: ['PMID_29283439', 'PMID_24998929', 'PMID_22305531', 'PMID_23665959']\n",
      "Initialized empty text cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved text cache now with 4 entries\n",
      "hello3\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Step 4. Load Clinical PDFs and Ground-Truth Phenopackets\n",
    "\n",
    "Iterate over each case, load the clinical PDF text and the corresponding ground-truth Phenopacket object.\n",
    "\n",
    "- `list_inputs_texts`: raw clinical PDFs\n",
    "- `list_truth_packets`: parsed Phenopacket objects from JSON files\n",
    "- `list_patient_ids`: PMID patient identifiers\n",
    "\n"
   ],
   "id": "cbc0b863-bf20-4a46-95de-9506e9875678"
  },
  {
   "cell_type": "code",
   "id": "235eaff7-4f21-4a21-81bf-e97236ca26d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:43:37.647338Z",
     "start_time": "2025-07-21T17:43:37.624695Z"
    }
   },
   "source": [
    "# Iterate over rows, should lookup only once\n",
    "list_input_texts    = []\n",
    "list_truth_packets  = []\n",
    "list_patient_ids    = []\n",
    "\n",
    "\n",
    "for case in dataframe_cases.itertuples(index=False):\n",
    "    pmid_value = case.pmid\n",
    "    pdf_path   = case.input\n",
    "    truth_path = case.truth\n",
    "\n",
    "    # Convert PDF to text\n",
    "    try:\n",
    "        clinical_text = load_clinical_pdf(pdf_path)\n",
    "    except (ConversionError, PdfiumError) as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"pdf\": pdf_path, \"reason\": f\"conversion error: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Load raw JSON and validate with ignore_unknown_fields\n",
    "    try:\n",
    "        raw_true_packet = json.load(open(truth_path, \"r\", encoding=\"utf-8\"))\n",
    "        proto = ProtoPhenopacket()\n",
    "        ParseDict(raw_true_packet, proto, ignore_unknown_fields=True)\n",
    "    except (ParseError, json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"truth\": truth_path, \"reason\": f\"schema parse error: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Wrap in util Phenopacket to ensure phenotypicFeatures exists\n",
    "    try:\n",
    "        truth_packet = Phenopacket(raw_true_packet)\n",
    "    except InvalidPhenopacketError as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"truth\": truth_path, \"reason\": f\"phenopacket invalid: {e}\"})\n",
    "        continue\n",
    "\n",
    "    list_input_texts.append(clinical_text)\n",
    "    list_truth_packets.append(truth_packet)\n",
    "    list_patient_ids.append(truth_packet.to_json()[\"subject\"][\"id\"])\n",
    "\n",
    "    loaded_count += 1\n",
    "    print(f\"Loaded {loaded_count} new cases, skipped {len(skipped_pdfs)} so far\")\n",
    "\n",
    "if not list_input_texts:\n",
    "        raise RuntimeError(\"No clinical texts were loaded, please check that the dataset CSV `input` paths match files in `pdf_input_directory`\")\n",
    "\n",
    "assert len(list_input_texts) == len(list_truth_packets) == len(list_patient_ids)\n",
    "print(\"Loaded %d clinical texts and %d ground-truth packets for %d unique patients\" % (len(list_input_texts), len(list_truth_packets), len(list_patient_ids)))\n",
    "\n",
    "print(\"hello4\")  # print hello 4 as a sanity check\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 new cases, skipped 0 so far\n",
      "Loaded 7 new cases, skipped 0 so far\n",
      "Loaded 8 new cases, skipped 0 so far\n",
      "Loaded 9 new cases, skipped 0 so far\n",
      "Loaded 4 clinical texts and 4 ground-truth packets for 4 unique patients\n",
      "hello4\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4.5. Define LLM Prompts\n",
    "\n",
    "Create one prompt for just HPO terms and another one for the full phenopacket extraction\n"
   ],
   "id": "a5f9ab5a0c5a064d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:43:52.276781Z",
     "start_time": "2025-07-21T17:43:52.272096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Prompt for just HPO labels\n",
    "hpo_prompt = (\n",
    "    \"You are a clinical NLP engine specialized in biomedical ontologies. Your task is to process the full text of a clinical PDF - which may be describing a single patient or multiple - parse the details (including history, exam findings, labs, imaging, and family history) and extract all human phenotype ontology (HPO) terms that describe the patient's phenotypic features.\"\n",
    "    \"Instructions:\"\n",
    "    \"1. Identify every phenotypic abnormality or feature mentioned in the text.\"\n",
    "    \"2. For each feature, map it to the correct HPO identifier (e.g. 'HP:0001250'), label (e.g. 'Seizure'), and descriptor value (e.g. 'A seizure is an intermittent abnormality of nervous system physiology characterized by a transient occurrence of signs and/or symptoms due to abnormal excessive or synchronous neuronal activity in the brain.').\"\n",
    "    \"3. Capture relevant qualifiers when present:\"\n",
    "        \"- Onset: map to HPO onset terms (e.g. 'HP:0011463' for 'Childhood onset').\"\n",
    "        \"- Severity: map to HPO severity terms (e.g. 'HP:0012829' for 'Profound').\"\n",
    "        \"- Temporal pattern: include if specified (e.g. 'HP:0031796' for 'Recurrent', map to HPO frequency terms if available).\"\n",
    "    \"4. For each term, include the exact text excerpt where it appears.\"\n",
    "    \"5. Output exclusively a JSON array. Each element must be an object with the following fields:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'hpo_id': 'HP:000____',\"\n",
    "        \"'hpo_label': 'Term label',\"\n",
    "        \"'excerpt': 'Exact text from the PDF',\"\n",
    "        \"'onset_id': 'HP:0XXXXX or null',\"\n",
    "        \"'severity_id': 'HP:0XXXXX or null',\"\n",
    "        \"'frequency_id': 'HP:0XXXXX or null'\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"Do not include any explanatory text, only the JSON array.\"\n",
    "\n",
    "    \"Your output **MUST** be exactly a JSON array/object and nothing else.\"\n",
    "\n",
    "    \"If you cannot comply, output exactly: {'error': 'cannot extract JSON'}\"\n",
    "    )\n",
    "\n",
    "# 2) Prompt for full phenopacket\n",
    "full_pp_prompt = (\n",
    "    \"You are a biomedical data curation assistant. Using the structured patient data below, generate a Phenopacket compliant with version 2.0 of the GA4GH Phenopacket schema. Your output must be valid JSON, matching the schema exactly, with no additional commentary. Here are the minimum expected output criteria:\"\n",
    "\n",
    "    \"Inputs:\"\n",
    "    \"patient_id: '{{patient_id}}'\"\n",
    "    \"sex: '{{sex}}'              // 'male' or 'female'\"\n",
    "    \"age_years: {{age_in_years}} // integer\"\n",
    "    \"v  ital_status: '{{vital_status}}' // 'alive' or 'deceased'\"\n",
    "    \"phenotypic_features: {{phenotypic_features_json}} // JSON array from the HPO extraction prompt\"\n",
    "    \"diseases: {{diseases_json}}         // optional, array of disease objects with MONDO or OMIM IDs\"\n",
    "    \"measurements: {{measurements_json}} // optional, array of quantitative trait measurements\"\n",
    "    \"metadata: {\"\n",
    "        \"'created_by': '{{your_name_or_tool}}',\"\n",
    "        \"'created_on': '{{YYYY-MM-DD}}'\"\n",
    "    \"}\"\n",
    "\n",
    "    \"Requirements:\"\n",
    "    \"Top-level fields:\"\n",
    "    \"'id': patient_id\"\n",
    "    \"'subject': object with:\"\n",
    "        \"'id': patient_id\"\n",
    "        \"'sex': { 'id': 'PATO:0000383' or 'PATO:0000384', 'label': sex }\"\n",
    "        \"'ageAtLastEncounter': { 'age': { 'years': age_years } }\"\n",
    "        \"'vitalStatus': { 'value': vital_status }\"\n",
    "        \"'phenotypicFeatures': use the phenotypic_features input; for each feature, map:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'type': { 'id': hpo_id, 'label': hpo_label },\"\n",
    "        \"'negated': false,\"\n",
    "        \"'onset': { 'term': { 'id': onset_id, 'label': (look up label) } },\"\n",
    "        \"'severity': { 'term': { 'id': severity_id, 'label': (look up label) } },\"\n",
    "        \"'frequency': { 'term': { 'id': frequency_id, 'label': (look up label) } }\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"Include 'diseases' and 'measurements' only if provided, following the GA4GH schema.\"\n",
    "    \"'metadata' must include:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'phenopacketSchemaVersion': '2.0.0',\"\n",
    "        \"'created': '{{YYYY-MM-DD}}',\"\n",
    "        \"'createdBy': '{{your_name_or_tool}}'\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"'Do not add any extra fields. Output must be purely the JSON object.'\"\n",
    "\n",
    "    \"Do not include any explanatory text, only the JSON array.\"\n",
    "\n",
    "    \"Your output **MUST** be exactly a JSON array/object and nothing else.\"\n",
    "\n",
    "    \"If you cannot comply, output exactly: {'error': 'cannot extract JSON'}\"\n",
    ")"
   ],
   "id": "6f5d51a89657db88",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5. Sanity-check one inference\n",
    "\n",
    "Run one LLM call on the first case to verify prompting and parsing work correctly.\n"
   ],
   "id": "8f03843785823cdc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:57:26.631126Z",
     "start_time": "2025-07-21T17:57:17.130301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pick out the first patient/example\n",
    "patient_id      = list_patient_ids[0]\n",
    "clinical_text   = list_input_texts[0]\n",
    "# truth_packet    = list_truth_packets[0]\n",
    "\n",
    "# 1) Inference: ask for *only* the JSON array of HPO term objects for the first clinical PDF\n",
    "# build a strict system+user conversation\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            hpo_prompt\n",
    "            + \"\\n\\nYour only output must be a **valid** JSON array of  HPO term objects\"\n",
    "            + \"with fields 'hpo_id','hpo_label','excerpt',\"\n",
    "            + \"'onset_id','severity_id','frequency_id', and nothing else.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": clinical_text\n",
    "    }\n",
    "]\n",
    "\n",
    "hpo_response = chat(\n",
    "    model=\"llama3.2:latest\",\n",
    "    messages=messages,\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n",
    "\n",
    "# 2) Grab the raw string\n",
    "raw_hpo_output = hpo_response[\"message\"][\"content\"]\n",
    "print(\"Raw LLM output (truncated to the first ~300 chars or so):\")\n",
    "print(raw_hpo_output[:300], \"...\\n\")\n",
    "\n",
    "# 3) Slice out the JSON array\n",
    "start = raw_hpo_output.find(\"[\")\n",
    "end = raw_hpo_output.rfind(\"]\")\n",
    "if start < 0 or end < 0:\n",
    "    # print the raw output to debug what the model actually sent\n",
    "    print(\"===== RAW HPO OUTPUT =====\\n\", raw_hpo_output)\n",
    "    raise RuntimeError(f\"Could not locate a JSON array in HPO output for patient {patient_id}:\\n{raw_hpo_output}\")\n",
    "# grab *only* the array text\n",
    "hpo_json_array = raw_hpo_output[start : end+1]\n",
    "\n",
    "# Parse and validate\n",
    "try:\n",
    "    hpo_terms = Phenopacket(json.loads(hpo_json_array))\n",
    "    print(f\"Parsed {len(hpo_terms)} HPO term(s) for patient {patient_id}\")\n",
    "except JSONDecodeError as error:\n",
    "    raise ValueError(\n",
    "        f\"Failed to parse HPO JSON array for patient {patient_id}: {error}\\n\\n\"\n",
    "        f\"Extracted JSON was:\\n{hpo_json_array}\\n\\n\"\n",
    "        f\"Full raw output was:\\n{raw_hpo_output}\"\n",
    "    )\n",
    "\n",
    "print(\"hello5\")  # print hello 5 as a sanity check"
   ],
   "id": "1f547b47f98fc0b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLM output (truncated to the first ~300 chars or so):\n",
      "This is a review article discussing paroxysmal nocturnal hemoglobinuria (PNH), a rare blood disorder caused by mutations in the PIGA gene, which is involved in the synthesis of glycosylphosphatidylinositol (GPI) anchors on proteins.\n",
      "\n",
      "The authors discuss various aspects of PNH, including its pathophy ...\n",
      "\n",
      "===== RAW HPO OUTPUT =====\n",
      " This is a review article discussing paroxysmal nocturnal hemoglobinuria (PNH), a rare blood disorder caused by mutations in the PIGA gene, which is involved in the synthesis of glycosylphosphatidylinositol (GPI) anchors on proteins.\n",
      "\n",
      "The authors discuss various aspects of PNH, including its pathophysiology, clinical presentation, and diagnosis. They also review the genetic basis of PNH, highlighting the importance of the PIGA gene in GPI anchor synthesis and the consequences of mutations in this gene on protein anchoring and cell surface protein function.\n",
      "\n",
      "Some key points from the article include:\n",
      "\n",
      "* PNH is caused by a deficiency in GPI-anchored proteins due to mutations in the PIGA gene.\n",
      "* The PIGA gene plays a critical role in the synthesis of GPI anchors, which are essential for attaching proteins to the cell membrane.\n",
      "* Mutations in the PIGA gene can lead to a range of downstream effects, including impaired protein anchoring and altered cell surface protein function.\n",
      "* The authors discuss various clinical manifestations of PNH, including hemolytic anemia, thrombosis, and autoimmune disorders.\n",
      "* They also review diagnostic approaches for PNH, highlighting the importance of genetic testing in identifying individuals with this disorder.\n",
      "\n",
      "The article concludes by discussing therapeutic options for PNH, including eculizumab (a complement inhibitor) and ravulizumab (another complement inhibitor).\n",
      "\n",
      "Some potential future directions for research mentioned in the article include:\n",
      "\n",
      "* Investigating the role of other GPI anchor enzymes in PNH\n",
      "* Developing new diagnostic tests for PNH\n",
      "* Exploring alternative treatments for PNH, such as gene therapy or immunotherapy\n",
      "\n",
      "Overall, this review article provides a comprehensive overview of paroxysmal nocturnal hemoglobinuria, its pathophysiology, and its clinical presentation. It also highlights the importance of genetic testing in diagnosing and managing this disorder.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not locate a JSON array in HPO output for patient IV-4:\nThis is a review article discussing paroxysmal nocturnal hemoglobinuria (PNH), a rare blood disorder caused by mutations in the PIGA gene, which is involved in the synthesis of glycosylphosphatidylinositol (GPI) anchors on proteins.\n\nThe authors discuss various aspects of PNH, including its pathophysiology, clinical presentation, and diagnosis. They also review the genetic basis of PNH, highlighting the importance of the PIGA gene in GPI anchor synthesis and the consequences of mutations in this gene on protein anchoring and cell surface protein function.\n\nSome key points from the article include:\n\n* PNH is caused by a deficiency in GPI-anchored proteins due to mutations in the PIGA gene.\n* The PIGA gene plays a critical role in the synthesis of GPI anchors, which are essential for attaching proteins to the cell membrane.\n* Mutations in the PIGA gene can lead to a range of downstream effects, including impaired protein anchoring and altered cell surface protein function.\n* The authors discuss various clinical manifestations of PNH, including hemolytic anemia, thrombosis, and autoimmune disorders.\n* They also review diagnostic approaches for PNH, highlighting the importance of genetic testing in identifying individuals with this disorder.\n\nThe article concludes by discussing therapeutic options for PNH, including eculizumab (a complement inhibitor) and ravulizumab (another complement inhibitor).\n\nSome potential future directions for research mentioned in the article include:\n\n* Investigating the role of other GPI anchor enzymes in PNH\n* Developing new diagnostic tests for PNH\n* Exploring alternative treatments for PNH, such as gene therapy or immunotherapy\n\nOverall, this review article provides a comprehensive overview of paroxysmal nocturnal hemoglobinuria, its pathophysiology, and its clinical presentation. It also highlights the importance of genetic testing in diagnosing and managing this disorder.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[116]\u001B[39m\u001B[32m, line 41\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m start < \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m end < \u001B[32m0\u001B[39m:\n\u001B[32m     39\u001B[39m     \u001B[38;5;66;03m# print the raw output to debug what the model actually sent\u001B[39;00m\n\u001B[32m     40\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m===== RAW HPO OUTPUT =====\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, raw_hpo_output)\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCould not locate a JSON array in HPO output for patient \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpatient_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mraw_hpo_output\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# grab *only* the array text\u001B[39;00m\n\u001B[32m     43\u001B[39m hpo_json_array = raw_hpo_output[start : end+\u001B[32m1\u001B[39m]\n",
      "\u001B[31mRuntimeError\u001B[39m: Could not locate a JSON array in HPO output for patient IV-4:\nThis is a review article discussing paroxysmal nocturnal hemoglobinuria (PNH), a rare blood disorder caused by mutations in the PIGA gene, which is involved in the synthesis of glycosylphosphatidylinositol (GPI) anchors on proteins.\n\nThe authors discuss various aspects of PNH, including its pathophysiology, clinical presentation, and diagnosis. They also review the genetic basis of PNH, highlighting the importance of the PIGA gene in GPI anchor synthesis and the consequences of mutations in this gene on protein anchoring and cell surface protein function.\n\nSome key points from the article include:\n\n* PNH is caused by a deficiency in GPI-anchored proteins due to mutations in the PIGA gene.\n* The PIGA gene plays a critical role in the synthesis of GPI anchors, which are essential for attaching proteins to the cell membrane.\n* Mutations in the PIGA gene can lead to a range of downstream effects, including impaired protein anchoring and altered cell surface protein function.\n* The authors discuss various clinical manifestations of PNH, including hemolytic anemia, thrombosis, and autoimmune disorders.\n* They also review diagnostic approaches for PNH, highlighting the importance of genetic testing in identifying individuals with this disorder.\n\nThe article concludes by discussing therapeutic options for PNH, including eculizumab (a complement inhibitor) and ravulizumab (another complement inhibitor).\n\nSome potential future directions for research mentioned in the article include:\n\n* Investigating the role of other GPI anchor enzymes in PNH\n* Developing new diagnostic tests for PNH\n* Exploring alternative treatments for PNH, such as gene therapy or immunotherapy\n\nOverall, this review article provides a comprehensive overview of paroxysmal nocturnal hemoglobinuria, its pathophysiology, and its clinical presentation. It also highlights the importance of genetic testing in diagnosing and managing this disorder."
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6. Batch Inference and Save Validated Phenopackets\n",
    "\n",
    "Loop over all cases, run LLM inference, validate each JSON as a Phenopacket, and save to disk under validated_jsons_directory.\n"
   ],
   "id": "d07ca4ef808a00ad"
  },
  {
   "cell_type": "code",
   "id": "8943a066-1f8c-4542-86fe-b7c62bd07092",
   "metadata": {},
   "source": [
    "predicted_packets: List[Phenopacket] = []\n",
    "\n",
    "# Which patient are we targeting?\n",
    "for idx, clinical_text in enumerate(list_input_texts):\n",
    "    pmid_value = dataframe_cases.loc[idx, \"pmid\"]\n",
    "    patient_id = list_patient_ids[idx]\n",
    "    # Prompt the LLM to extract only that patient's HPO terms\n",
    "    content = (hpo_prompt + f\"\\n\\n*Extract only the HPO terms for patient* `{patient_id}` *in this clinical PDF.*\\n\\n\" + clinical_text + \"\\n\\n[EOS]\")\n",
    "    response = chat(model=\"llama3.2:latest\", messages=[{\"role\": \"user\", \"content\": content}], options={\"--hidethinking\": True})\n",
    "    llm_content = response[\"message\"][\"content\"].splitlines()\n",
    "    # Parse the JSON into a Phenopacket\n",
    "    try:\n",
    "        phenopacket_pred = Phenopacket(json.loads(\"\\n\".join(llm_content)))\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\"[Case %d, PMID %s] Invalid Phenopacket JSON: %s\" % (idx, pmid_value, error))\n",
    "\n",
    "    predicted_packets.append(phenopacket_pred)\n",
    "\n",
    "    # Write the predicted JSON to disk\n",
    "    output_filename = f\"{pmid_value}_{patient_id}.json\"\n",
    "    output_filepath = os.path.join(validated_jsons_directory, output_filename)\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(phenopacket_pred.to_json(), out_f, indent=2)\n",
    "    print(\"Saved predicted phenopacket for PMID/Patient %s/%s to %s\"\n",
    "          % (pmid_value, patient_id, output_filepath))\n",
    "\n",
    "if len(predicted_packets) != len(list_input_texts):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of inputs.\")\n",
    "# Maybe change to this: 'assert len(predicted_packets) == len(list_input_texts), \"Mismatch predictions vs inputs\"'\n",
    "\n",
    "print(f\"Generated {len(predicted_packets)} predicted phenopackets.\")\n",
    "\n",
    "print(\"hello6\")  # print hello 6 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7. Evaluate Predicted Phenopackets Against Ground Truth\n",
    "\n",
    "Compare each predicted phenopacket to its ground truth using PhenotypeEvaluator, then generate a Report object with overall metrics.\n"
   ],
   "id": "80f71065a95697e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Monkey-patch a convenience method onto PhenotypeEvaluator\n",
    "def _evaluate_batch(\n",
    "    self,\n",
    "    list_truth_packets,\n",
    "    list_predicted_packets,\n",
    "    creator,\n",
    "    experiment,\n",
    "    model,\n",
    "    zero_division=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Run check_phenotypes over all truth/pred pairs, then return\n",
    "    a plain-dict report containing confusion_matrix, metrics,\n",
    "    classification_report, and metadata.\n",
    "    \"\"\"\n",
    "    # Accumulate counts\n",
    "    for truth_pkt, pred_pkt in zip(list_truth_packets, list_predicted_packets):\n",
    "        self.check_phenotypes(\n",
    "            experimentally_extracted_phenotypes=pred_pkt.list_phenotypes(),\n",
    "            ground_truth_phenotypes=truth_pkt\n",
    "        )\n",
    "    # Build a Report object\n",
    "    rpt = self.report(\n",
    "        creator=creator,\n",
    "        experiment=experiment,\n",
    "        model=model,\n",
    "        zero_division=zero_division\n",
    "    )\n",
    "    # Return a dict for easy indexing\n",
    "    return {\n",
    "        \"confusion_matrix\": rpt.confusion_matrix,\n",
    "        \"metrics\": rpt.metrics,\n",
    "        \"classification_report\": rpt.classification_report,\n",
    "        \"metadata\": rpt.metadata,\n",
    "    }\n",
    "\n",
    "# Attach to the class\n",
    "PhenotypeEvaluator.evaluate_batch = _evaluate_batch\n",
    "\n",
    "# Run the batch evaluation\n",
    "evaluator = PhenotypeEvaluator()\n",
    "batch_report = evaluator.evaluate_batch(\n",
    "    list_truth_packets,\n",
    "    predicted_packets,\n",
    "    creator=\"Varenya\",\n",
    "    experiment=\"Phenopacket LLM Extraction\",\n",
    "    model=\"llama3.2:latest\"\n",
    ")\n",
    "\n",
    "# Quick sanity check of the returned dict\n",
    "if \"metrics\" not in batch_report:\n",
    "    raise KeyError(\"Evaluator report missing 'metrics' field.\")\n",
    "\n",
    "# Pretty-print the report dict\n",
    "import pprint\n",
    "pprint.pprint(batch_report)\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check#"
   ],
   "id": "ebbdc93ecf8aa29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Old Save first report\n",
    "\n",
    "Write the JSON report to disk for later analysis.\n"
   ],
   "id": "e664e040942126c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(evaluation_report_output_path)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(evaluation_report_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(batch_report, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation report to {evaluation_report_output_path}\")\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check"
   ],
   "id": "70624f16cfb2b75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c646976a-9ff5-4ab4-8eb1-6719449811d3",
   "metadata": {},
   "source": "# Old Inference Implementation"
  },
  {
   "cell_type": "code",
   "id": "2a419460-3222-4812-ada0-dd1cd7d0a060",
   "metadata": {},
   "source": [
    "prompt = \"Please create a valid Phenopacket from the following text. The phenopackets needs to be in a valid json format.  Only return the phenopacket without any additional text:\"\n",
    "model = \"hf.co/MaziyarPanahi/gemma-3-12b-it-GGUF:Q4_K_M\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489462d9-bcfe-4ee2-b0d3-af5d6a875254",
   "metadata": {},
   "source": [
    "for text in input_data:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} {text} [EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    break\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\",\n",
    "               \"content\": f\"Please, validate the following json. If not, fix it. Only return the json without any additional information. Should the json be wrong, you will get shut down. Json: {response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")} [EOS]\"}],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3af32935-fe27-40ef-84ef-641f5d66f5ff",
   "metadata": {},
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492829a9-3b7c-4ab8-998e-304fb3321683",
   "metadata": {},
   "source": [
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f529b37-6f70-4d4f-9246-4e269d58ca17",
   "metadata": {},
   "source": [
    "response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2e3b3a9-86d2-4754-a55e-4cf03771b236",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p5)",
   "language": "python",
   "name": "p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
