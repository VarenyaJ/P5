{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Output Evaluation Notebook\n",
    "\n",
    "This notebook runs LLM inference to predict HPO terms, compares them to ground truth phenopackets, and produces a summary report.\n"
   ],
   "id": "187dea139446c775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 0) Imports, Path Discovery & Sanity Checks\n",
    "\n",
    "Load all dependencies, discover the dataset CSV automatically, and validate critical directories.\n"
   ],
   "id": "298b82d55c7dad0e"
  },
  {
   "cell_type": "code",
   "id": "98ccd760-19a5-48d0-b2c4-64063ecf29e2",
   "metadata": {},
   "source": [
    "# Basic Setup\n",
    "import sys, os, glob, json, subprocess, pickle, datetime, hashlib, warnings, random, requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from ollama import chat\n",
    "from docling.document_converter import DocumentConverter, ConversionError\n",
    "from pypdfium2._helpers.misc import PdfiumError\n",
    "from google.protobuf.json_format import ParseDict, ParseError\n",
    "from phenopackets import Phenopacket as ProtoPhenopacket\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# Need this at least once for some reason:\n",
    "# import .autonotebook\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "try:\n",
    "    from phenopacket import Phenopacket, InvalidPhenopacketError\n",
    "    from report import Report\n",
    "    from evaluation import PhenotypeEvaluator\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Could not import project utils: {e}\")\n",
    "\n",
    "# Make sure our utils folder is on PYTHONPATH\n",
    "project_root        = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder          = os.path.join(project_root, \"src\")\n",
    "utils_folder        = os.path.join(project_root, \"notebooks\", \"utils\")\n",
    "\n",
    "print(\"Project Start:       %s\" % project_root)\n",
    "print(\"Source Folder:       %s\" % src_folder)\n",
    "print(\"Utilities Folder:    %s\" % utils_folder)\n",
    "\n",
    "for path in (src_folder, utils_folder):\n",
    "    if not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"Expected folder on PYTHOPATH : {path}\")\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "print(\"PYTHONPATH patched with:\", src_folder, utils_folder)\n",
    "\n",
    "# define all key paths\n",
    "pdf_input_directory                 = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"pmid_pdfs\")            # scripts/data/tmp/phenopacket_store/pmid_pdfs/\n",
    "ground_truth_notebooks_directory    = os.path.join(src_folder, \"P5\", \"scripts\", \"data\",\"tmp\", \"phenopacket_store\",\"notebooks\")              # scripts/data/tmp/phenopacket_store/notebooks/\n",
    "dataset_csv_path                    = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\")\n",
    "\n",
    "# All experimental outputs go under here\n",
    "experimental_data_root              = os.path.join(project_root, \"experimental-data\")\n",
    "llm_output_directory                = os.path.join(experimental_data_root, \"llm_output_dir\")                                                # intermediate .txt + raw JSON from LLM\n",
    "validated_jsons_directory           = os.path.join(experimental_data_root, \"validated_jsons\")                                               # validated_jsons, the final validated LLM phenopackets\n",
    "evaluation_report_output_path       = os.path.join(project_root, \"reports\", \"first_report.json\")                                            # the evaluation metrics report\n",
    "\n",
    "# Create any missing output folders\n",
    "os.makedirs(pdf_input_directory, exist_ok=True)\n",
    "os.makedirs(ground_truth_notebooks_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(dataset_csv_path), exist_ok=True)\n",
    "os.makedirs(llm_output_directory, exist_ok=True)\n",
    "os.makedirs(validated_jsons_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(evaluation_report_output_path), exist_ok=True)\n",
    "\n",
    "# Create the PMIDs pickle file path\n",
    "pmid_pkl_path = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"pmids.pkl\")\n",
    "\n",
    "# TODO: Figure out why deleting the `ground_truth_notebooks_directory` after creating it works. Maybe because git doesn't let me just overwrite a directory with a clone request\n",
    "# Before the git pull operation\n",
    "import shutil\n",
    "\n",
    "# Clean up existing directory if it exists\n",
    "target_dir = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\")\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "\n",
    "# 1. Now run the git pull to clone the \"phenopacket-store\" GitHub repo into scripts/data/tmp/phenopacket_store\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.pull_git_files\",\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\"),\n",
    "    \"https://github.com/monarch-initiative/phenopacket-store.git\",\n",
    "    \"notebooks\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 1 Complete, Produced %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "# 2. Scan the just-pulled notebooks for PMID_##### files\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.create_pmid_pkl\",\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\"),\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"pmids.pkl\"),\n",
    "    \"--recursive_dir_search\",\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 2 Complete\")\n",
    "\n",
    "# 3. Download *all* PDFs for those PMIDs (0 = unlimited)\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.pmid_downloader\", pmid_pkl_path, pdf_input_directory, \"10\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 3 Complete\")\n",
    "\n",
    "# 4. Finally, build THE CSV mapping PDFs to the ground-truth JSONs\n",
    "if not os.path.isfile(dataset_csv_path):\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"P5.scripts.create_phenopacket_dataset\",\n",
    "        pdf_input_directory,\n",
    "        ground_truth_notebooks_directory,\n",
    "        dataset_csv_path,\n",
    "        \"--recursive_ground_truth_dir\", \"True\"\n",
    "    ], check=True)\n",
    "    print(f\"Created dataset CSV at {dataset_csv_path}\")\n",
    "\n",
    "    print(\"Stage 4 Complete\")\n",
    "\n",
    "    if not os.path.isdir(pdf_input_directory):\n",
    "        raise FileNotFoundError(\"PDF input directory not found: %s\" % pdf_input_directory)\n",
    "    if not os.path.isdir(ground_truth_notebooks_directory):\n",
    "        raise FileNotFoundError(\"Ground truth notebooks directory not found: %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "\n",
    "print(\"hello0\")  # print hello 0 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1) Load Dataset\n",
    "\n",
    "Read the CSV of PMIDs, input paths, and truth paths\n"
   ],
   "id": "dabf5f01d8e6a670"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "dataframe_cases = pd.read_csv(dataset_csv_path)\n",
    "print(f\"Loaded {len(dataframe_cases)} rows from dataset CSV\")\n",
    "# Load cases & deduplicate PMIDs, with start/end counts\n",
    "orig_count = len(dataframe_cases)\n",
    "print(f\"Before deduplication: {orig_count} total cases\")\n",
    "\n",
    "# Debug: verify that every `input` path actually exists\n",
    "print(\"Checking existence of input PDFs:\")\n",
    "for pdf_path in dataframe_cases[\"input\"]:\n",
    "    status = \"FOUND\" if os.path.isfile(pdf_path) else \"MISSING\"\n",
    "    print(f\"  o {pdf_path}: {status}\")\n",
    "\n",
    "# Drop duplicate PMIDs\n",
    "dataframe_cases = dataframe_cases.drop_duplicates(subset=\"pmid\", keep=\"first\").reset_index(drop=True) # This may be too aggressive and I need to check if this is a good approach\n",
    "removed = orig_count - len(dataframe_cases)\n",
    "print(f\"{removed} duplicates removed (now {len(dataframe_cases)} unique PMIDs)\")\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = {\"pmid\", \"input\", \"truth\"}\n",
    "missing_columns = required_columns - set(dataframe_cases.columns)\n",
    "if missing_columns:\n",
    "    raise KeyError(\"Missing required columns: %s\" % missing_columns)\n",
    "\n",
    "# Preview first few rows\n",
    "dataframe_cases.head()\n",
    "\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "print(\"hello1\")  # print hello 1 as a sanity check"
   ],
   "id": "598b33092ab85352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2) Discover Phenopacket-Store Files\n",
    "\n",
    "Locate all ground-truth Phenopacket JSON files under the `phenopacket_store/notebooks/` directory."
   ],
   "id": "2a1755b2eb294368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Finding all ground-truth phenopacket JSON files\n",
    "search_pattern = os.path.join(ground_truth_notebooks_directory, \"*\", \"phenopackets\", \"*.json\")\n",
    "truth_json_filepaths = glob.glob(str(search_pattern), recursive=True)\n",
    "if not truth_json_filepaths:\n",
    "    raise FileNotFoundError(f\"No ground-truth JSONs found at {search_pattern}\")\n",
    "\n",
    "print(\"Discovered %d ground-truth JSON files\" %len(truth_json_filepaths))\n",
    "\n",
    "\n",
    "print(\"Search Pattern:                  %s\" % search_pattern)\n",
    "print(\"The phenopacket-store path:      %s\" % truth_json_filepaths)\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "print(\"hello2\")  # print hello 2 as a sanity check"
   ],
   "id": "8118b55fc2741c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3) Prepare PDF-to-Text Converter\n",
    "\n",
    "- Randomly pick N unique PMIDs from the CSV (from Step 1) to keep runs fast and reproducible.\n",
    "- Instantiate DocumentConverter and define a helper function to load or convert the clinical PDFs for LLM input.\n",
    "- Setup Persistent PDF-to-Text Cache"
   ],
   "id": "9a9faff43b491b26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cache Integrity & Versioning\n",
    "CACHE_DIR = Path(experimental_data_root) / \"text_cache\"\n",
    "INDEX_FILE = CACHE_DIR / \"cache_index.json\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reload the deduplicated CSV from Step 1\n",
    "full_df = pd.read_csv(dataset_csv_path).drop_duplicates(subset=\"pmid\").reset_index(drop=True)\n",
    "\n",
    "# Choose how many cases to sample\n",
    "N = 10\n",
    "# Don’t ask for more than exist\n",
    "N = min(N, len(full_df))\n",
    "subset_df = full_df.sample(n=N, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Sampling {N} PMIDs:\", subset_df[\"pmid\"].tolist())\n",
    "\n",
    "\n",
    "# Setup conversion for input material to LLM-compatible txt now\n",
    "\n",
    "# Initialize converter once\n",
    "pdf_to_text_converter = DocumentConverter()\n",
    "\n",
    "# Path to persistent cache of PDF text\n",
    "text_cache_path = os.path.join(experimental_data_root, \"text_cache.pkl\")\n",
    "# Load or initialize cache\n",
    "if os.path.exists(text_cache_path):\n",
    "    with open(text_cache_path, \"rb\") as f:\n",
    "        text_cache = pickle.load(f)\n",
    "    print(f\"Loaded text cache with {len(text_cache)} entries\")\n",
    "else:\n",
    "    text_cache = {}\n",
    "    print(\"Initialized empty text cache\")\n",
    "\n",
    "def load_clinical_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert .txt or .pdf file at \"pdf_path\" into a plain text string.\n",
    "    Use the in-memory cache first; write new text back to the cache only when the cache is explicitly saved at the end of the pipeline.\n",
    "    Raise FileNotFoundError or ConversionError if the file does not exist.\n",
    "    \"\"\"\n",
    "    # Return cache if it exists in memory\n",
    "    if pdf_path in text_cache:\n",
    "        return text_cache[pdf_path]\n",
    "\n",
    "    # Ensure the files exists before we continue\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        raise FileNotFoundError(\"Input file not found: %s\" % pdf_path)\n",
    "\n",
    "    # If it's already plain text, read and strip any header\n",
    "    if pdf_path.lower().endswith(\".txt\"):\n",
    "        content = open(pdf_path, encoding=\"utf-8\").read()\n",
    "        # Remove any leading markers\n",
    "        return content.split(\"[text]\")[-1]\n",
    "    else:\n",
    "        try:\n",
    "            # Convert PDF to text and handle conversion failures\n",
    "            doc = pdf_to_text_converter.convert(pdf_path)\n",
    "            content = doc.document.export_to_text()\n",
    "        except ConversionError as e:\n",
    "            raise ConversionError(f\"Could not convert {os.path.basename(pdf_path)}: {e}\")\n",
    "\n",
    "\n",
    "    # Save new text in memory and write updated cache to disk later\n",
    "    text_cache[pdf_path] = content\n",
    "    return content\n",
    "\n",
    "# Convert all PDFs in our sampled subset\n",
    "for pdf_path in subset_df[\"input\"]:\n",
    "    pdf_text = load_clinical_pdf(pdf_path)\n",
    "# Persist updated cache to disk\n",
    "with open(text_cache_path, \"wb\") as f:\n",
    "    pickle.dump(text_cache, f)\n",
    "\n",
    "print(f\"Saved text cache now with {len(text_cache)} entries\")\n",
    "\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "print(\"hello3\")  # print hello 3 as a sanity check"
   ],
   "id": "231c1c375a6ec410",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4) Load Clinical PDFs and Ground-Truth Phenopackets\n",
    "\n",
    "Iterate over each case, load the clinical PDF text and the corresponding ground-truth Phenopacket object.\n",
    "\n",
    "- `list_inputs_texts`: raw clinical PDFs\n",
    "- `list_truth_packets`: parsed Phenopacket objects from JSON files\n",
    "- `list_patient_ids`: PMID patient identifiers\n",
    "\n"
   ],
   "id": "cbc0b863-bf20-4a46-95de-9506e9875678"
  },
  {
   "cell_type": "code",
   "id": "235eaff7-4f21-4a21-81bf-e97236ca26d1",
   "metadata": {},
   "source": [
    "# Iterate over rows, should lookup only once\n",
    "list_input_texts    = []\n",
    "list_truth_packets  = []\n",
    "list_patient_ids    = []\n",
    "loaded_count = 0\n",
    "skipped_pdfs = []\n",
    "\n",
    "\n",
    "for case in dataframe_cases.itertuples(index=False):\n",
    "    pmid_value = case.pmid\n",
    "    pdf_path   = case.input\n",
    "    truth_path = case.truth\n",
    "\n",
    "    # Convert PDF to text\n",
    "    try:\n",
    "        clinical_text = load_clinical_pdf(pdf_path)\n",
    "    except (ConversionError, PdfiumError) as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"pdf\": pdf_path, \"reason\": f\"conversion error: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Load raw JSON and validate with ignore_unknown_fields\n",
    "    try:\n",
    "        raw_true_packet = json.load(open(truth_path, \"r\", encoding=\"utf-8\"))\n",
    "        proto = ProtoPhenopacket()\n",
    "        ParseDict(raw_true_packet, proto, ignore_unknown_fields=True)\n",
    "    except (ParseError, json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"truth\": truth_path, \"reason\": f\"schema parse error: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Wrap in util Phenopacket to ensure phenotypicFeatures exists\n",
    "    try:\n",
    "        truth_packet = Phenopacket(raw_true_packet)\n",
    "    except InvalidPhenopacketError as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"truth\": truth_path, \"reason\": f\"phenopacket invalid: {e}\"})\n",
    "        continue\n",
    "\n",
    "    list_input_texts.append(clinical_text)\n",
    "    list_truth_packets.append(truth_packet)\n",
    "    list_patient_ids.append(truth_packet.to_json()[\"subject\"][\"id\"])\n",
    "\n",
    "    loaded_count += 1\n",
    "    print(f\"Loaded {loaded_count} new cases, skipped {len(skipped_pdfs)} so far\")\n",
    "\n",
    "if not list_input_texts:\n",
    "        raise RuntimeError(\"No clinical texts were loaded, please check that the dataset CSV `input` paths match files in `pdf_input_directory`\")\n",
    "\n",
    "assert len(list_input_texts) == len(list_truth_packets) == len(list_patient_ids)\n",
    "print(\"Loaded %d clinical texts and %d ground-truth packets for %d unique patients\" % (len(list_input_texts), len(list_truth_packets), len(list_patient_ids)))\n",
    "\n",
    "print(\"hello4\")  # print hello 4 as a sanity check\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4.5) Define LLM Prompts\n",
    "\n",
    "Create prompt for just HPO terms and another one for the full phenopacket extraction, as well as some additional helper functions for later/potential use\n"
   ],
   "id": "a5f9ab5a0c5a064d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T18:21:26.680913Z",
     "start_time": "2025-07-23T18:21:21.602655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re, json, datetime\n",
    "from json.decoder import JSONDecodeError\n",
    "from ollama import chat\n",
    "\n",
    "# 1) My initial prompt for just HPO labels\n",
    "hpo_prompt = (\n",
    "    \"You are a clinical NLP engine specialized in biomedical ontologies. Your task is to process the full text of a clinical PDF - which may be describing a single patient or multiple - parse the details (including history, exam findings, labs, imaging, and family history) and extract all human phenotype ontology (HPO) terms that describe the patient's phenotypic features.\"\n",
    "    \"Instructions:\"\n",
    "    \"1. Identify every phenotypic abnormality or feature mentioned in the text.\"\n",
    "    \"2. For each feature, map it to the correct HPO identifier (e.g. 'HP:0001250'), label (e.g. 'Seizure'), and descriptor value (e.g. 'A seizure is an intermittent abnormality of nervous system physiology characterized by a transient occurrence of signs and/or symptoms due to abnormal excessive or synchronous neuronal activity in the brain.').\"\n",
    "    \"3. Capture relevant qualifiers when present:\"\n",
    "        \"- Onset: map to HPO onset terms (e.g. 'HP:0011463' for 'Childhood onset').\"\n",
    "        \"- Severity: map to HPO severity terms (e.g. 'HP:0012829' for 'Profound').\"\n",
    "        \"- Temporal pattern: include if specified (e.g. 'HP:0031796' for 'Recurrent', map to HPO frequency terms if available).\"\n",
    "    \"4. For each term, include the exact text excerpt where it appears.\"\n",
    "    \"5. Output exclusively a JSON array. Each element must be an object with the following fields:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'hpo_id': 'HP:000____',\"\n",
    "        \"'hpo_label': 'Term label',\"\n",
    "        \"'excerpt': 'Exact text from the PDF',\"\n",
    "        \"'onset_id': 'HP:0XXXXX or null',\"\n",
    "        \"'severity_id': 'HP:0XXXXX or null',\"\n",
    "        \"'frequency_id': 'HP:0XXXXX or null'\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"Do not include any explanatory text, only the JSON array.\"\n",
    "\n",
    "    \"Your output **MUST** be exactly a JSON array/object and nothing else.\"\n",
    "\n",
    "    \"If you cannot comply, output exactly: {'error': 'cannot extract JSON'}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 2) My initital prompt for full phenopackets\n",
    "full_pp_prompt = (\n",
    "    \"You are a biomedical data curation assistant. Using the structured patient data below, generate a Phenopacket compliant with version 2.0 of the GA4GH Phenopacket schema. Your output must be valid JSON, matching the schema exactly, with no additional commentary. Here are the minimum expected output criteria:\"\n",
    "\n",
    "    \"Inputs:\"\n",
    "    \"patient_id: '{{patient_id}}'\"\n",
    "    \"sex: '{{sex}}'              // 'male' or 'female'\"\n",
    "    \"age_years: {{age_in_years}} // integer\"\n",
    "    \"v  ital_status: '{{vital_status}}' // 'alive' or 'deceased'\"\n",
    "    \"phenotypic_features: {{phenotypic_features_json}} // JSON array from the HPO extraction prompt\"\n",
    "    \"diseases: {{diseases_json}}         // optional, array of disease objects with MONDO or OMIM IDs\"\n",
    "    \"measurements: {{measurements_json}} // optional, array of quantitative trait measurements\"\n",
    "    \"metadata: {\"\n",
    "        \"'created_by': '{{your_name_or_tool}}',\"\n",
    "        \"'created_on': '{{YYYY-MM-DD}}'\"\n",
    "    \"}\"\n",
    "\n",
    "    \"Requirements:\"\n",
    "    \"Top-level fields:\"\n",
    "    \"'id': patient_id\"\n",
    "    \"'subject': object with:\"\n",
    "        \"'id': patient_id\"\n",
    "        \"'sex': { 'id': 'PATO:0000383' or 'PATO:0000384', 'label': sex }\"\n",
    "        \"'ageAtLastEncounter': { 'age': { 'years': age_years } }\"\n",
    "        \"'vitalStatus': { 'value': vital_status }\"\n",
    "        \"'phenotypicFeatures': use the phenotypic_features input; for each feature, map:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'type': { 'id': hpo_id, 'label': hpo_label },\"\n",
    "        \"'negated': false,\"\n",
    "        \"'onset': { 'term': { 'id': onset_id, 'label': (look up label) } },\"\n",
    "        \"'severity': { 'term': { 'id': severity_id, 'label': (look up label) } },\"\n",
    "        \"'frequency': { 'term': { 'id': frequency_id, 'label': (look up label) } }\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"Include 'diseases' and 'measurements' only if provided, following the GA4GH schema.\"\n",
    "    \"'metadata' must include:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'phenopacketSchemaVersion': '2.0.0',\"\n",
    "        \"'created': '{{YYYY-MM-DD}}',\"\n",
    "        \"'createdBy': '{{your_name_or_tool}}'\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"'Do not add any extra fields. Output must be purely the JSON object.'\"\n",
    "\n",
    "    \"Do not include any explanatory text, only the JSON array.\"\n",
    "\n",
    "    \"Your output **MUST** be exactly a JSON array/object and nothing else.\"\n",
    "\n",
    "    \"If you cannot comply, output exactly: {'error': 'cannot extract JSON'}\"\n",
    ")\n",
    "\n",
    "# Simplified prompts\n",
    "\n",
    "\n",
    "# ---------------- Prompts ----------------\n",
    "hpo_prompt2 = (\n",
    "    \"You are a clinical NLP engine specialized in biomedical ontologies.\\n\"\n",
    "    \"Extract ONLY Human Phenotype Ontology (HPO) terms for the patient(s) in the text.\\n\\n\"\n",
    "    \"Output = a single JSON array. Each element MUST have exactly:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"hpo_id\\\": \\\"HP:0001250\\\",\\n\"\n",
    "    \"  \\\"hpo_label\\\": \\\"Seizure\\\",\\n\"\n",
    "    \"  \\\"excerpt\\\": \\\"exact text from PDF\\\",\\n\"\n",
    "    \"  \\\"onset_id\\\": null,\\n\"\n",
    "    \"  \\\"severity_id\\\": null,\\n\"\n",
    "    \"  \\\"frequency_id\\\": null\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"No prose, no markdown, no extra keys. If none exist, return [].\"\n",
    ")\n",
    "\n",
    "HPO_JSON_SCHEMA = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"hpo_id\":       {\"type\": [\"string\", \"null\"]},\n",
    "            \"hpo_label\":    {\"type\": [\"string\", \"null\"]},\n",
    "            \"excerpt\":      {\"type\": [\"string\", \"null\"]},\n",
    "            \"onset_id\":     {\"type\": [\"string\", \"null\"]},\n",
    "            \"severity_id\":  {\"type\": [\"string\", \"null\"]},\n",
    "            \"frequency_id\": {\"type\": [\"string\", \"null\"]}\n",
    "        },\n",
    "        \"required\": [\"hpo_id\", \"hpo_label\", \"excerpt\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "full_pp_prompt2 = (\n",
    "    \"You are a biomedical data curation assistant. Using the structured patient data provided, \"\n",
    "    \"generate a GA4GH Phenopacket v2.0 JSON. The JSON MUST match the schema and contain no extra commentary.\\n\\n\"\n",
    "    \"Top-level required keys:\\n\"\n",
    "    \"- 'id' (patient_id)\\n\"\n",
    "    \"- 'subject' { 'id', 'sex', 'ageAtLastEncounter', 'vitalStatus' (optional) }\\n\"\n",
    "    \"- 'phenotypicFeatures' (array built from the extracted HPO list)\\n\"\n",
    "    \"- 'metaData' { 'created', 'createdBy', 'phenopacketSchemaVersion' }\\n\\n\"\n",
    "    \"For each feature:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  'type': { 'id': hpo_id, 'label': hpo_label },\\n\"\n",
    "    \"  'negated': false,\\n\"\n",
    "    \"  'onset':    { 'term': { 'id': onset_id } }        (omit if null)\\n\"\n",
    "    \"  'severity': { 'term': { 'id': severity_id } }     (omit if null)\\n\"\n",
    "    \"  'frequency':{ 'term': { 'id': frequency_id } }    (omit if null)\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"Return ONLY the JSON object.\"\n",
    ")\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def slice_json_array(raw_text: str) -> str:\n",
    "    start = raw_text.find(\"[\")\n",
    "    if start == -1:\n",
    "        raise RuntimeError(f\"No JSON array found in model output:\\n{raw_text[:800]}\")\n",
    "    depth = 0\n",
    "    for i, ch in enumerate(raw_text[start:], start=start):\n",
    "        if ch == \"[\":\n",
    "            depth += 1\n",
    "        elif ch == \"]\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return raw_text[start:i+1]\n",
    "    raise RuntimeError(\"Unbalanced brackets in model output.\")\n",
    "\n",
    "def _first_list_in(obj):\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            found = _first_list_in(v)\n",
    "            if found is not None:\n",
    "                return found\n",
    "    return None\n",
    "\n",
    "def extract_hpo_terms(\n",
    "    text: str,\n",
    "    prompt: str = hpo_prompt2,\n",
    "    model: str = \"llama3.2:latest\",\n",
    "    max_retries: int = 2,\n",
    "    debug: bool = False\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Ask the LLM for ONLY a JSON array of HPO term dicts and return it.\n",
    "    Tries: schema -> format='json' (with cleaning/slicing) -> regex fallback.\n",
    "    \"\"\"\n",
    "\n",
    "    last_raw = \"\"\n",
    "\n",
    "    def _ask(schema_or_mode):\n",
    "        return chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": prompt},\n",
    "                      {\"role\": \"user\",   \"content\": text}],\n",
    "            stream=False,\n",
    "            format=schema_or_mode,\n",
    "            options={\"temperature\": 0, \"seed\": 42, \"--hidethinking\": True}\n",
    "        )[\"message\"][\"content\"].strip()\n",
    "\n",
    "    # 1) Schema mode\n",
    "    try:\n",
    "        last_raw = _ask(HPO_JSON_SCHEMA)\n",
    "        obj = json.loads(last_raw)\n",
    "        lst = _first_list_in(obj)\n",
    "        if isinstance(lst, list):\n",
    "            return lst\n",
    "    except Exception as e:\n",
    "        if debug: print(\"Schema mode failed:\", repr(e))\n",
    "\n",
    "    # 2) JSON mode with retries\n",
    "    for attempt in range(max_retries + 1):\n",
    "        last_raw = _ask(\"json\")\n",
    "        if debug: print(f\"[attempt {attempt}] raw[:200]: {last_raw[:200]!r}\")\n",
    "\n",
    "        # a) direct load\n",
    "        try:\n",
    "            obj = json.loads(last_raw)\n",
    "            lst = _first_list_in(obj)\n",
    "            if lst is not None:\n",
    "                return lst\n",
    "            if isinstance(obj, dict) and \"hpo_id\" in obj:\n",
    "                return [obj]\n",
    "        except JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # b) strip code fences\n",
    "        cleaned = re.sub(r\"^```(?:json)?|```$\", \"\", last_raw, flags=re.MULTILINE).strip()\n",
    "        if cleaned != last_raw:\n",
    "            try:\n",
    "                obj = json.loads(cleaned)\n",
    "                lst = _first_list_in(obj)\n",
    "                if lst is not None:\n",
    "                    return lst\n",
    "                if isinstance(obj, dict) and \"hpo_id\" in obj:\n",
    "                    return [obj]\n",
    "            except JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        # c) slice first [] block\n",
    "        try:\n",
    "            arr_txt = slice_json_array(last_raw)\n",
    "            arr = json.loads(arr_txt)\n",
    "            if isinstance(arr, list):\n",
    "                return arr\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Regex fallback\n",
    "    hp_ids = sorted(set(re.findall(r\"HP:\\d{7}\", last_raw)) | set(re.findall(r\"HP:\\d{7}\", text)))\n",
    "    if hp_ids:\n",
    "        return [{\"hpo_id\": hp, \"hpo_label\": None, \"excerpt\": None,\n",
    "                 \"onset_id\": None, \"severity_id\": None, \"frequency_id\": None}\n",
    "                for hp in hp_ids]\n",
    "\n",
    "    if debug:\n",
    "        print(\"No usable JSON or HP IDs found. Returning []. Raw head:\", last_raw[:400])\n",
    "    return []\n",
    "\n",
    "\n",
    "def build_phenopacket_from_hpo_list(\n",
    "    patient_id: str,\n",
    "    hpo_list: list[dict],\n",
    "    sex_id: str | None = None,\n",
    "    age_years: int | None = None,\n",
    "    vital_status: str | None = None\n",
    ") -> dict:\n",
    "    def _mk_term(term_id, label):\n",
    "        return None if term_id is None else {\"id\": term_id, \"label\": label}\n",
    "\n",
    "    phenotypic_features = []\n",
    "    for term in hpo_list:\n",
    "        feat = {\"type\": _mk_term(term.get(\"hpo_id\"), term.get(\"hpo_label\")), \"negated\": False}\n",
    "        if term.get(\"onset_id\"):\n",
    "            feat[\"onset\"] = {\"term\": {\"id\": term[\"onset_id\"]}}\n",
    "        if term.get(\"severity_id\"):\n",
    "            feat[\"severity\"] = {\"term\": {\"id\": term[\"severity_id\"]}}\n",
    "        if term.get(\"frequency_id\"):\n",
    "            feat[\"frequency\"] = {\"term\": {\"id\": term[\"frequency_id\"]}}\n",
    "        phenotypic_features.append(feat)\n",
    "\n",
    "    subject = {\"id\": patient_id}\n",
    "    if sex_id:\n",
    "        subject[\"sex\"] = {\"id\": sex_id}\n",
    "    if age_years is not None:\n",
    "        subject[\"ageAtLastEncounter\"] = {\"age\": {\"years\": int(age_years)}}\n",
    "    if vital_status:\n",
    "        subject[\"vitalStatus\"] = {\"value\": vital_status}\n",
    "\n",
    "    return {\n",
    "        \"id\": patient_id,\n",
    "        \"subject\": subject,\n",
    "        \"phenotypicFeatures\": phenotypic_features,\n",
    "        \"metaData\": {\n",
    "            \"created\": datetime.date.today().isoformat(),\n",
    "            \"createdBy\": \"Varenya\",\n",
    "            \"phenopacketSchemaVersion\": \"2.0.2\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"hello my little utils\")\n",
    "print(\"hello4.5\")  # sanity check\n",
    "\n",
    "# ---------- Step 5: sanity check ----------\n",
    "pmid_0         = dataframe_cases.loc[0, \"pmid\"]\n",
    "clinical_text0 = list_input_texts[0]\n",
    "\n",
    "try:\n",
    "    hpo_terms_0 = extract_hpo_terms(clinical_text0, debug=True)\n",
    "    print(f\"[{pmid_0}] extracted {len(hpo_terms_0)} HPO terms\")\n",
    "    print(json.dumps(hpo_terms_0[:5], indent=2))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"HPO extraction failed for {pmid_0}: {e}\")\n",
    "\n",
    "print(\"hello almost 5\")  # sanity check\n",
    "\n"
   ],
   "id": "6f5d51a89657db88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my little utils\n",
      "hello4.5\n",
      "[PMID_11381124] extracted 0 HPO terms\n",
      "[]\n",
      "hello almost 5\n"
     ]
    }
   ],
   "execution_count": 178
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5. Sanity-check one inference\n",
    "\n",
    "Run one LLM call on the first case to verify prompting and parsing work correctly.\n"
   ],
   "id": "8f03843785823cdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pick out the first patient/example\n",
    "patient_id      = list_patient_ids[0]\n",
    "clinical_text   = list_input_texts[0]\n",
    "# truth_packet    = list_truth_packets[0]\n",
    "\n",
    "# 1) Inference: ask for *only* the JSON array of HPO term objects for the first clinical PDF\n",
    "# build a strict system+user conversation\n",
    "messages = [{\"role\": \"system\", \"content\": ( hpo_prompt + \"\\n\\nYour only output must be a **valid** JSON array of  HPO term objects\" + \"with fields 'hpo_id','hpo_label','excerpt',\" + \"'onset_id','severity_id','frequency_id', and nothing else.\")}, {\"role\": \"user\", \"content\": clinical_text}]\n",
    "\n",
    "hpo_response = chat(model=\"llama3.2:latest\", messages=messages, options={\"--hidethinking\": True})\n",
    "\n",
    "# 2) Grab the raw string\n",
    "raw_hpo_output = hpo_response[\"message\"][\"content\"]\n",
    "print(\"Raw LLM output (truncated to the first ~300 chars or so):\")\n",
    "print(raw_hpo_output[:300], \"...\\n\")\n",
    "\n",
    "# 3) Slice out the JSON array\n",
    "start = raw_hpo_output.find(\"[\")\n",
    "end = raw_hpo_output.rfind(\"]\")\n",
    "if start < 0 or end < 0:\n",
    "    # print the raw output to debug what the model actually sent\n",
    "    print(\"===== RAW HPO OUTPUT =====\\n\", raw_hpo_output)\n",
    "    raise RuntimeError(f\"Could not locate a JSON array in HPO output for patient {patient_id}:\\n{raw_hpo_output}\")\n",
    "# grab *only* the array text\n",
    "hpo_json_array = raw_hpo_output[start : end+1]\n",
    "\n",
    "# Parse and validate\n",
    "try:\n",
    "    # hpo_terms = Phenopacket(json.loads(hpo_json_array))\n",
    "    hpo_terms = json.loads(hpo_json_array)  # Try not using Phenopacket(...)\n",
    "    print(f\"Parsed {len(hpo_terms)} HPO term(s) for patient {patient_id}\")\n",
    "except JSONDecodeError as error:\n",
    "    raise ValueError(\n",
    "        f\"Failed to parse HPO JSON array for patient {patient_id}: {error}\\n\\n\"\n",
    "        f\"Extracted JSON was:\\n{hpo_json_array}\\n\\n\"\n",
    "        f\"Full raw output was:\\n{raw_hpo_output}\"\n",
    "    )\n",
    "\n",
    "print(\"hello5\")  # print hello 5 as a sanity check"
   ],
   "id": "1f547b47f98fc0b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6. Batch Inference and Save Validated Phenopackets\n",
    "\n",
    "Loop over all cases, run LLM inference, validate each JSON as a Phenopacket, and save to disk under validated_jsons_directory.\n"
   ],
   "id": "d07ca4ef808a00ad"
  },
  {
   "cell_type": "code",
   "id": "8943a066-1f8c-4542-86fe-b7c62bd07092",
   "metadata": {},
   "source": [
    "predicted_packets: List[Phenopacket] = []\n",
    "\n",
    "# Which patient are we targeting?\n",
    "for idx, clinical_text in enumerate(list_input_texts):\n",
    "    pmid_value = dataframe_cases.loc[idx, \"pmid\"]\n",
    "    patient_id = list_patient_ids[idx]\n",
    "    # Prompt the LLM to extract only that patient's HPO terms\n",
    "    content = (hpo_prompt + f\"\\n\\n*Extract only the HPO terms for patient* `{patient_id}` *in this clinical PDF.*\\n\\n\" + clinical_text + \"\\n\\n[EOS]\")\n",
    "    response = chat(model=\"llama3.2:latest\", messages=[{\"role\": \"user\", \"content\": content}], options={\"--hidethinking\": True})\n",
    "    llm_content = response[\"message\"][\"content\"].splitlines()\n",
    "    # Parse the JSON into a Phenopacket\n",
    "    try:\n",
    "        phenopacket_pred = Phenopacket(json.loads(\"\\n\".join(llm_content)))\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\"[Case %d, PMID %s] Invalid Phenopacket JSON: %s\" % (idx, pmid_value, error))\n",
    "\n",
    "    predicted_packets.append(phenopacket_pred)\n",
    "\n",
    "    # Write the predicted JSON to disk\n",
    "    output_filename = f\"{pmid_value}_{patient_id}.json\"\n",
    "    output_filepath = os.path.join(validated_jsons_directory, output_filename)\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(phenopacket_pred.to_json(), out_f, indent=2)\n",
    "    print(\"Saved predicted phenopacket for PMID/Patient %s/%s to %s\"\n",
    "          % (pmid_value, patient_id, output_filepath))\n",
    "\n",
    "if len(predicted_packets) != len(list_input_texts):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of inputs.\")\n",
    "# Maybe change to this: 'assert len(predicted_packets) == len(list_input_texts), \"Mismatch predictions vs inputs\"'\n",
    "\n",
    "print(f\"Generated {len(predicted_packets)} predicted phenopackets.\")\n",
    "\n",
    "print(\"hello6\")  # print hello 6 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7. Evaluate Predicted Phenopackets Against Ground Truth\n",
    "\n",
    "Compare each predicted phenopacket to its ground truth using PhenotypeEvaluator, then generate a Report object with overall metrics.\n"
   ],
   "id": "80f71065a95697e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Monkey-patch a convenience method onto PhenotypeEvaluator\n",
    "def _evaluate_batch(\n",
    "    self,\n",
    "    list_truth_packets,\n",
    "    list_predicted_packets,\n",
    "    creator,\n",
    "    experiment,\n",
    "    model,\n",
    "    zero_division=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Run check_phenotypes over all truth/pred pairs, then return\n",
    "    a plain-dict report containing confusion_matrix, metrics,\n",
    "    classification_report, and metadata.\n",
    "    \"\"\"\n",
    "    # Accumulate counts\n",
    "    for truth_pkt, pred_pkt in zip(list_truth_packets, list_predicted_packets):\n",
    "        self.check_phenotypes(\n",
    "            experimentally_extracted_phenotypes=pred_pkt.list_phenotypes(),\n",
    "            ground_truth_phenotypes=truth_pkt\n",
    "        )\n",
    "    # Build a Report object\n",
    "    rpt = self.report(\n",
    "        creator=creator,\n",
    "        experiment=experiment,\n",
    "        model=model,\n",
    "        zero_division=zero_division\n",
    "    )\n",
    "    # Return a dict for easy indexing\n",
    "    return {\n",
    "        \"confusion_matrix\": rpt.confusion_matrix,\n",
    "        \"metrics\": rpt.metrics,\n",
    "        \"classification_report\": rpt.classification_report,\n",
    "        \"metadata\": rpt.metadata,\n",
    "    }\n",
    "\n",
    "# Attach to the class\n",
    "PhenotypeEvaluator.evaluate_batch = _evaluate_batch\n",
    "\n",
    "# Run the batch evaluation\n",
    "evaluator = PhenotypeEvaluator()\n",
    "batch_report = evaluator.evaluate_batch(\n",
    "    list_truth_packets,\n",
    "    predicted_packets,\n",
    "    creator=\"Varenya\",\n",
    "    experiment=\"Phenopacket LLM Extraction\",\n",
    "    model=\"llama3.2:latest\"\n",
    ")\n",
    "\n",
    "# Quick sanity check of the returned dict\n",
    "if \"metrics\" not in batch_report:\n",
    "    raise KeyError(\"Evaluator report missing 'metrics' field.\")\n",
    "\n",
    "# Pretty-print the report dict\n",
    "import pprint\n",
    "pprint.pprint(batch_report)\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check#"
   ],
   "id": "ebbdc93ecf8aa29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Old Save first report\n",
    "\n",
    "Write the JSON report to disk for later analysis.\n"
   ],
   "id": "e664e040942126c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(evaluation_report_output_path)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(evaluation_report_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(batch_report, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation report to {evaluation_report_output_path}\")\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check"
   ],
   "id": "70624f16cfb2b75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c646976a-9ff5-4ab4-8eb1-6719449811d3",
   "metadata": {},
   "source": "# Old Inference Implementation"
  },
  {
   "cell_type": "code",
   "id": "2a419460-3222-4812-ada0-dd1cd7d0a060",
   "metadata": {},
   "source": [
    "prompt = \"Please create a valid Phenopacket from the following text. The phenopackets needs to be in a valid json format.  Only return the phenopacket without any additional text:\"\n",
    "model = \"hf.co/MaziyarPanahi/gemma-3-12b-it-GGUF:Q4_K_M\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489462d9-bcfe-4ee2-b0d3-af5d6a875254",
   "metadata": {},
   "source": [
    "for text in input_data:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} {text} [EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    break\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\",\n",
    "               \"content\": f\"Please, validate the following json. If not, fix it. Only return the json without any additional information. Should the json be wrong, you will get shut down. Json: {response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")} [EOS]\"}],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3af32935-fe27-40ef-84ef-641f5d66f5ff",
   "metadata": {},
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492829a9-3b7c-4ab8-998e-304fb3321683",
   "metadata": {},
   "source": [
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f529b37-6f70-4d4f-9246-4e269d58ca17",
   "metadata": {},
   "source": [
    "response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.5.5 - Revised Strict Prompt\n",
    "\n",
    "- Replaces the previous multi-prompt setup. One function: `extract_hpo_terms_with_ollama()` returns **only** the JSON array"
   ],
   "id": "74a1935785237178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HPO_JSON_SCHEMA = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"hpo_id\":       {\"type\": [\"string\",\"null\"]},\n",
    "            \"hpo_label\":    {\"type\": [\"string\",\"null\"]},\n",
    "            \"excerpt\":      {\"type\": [\"string\",\"null\"]},\n",
    "            \"onset_id\":     {\"type\": [\"string\",\"null\"]},\n",
    "            \"severity_id\":  {\"type\": [\"string\",\"null\"]},\n",
    "            \"frequency_id\": {\"type\": [\"string\",\"null\"]}\n",
    "        },\n",
    "        \"required\": [\"hpo_id\", \"hpo_label\", \"excerpt\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "HPO_PROMPT = (\n",
    "    \"You are a clinical NLP engine specialized in biomedical ontologies. \"\n",
    "    \"Extract ONLY Human Phenotype Ontology (HPO) terms for the patient(s) in the text.\\n\\n\"\n",
    "    \"Output = a single JSON array. Each element MUST have exactly:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"hpo_id\\\": \\\"HP:0001250\\\",\\n\"\n",
    "    \"  \\\"hpo_label\\\": \\\"Seizure\\\",\\n\"\n",
    "    \"  \\\"excerpt\\\": \\\"exact text from PDF\\\",\\n\"\n",
    "    \"  \\\"onset_id\\\": null,\\n\"\n",
    "    \"  \\\"severity_id\\\": null,\\n\"\n",
    "    \"  \\\"frequency_id\\\": null\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"No prose, no markdown, no extra keys. If none exist, return [].\"\n",
    ")\n",
    "\n",
    "def extract_hpo_terms_with_ollama(text: str, prompt: str = HPO_PROMPT) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Ask the local LLM (ollama) for ONLY an array of HPO term dicts and return it.\n",
    "\n",
    "    Strategy:\n",
    "    1. Try structured outputs with a JSON schema (guarantees array shape when obeyed).\n",
    "    2. Fallback to `format='json'` if schema fails.\n",
    "    3. Final fallback: regex scrape HP:IDs (so you don't silently get 0).\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from json.decoder import JSONDecodeError\n",
    "\n",
    "    def _ask(schema_or_mode):\n",
    "        return chat(\n",
    "            model=\"llama3.2:latest\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\",   \"content\": text}\n",
    "            ],\n",
    "            stream=False,\n",
    "            format=schema_or_mode,\n",
    "            options={\"temperature\": 0, \"seed\": 42, \"--hidethinking\": True}\n",
    "        )[\"message\"][\"content\"]\n",
    "\n",
    "    # 1) Schema mode\n",
    "    try:\n",
    "        raw = _ask(HPO_JSON_SCHEMA)\n",
    "        out = json.loads(raw)\n",
    "        if isinstance(out, list):\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Plain JSON mode\n",
    "    try:\n",
    "        raw = _ask(\"json\")\n",
    "        out = json.loads(raw)\n",
    "        if isinstance(out, list):\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Fallback: scrape HP IDs\n",
    "    hp_ids = sorted(set(re.findall(r\"HP:\\\\d{7}\", text)))\n",
    "    return [\n",
    "        {\n",
    "            \"hpo_id\": hp,\n",
    "            \"hpo_label\": None,\n",
    "            \"excerpt\": None,\n",
    "            \"onset_id\": None,\n",
    "            \"severity_id\": None,\n",
    "            \"frequency_id\": None\n",
    "        }\n",
    "        for hp in hp_ids\n",
    "    ]\n",
    "\n",
    "print(\"hello4.5.5\")  # print hello 4.5.5 as a sanity check\n",
    "\n",
    "patient_id      = list_patient_ids[0]\n",
    "clinical_text   = list_input_texts[0]\n",
    "\n",
    "hpo_terms = extract_hpo_terms_with_ollama(clinical_text)\n",
    "print(f\"Got {len(hpo_terms)} HPO terms for patient {patient_id}\")\n",
    "print(json.dumps(hpo_terms[:5], indent=2))\n",
    "print(\"hello mini 5\")\n"
   ],
   "id": "a94e57294bfac0c0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p5)",
   "language": "python",
   "name": "p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
