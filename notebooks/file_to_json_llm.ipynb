{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Output Evaluation Notebook\n",
    "\n",
    "This notebook runs LLM inference to predict HPO terms, compares them to ground truth phenopackets, and produces a summary report.\n"
   ],
   "id": "187dea139446c775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 0) Imports, Path Discovery & Sanity Checks\n",
    "\n",
    "Load all dependencies, discover the dataset CSV automatically, and validate critical directories.\n"
   ],
   "id": "298b82d55c7dad0e"
  },
  {
   "cell_type": "code",
   "id": "98ccd760-19a5-48d0-b2c4-64063ecf29e2",
   "metadata": {},
   "source": [
    "# Basic Setup\n",
    "import sys, os, glob, json, subprocess, pickle, datetime, hashlib, warnings, random, requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from ollama import chat\n",
    "from docling.document_converter import DocumentConverter, ConversionError\n",
    "from pypdfium2._helpers.misc import PdfiumError\n",
    "from google.protobuf.json_format import ParseDict, ParseError\n",
    "from phenopackets import Phenopacket as ProtoPhenopacket\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# Need this at least once for some reason:\n",
    "# import .autonotebook\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "try:\n",
    "    from phenopacket import Phenopacket, InvalidPhenopacketError\n",
    "    from report import Report\n",
    "    from evaluation import PhenotypeEvaluator\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Could not import project utils: {e}\")\n",
    "\n",
    "# Make sure our utils folder is on PYTHONPATH\n",
    "project_root        = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder          = os.path.join(project_root, \"src\")\n",
    "utils_folder        = os.path.join(project_root, \"notebooks\", \"utils\")\n",
    "\n",
    "print(\"Project Start:       %s\" % project_root)\n",
    "print(\"Source Folder:       %s\" % src_folder)\n",
    "print(\"Utilities Folder:    %s\" % utils_folder)\n",
    "\n",
    "for path in (src_folder, utils_folder):\n",
    "    if not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"Expected folder on PYTHOPATH : {path}\")\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "print(\"PYTHONPATH patched with:\", src_folder, utils_folder)\n",
    "\n",
    "# define all key paths\n",
    "pdf_input_directory                 = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"pmid_pdfs\")            # scripts/data/tmp/phenopacket_store/pmid_pdfs/\n",
    "ground_truth_notebooks_directory    = os.path.join(src_folder, \"P5\", \"scripts\", \"data\",\"tmp\", \"phenopacket_store\",\"notebooks\")              # scripts/data/tmp/phenopacket_store/notebooks/\n",
    "dataset_csv_path                    = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\")\n",
    "\n",
    "# All experimental outputs go under here\n",
    "experimental_data_root              = os.path.join(project_root, \"experimental-data\")\n",
    "llm_output_directory                = os.path.join(experimental_data_root, \"llm_output_dir\")                                                # intermediate .txt + raw JSON from LLM\n",
    "validated_jsons_directory           = os.path.join(experimental_data_root, \"validated_jsons\")                                               # validated_jsons, the final validated LLM phenopackets\n",
    "evaluation_report_output_path       = os.path.join(project_root, \"reports\", \"first_report.json\")                                            # the evaluation metrics report\n",
    "\n",
    "# Create any missing output folders\n",
    "os.makedirs(pdf_input_directory, exist_ok=True)\n",
    "os.makedirs(ground_truth_notebooks_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(dataset_csv_path), exist_ok=True)\n",
    "os.makedirs(llm_output_directory, exist_ok=True)\n",
    "os.makedirs(validated_jsons_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(evaluation_report_output_path), exist_ok=True)\n",
    "\n",
    "# Create the PMIDs pickle file path\n",
    "pmid_pkl_path = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"pmids.pkl\")\n",
    "\n",
    "# TODO: Figure out why deleting the `ground_truth_notebooks_directory` after creating it works. Maybe because git doesn't let me just overwrite a directory with a clone request\n",
    "# Before the git pull operation\n",
    "import shutil\n",
    "\n",
    "# Clean up existing directory if it exists\n",
    "target_dir = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\")\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "\n",
    "# 1. Now run the git pull to clone the \"phenopacket-store\" GitHub repo into scripts/data/tmp/phenopacket_store\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.pull_git_files\",\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\"),\n",
    "    \"https://github.com/monarch-initiative/phenopacket-store.git\",\n",
    "    \"notebooks\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 1 Complete, Produced %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "# 2. Scan the just-pulled notebooks for PMID_##### files\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.create_pmid_pkl\",\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\"),\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"pmids.pkl\"),\n",
    "    \"--recursive_dir_search\",\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 2 Complete\")\n",
    "\n",
    "# 3. Download *all* PDFs for those PMIDs (0 = unlimited)\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.pmid_downloader\", pmid_pkl_path, pdf_input_directory, \"10\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 3 Complete\")\n",
    "\n",
    "# 4. Finally, build THE CSV mapping PDFs to the ground-truth JSONs\n",
    "if not os.path.isfile(dataset_csv_path):\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"P5.scripts.create_phenopacket_dataset\",\n",
    "        pdf_input_directory,\n",
    "        ground_truth_notebooks_directory,\n",
    "        dataset_csv_path,\n",
    "        \"--recursive_ground_truth_dir\", \"True\"\n",
    "    ], check=True)\n",
    "    print(f\"Created dataset CSV at {dataset_csv_path}\")\n",
    "\n",
    "    print(\"Stage 4 Complete\")\n",
    "\n",
    "    if not os.path.isdir(pdf_input_directory):\n",
    "        raise FileNotFoundError(\"PDF input directory not found: %s\" % pdf_input_directory)\n",
    "    if not os.path.isdir(ground_truth_notebooks_directory):\n",
    "        raise FileNotFoundError(\"Ground truth notebooks directory not found: %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "\n",
    "print(\"hello0\")  # print hello 0 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1) Load Dataset\n",
    "\n",
    "Read the CSV of PMIDs, input paths, and truth paths\n"
   ],
   "id": "dabf5f01d8e6a670"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:02:06.285916Z",
     "start_time": "2025-07-23T14:02:06.257568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "dataframe_cases = pd.read_csv(dataset_csv_path)\n",
    "print(f\"Loaded {len(dataframe_cases)} rows from dataset CSV\")\n",
    "# Load cases & deduplicate PMIDs, with start/end counts\n",
    "orig_count = len(dataframe_cases)\n",
    "print(f\"Before deduplication: {orig_count} total cases\")\n",
    "\n",
    "# Debug: verify that every `input` path actually exists\n",
    "print(\"Checking existence of input PDFs:\")\n",
    "for pdf_path in dataframe_cases[\"input\"]:\n",
    "    status = \"FOUND\" if os.path.isfile(pdf_path) else \"MISSING\"\n",
    "    print(f\"  o {pdf_path}: {status}\")\n",
    "\n",
    "# Drop duplicate PMIDs\n",
    "dataframe_cases = dataframe_cases.drop_duplicates(subset=\"pmid\", keep=\"first\").reset_index(drop=True) # This may be too aggressive and I need to check if this is a good approach\n",
    "removed = orig_count - len(dataframe_cases)\n",
    "print(f\"{removed} duplicates removed (now {len(dataframe_cases)} unique PMIDs)\")\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = {\"pmid\", \"input\", \"truth\"}\n",
    "missing_columns = required_columns - set(dataframe_cases.columns)\n",
    "if missing_columns:\n",
    "    raise KeyError(\"Missing required columns: %s\" % missing_columns)\n",
    "\n",
    "# Preview first few rows\n",
    "dataframe_cases.head()\n",
    "\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "print(\"hello1\")  # print hello 1 as a sanity check"
   ],
   "id": "598b33092ab85352",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 rows from dataset CSV\n",
      "Before deduplication: 7 total cases\n",
      "Checking existence of input PDFs:\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_28103835.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_28103835.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_11118249.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_11118249.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_11118249.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_11118249.pdf: FOUND\n",
      "  o /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_19883511.pdf: FOUND\n",
      "4 duplicates removed (now 3 unique PMIDs)\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n",
      "hello1\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2) Discover Phenopacket-Store Files\n",
    "\n",
    "Locate all ground-truth Phenopacket JSON files under the `phenopacket_store/notebooks/` directory."
   ],
   "id": "2a1755b2eb294368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:02:09.047Z",
     "start_time": "2025-07-23T14:02:08.971779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cache Integrity & Versioning\n",
    "CACHE_DIR = Path(experimental_data_root) / \"text_cache\"\n",
    "INDEX_FILE = CACHE_DIR / \"cache_index.json\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _hash_pdf(pdf_path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    h.update(pdf_path.read_bytes())\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_or_update_cache(pmids: list[str], pmid_to_pdf: dict[str, Path]):\n",
    "    if INDEX_FILE.exists():\n",
    "        index = json.loads(INDEX_FILE.read_text())\n",
    "    else:\n",
    "        index = {}\n",
    "    for pmid in pmids:\n",
    "        pdf = pmid_to_pdf[pmid]\n",
    "        digest = _hash_pdf(pdf)\n",
    "        if pmid not in index or index[pmid][\"hash\"] != digest:\n",
    "            text = extract_text_from_pdf(pdf)\n",
    "            with open(CACHE_DIR / f\"{pmid}.pkl\", \"wb\") as fh:\n",
    "                pickle.dump(text, fh)\n",
    "            index[pmid] = {\n",
    "                \"hash\": digest,\n",
    "                \"cached_at\": datetime.datetime.utcnow().isoformat()\n",
    "            }\n",
    "    INDEX_FILE.write_text(json.dumps(index, indent=2))\n",
    "\n",
    "\n",
    "# Finding all ground-truth phenopacket JSON files\n",
    "search_pattern = os.path.join(ground_truth_notebooks_directory, \"*\", \"phenopackets\", \"*.json\")\n",
    "truth_json_filepaths = glob.glob(search_pattern, recursive=True)\n",
    "if not truth_json_filepaths:\n",
    "    raise FileNotFoundError(f\"No ground-truth JSONs found at {search_pattern}\")\n",
    "\n",
    "print(\"Discovered %d ground-truth JSON files\" %len(truth_json_filepaths))\n",
    "\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "print(\"hello2\")  # print hello 2 as a sanity check"
   ],
   "id": "8118b55fc2741c2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 7969 ground-truth JSON files\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n",
      "hello2\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3) Prepare PDF-to-Text Converter\n",
    "\n",
    "- Randomly pick N unique PMIDs from the CSV (from Step 1) to keep runs fast and reproducible.\n",
    "- Instantiate DocumentConverter and define a helper function to load or convert the clinical PDFs for LLM input.\n",
    "- Setup Persistent PDF-to-Text Cache"
   ],
   "id": "9a9faff43b491b26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:02:12.031860Z",
     "start_time": "2025-07-23T14:02:12.007009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload the deduplicated CSV from Step 1\n",
    "full_df = pd.read_csv(dataset_csv_path).drop_duplicates(subset=\"pmid\").reset_index(drop=True)\n",
    "\n",
    "# Choose how many cases to sample\n",
    "N = 10\n",
    "# Don’t ask for more than exist\n",
    "N = min(N, len(full_df))\n",
    "subset_df = full_df.sample(n=N, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Sampling {N} PMIDs:\", subset_df[\"pmid\"].tolist())\n",
    "\n",
    "\n",
    "# Setup conversion for input material to LLM-compatible txt now\n",
    "\n",
    "# Initialize converter once\n",
    "pdf_to_text_converter = DocumentConverter()\n",
    "\n",
    "# Path to persistent cache of PDF text\n",
    "text_cache_path = os.path.join(experimental_data_root, \"text_cache.pkl\")\n",
    "# Load or initialize cache\n",
    "if os.path.exists(text_cache_path):\n",
    "    with open(text_cache_path, \"rb\") as f:\n",
    "        text_cache = pickle.load(f)\n",
    "    print(f\"Loaded text cache with {len(text_cache)} entries\")\n",
    "else:\n",
    "    text_cache = {}\n",
    "    print(\"Initialized empty text cache\")\n",
    "\n",
    "def load_clinical_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert .txt or .pdf file at \"pdf_path\" into a plain text string.\n",
    "    Use the in-memory cache first; write new text back to the cache only when the cache is explicitly saved at the end of the pipeline.\n",
    "    Raise FileNotFoundError or ConversionError if the file does not exist.\n",
    "    \"\"\"\n",
    "    # Return cache if it exists in memory\n",
    "    if pdf_path in text_cache:\n",
    "        return text_cache[pdf_path]\n",
    "\n",
    "    # Ensure the files exists before we continue\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        raise FileNotFoundError(\"Input file not found: %s\" % pdf_path)\n",
    "\n",
    "    # If it's already plain text, read and strip any header\n",
    "    if pdf_path.lower().endswith(\".txt\"):\n",
    "        content = open(pdf_path, encoding=\"utf-8\").read()\n",
    "        # Remove any leading markers\n",
    "        return content.split(\"[text]\")[-1]\n",
    "    else:\n",
    "        try:\n",
    "            # Convert PDF to text and handle conversion failures\n",
    "            doc = pdf_to_text_converter.convert(pdf_path)\n",
    "            content = doc.document.export_to_text()\n",
    "        except ConversionError as e:\n",
    "            raise ConversionError(f\"Could not convert {os.path.basename(pdf_path)}: {e}\")\n",
    "\n",
    "\n",
    "    # Save new text in memory and write updated cache to disk later\n",
    "    text_cache[pdf_path] = content\n",
    "    return content\n",
    "\n",
    "# Convert all PDFs in our sampled subset\n",
    "for pdf_path in subset_df[\"input\"]:\n",
    "    pdf_text = load_clinical_pdf(pdf_path)\n",
    "# Persist updated cache to disk\n",
    "with open(text_cache_path, \"wb\") as f:\n",
    "    pickle.dump(text_cache, f)\n",
    "\n",
    "print(f\"Saved text cache now with {len(text_cache)} entries\")\n",
    "\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "print(\"hello3\")  # print hello 3 as a sanity check"
   ],
   "id": "231c1c375a6ec410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 3 PMIDs: ['PMID_28103835', 'PMID_11118249', 'PMID_19883511']\n",
      "Loaded text cache with 3 entries\n",
      "Saved text cache now with 3 entries\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n",
      "hello3\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4) Load Clinical PDFs and Ground-Truth Phenopackets\n",
    "\n",
    "Iterate over each case, load the clinical PDF text and the corresponding ground-truth Phenopacket object.\n",
    "\n",
    "- `list_inputs_texts`: raw clinical PDFs\n",
    "- `list_truth_packets`: parsed Phenopacket objects from JSON files\n",
    "- `list_patient_ids`: PMID patient identifiers\n",
    "\n"
   ],
   "id": "cbc0b863-bf20-4a46-95de-9506e9875678"
  },
  {
   "cell_type": "code",
   "id": "235eaff7-4f21-4a21-81bf-e97236ca26d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:02:17.390460Z",
     "start_time": "2025-07-23T14:02:17.375137Z"
    }
   },
   "source": [
    "# Iterate over rows, should lookup only once\n",
    "list_input_texts    = []\n",
    "list_truth_packets  = []\n",
    "list_patient_ids    = []\n",
    "loaded_count = 0\n",
    "skipped_pdfs = []\n",
    "\n",
    "\n",
    "for case in dataframe_cases.itertuples(index=False):\n",
    "    pmid_value = case.pmid\n",
    "    pdf_path   = case.input\n",
    "    truth_path = case.truth\n",
    "\n",
    "    # Convert PDF to text\n",
    "    try:\n",
    "        clinical_text = load_clinical_pdf(pdf_path)\n",
    "    except (ConversionError, PdfiumError) as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"pdf\": pdf_path, \"reason\": f\"conversion error: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Load raw JSON and validate with ignore_unknown_fields\n",
    "    try:\n",
    "        raw_true_packet = json.load(open(truth_path, \"r\", encoding=\"utf-8\"))\n",
    "        proto = ProtoPhenopacket()\n",
    "        ParseDict(raw_true_packet, proto, ignore_unknown_fields=True)\n",
    "    except (ParseError, json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"truth\": truth_path, \"reason\": f\"schema parse error: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Wrap in util Phenopacket to ensure phenotypicFeatures exists\n",
    "    try:\n",
    "        truth_packet = Phenopacket(raw_true_packet)\n",
    "    except InvalidPhenopacketError as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"truth\": truth_path, \"reason\": f\"phenopacket invalid: {e}\"})\n",
    "        continue\n",
    "\n",
    "    list_input_texts.append(clinical_text)\n",
    "    list_truth_packets.append(truth_packet)\n",
    "    list_patient_ids.append(truth_packet.to_json()[\"subject\"][\"id\"])\n",
    "\n",
    "    loaded_count += 1\n",
    "    print(f\"Loaded {loaded_count} new cases, skipped {len(skipped_pdfs)} so far\")\n",
    "\n",
    "if not list_input_texts:\n",
    "        raise RuntimeError(\"No clinical texts were loaded, please check that the dataset CSV `input` paths match files in `pdf_input_directory`\")\n",
    "\n",
    "assert len(list_input_texts) == len(list_truth_packets) == len(list_patient_ids)\n",
    "print(\"Loaded %d clinical texts and %d ground-truth packets for %d unique patients\" % (len(list_input_texts), len(list_truth_packets), len(list_patient_ids)))\n",
    "\n",
    "print(\"hello4\")  # print hello 4 as a sanity check\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 new cases, skipped 0 so far\n",
      "Loaded 2 new cases, skipped 0 so far\n",
      "Loaded 3 new cases, skipped 0 so far\n",
      "Loaded 3 clinical texts and 3 ground-truth packets for 3 unique patients\n",
      "hello4\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4.5) Define LLM Prompts\n",
    "\n",
    "Create prompt for just HPO terms and another one for the full phenopacket extraction, as well as some additional helper functions for later/potential use\n"
   ],
   "id": "a5f9ab5a0c5a064d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:02:20.168487Z",
     "start_time": "2025-07-23T14:02:20.158365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Prompt for just HPO labels\n",
    "hpo_prompt = (\n",
    "    \"You are a clinical NLP engine specialized in biomedical ontologies. Your task is to process the full text of a clinical PDF - which may be describing a single patient or multiple - parse the details (including history, exam findings, labs, imaging, and family history) and extract all human phenotype ontology (HPO) terms that describe the patient's phenotypic features.\"\n",
    "    \"Instructions:\"\n",
    "    \"1. Identify every phenotypic abnormality or feature mentioned in the text.\"\n",
    "    \"2. For each feature, map it to the correct HPO identifier (e.g. 'HP:0001250'), label (e.g. 'Seizure'), and descriptor value (e.g. 'A seizure is an intermittent abnormality of nervous system physiology characterized by a transient occurrence of signs and/or symptoms due to abnormal excessive or synchronous neuronal activity in the brain.').\"\n",
    "    \"3. Capture relevant qualifiers when present:\"\n",
    "        \"- Onset: map to HPO onset terms (e.g. 'HP:0011463' for 'Childhood onset').\"\n",
    "        \"- Severity: map to HPO severity terms (e.g. 'HP:0012829' for 'Profound').\"\n",
    "        \"- Temporal pattern: include if specified (e.g. 'HP:0031796' for 'Recurrent', map to HPO frequency terms if available).\"\n",
    "    \"4. For each term, include the exact text excerpt where it appears.\"\n",
    "    \"5. Output exclusively a JSON array. Each element must be an object with the following fields:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'hpo_id': 'HP:000____',\"\n",
    "        \"'hpo_label': 'Term label',\"\n",
    "        \"'excerpt': 'Exact text from the PDF',\"\n",
    "        \"'onset_id': 'HP:0XXXXX or null',\"\n",
    "        \"'severity_id': 'HP:0XXXXX or null',\"\n",
    "        \"'frequency_id': 'HP:0XXXXX or null'\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"Do not include any explanatory text, only the JSON array.\"\n",
    "\n",
    "    \"Your output **MUST** be exactly a JSON array/object and nothing else.\"\n",
    "\n",
    "    \"If you cannot comply, output exactly: {'error': 'cannot extract JSON'}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 2) Prompt for full phenopacket\n",
    "full_pp_prompt = (\n",
    "    \"You are a biomedical data curation assistant. Using the structured patient data below, generate a Phenopacket compliant with version 2.0 of the GA4GH Phenopacket schema. Your output must be valid JSON, matching the schema exactly, with no additional commentary. Here are the minimum expected output criteria:\"\n",
    "\n",
    "    \"Inputs:\"\n",
    "    \"patient_id: '{{patient_id}}'\"\n",
    "    \"sex: '{{sex}}'              // 'male' or 'female'\"\n",
    "    \"age_years: {{age_in_years}} // integer\"\n",
    "    \"v  ital_status: '{{vital_status}}' // 'alive' or 'deceased'\"\n",
    "    \"phenotypic_features: {{phenotypic_features_json}} // JSON array from the HPO extraction prompt\"\n",
    "    \"diseases: {{diseases_json}}         // optional, array of disease objects with MONDO or OMIM IDs\"\n",
    "    \"measurements: {{measurements_json}} // optional, array of quantitative trait measurements\"\n",
    "    \"metadata: {\"\n",
    "        \"'created_by': '{{your_name_or_tool}}',\"\n",
    "        \"'created_on': '{{YYYY-MM-DD}}'\"\n",
    "    \"}\"\n",
    "\n",
    "    \"Requirements:\"\n",
    "    \"Top-level fields:\"\n",
    "    \"'id': patient_id\"\n",
    "    \"'subject': object with:\"\n",
    "        \"'id': patient_id\"\n",
    "        \"'sex': { 'id': 'PATO:0000383' or 'PATO:0000384', 'label': sex }\"\n",
    "        \"'ageAtLastEncounter': { 'age': { 'years': age_years } }\"\n",
    "        \"'vitalStatus': { 'value': vital_status }\"\n",
    "        \"'phenotypicFeatures': use the phenotypic_features input; for each feature, map:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'type': { 'id': hpo_id, 'label': hpo_label },\"\n",
    "        \"'negated': false,\"\n",
    "        \"'onset': { 'term': { 'id': onset_id, 'label': (look up label) } },\"\n",
    "        \"'severity': { 'term': { 'id': severity_id, 'label': (look up label) } },\"\n",
    "        \"'frequency': { 'term': { 'id': frequency_id, 'label': (look up label) } }\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"Include 'diseases' and 'measurements' only if provided, following the GA4GH schema.\"\n",
    "    \"'metadata' must include:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'phenopacketSchemaVersion': '2.0.0',\"\n",
    "        \"'created': '{{YYYY-MM-DD}}',\"\n",
    "        \"'createdBy': '{{your_name_or_tool}}'\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"'Do not add any extra fields. Output must be purely the JSON object.'\"\n",
    "\n",
    "    \"Do not include any explanatory text, only the JSON array.\"\n",
    "\n",
    "    \"Your output **MUST** be exactly a JSON array/object and nothing else.\"\n",
    "\n",
    "    \"If you cannot comply, output exactly: {'error': 'cannot extract JSON'}\"\n",
    ")\n",
    "\n",
    "\n",
    "# JSON extraction + phenopacket builder helpers\n",
    "def slice_json_array(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the first top-level JSON array substring from raw_text.\n",
    "    Raise RuntimeError if none found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\[.*\\]\", raw_text, re.S)\n",
    "    if not match:\n",
    "        raise RuntimeError(f\"No JSON array found in model output:\\n{raw_text[:800]}\")\n",
    "    return match.group(0)\n",
    "\n",
    "\n",
    "def parse_hpo_array(array_text: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse JSON array text into a python list; raise informative errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = json.loads(array_text)\n",
    "    except JSONDecodeError as e:\n",
    "        raise ValueError(f\"JSON decode error: {e}\\n\\n{array_text[:800]}\")\n",
    "    if not isinstance(obj, list):\n",
    "        raise TypeError(\"Expected a JSON array.\")\n",
    "    return obj\n",
    "\n",
    "\n",
    "def build_phenopacket_from_hpo_list(patient_id: str, hpo_list: list[dict], sex_id: str | None = None, age_years: int | None = None, vital_status: str | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Map simple HPO term dicts into a GA4GH Phenopacket v2 JSON dict.\n",
    "    Fill unknown qualifiers with None. Extend as needed.\n",
    "    \"\"\"\n",
    "    def _mk_term(term_id, label):\n",
    "        if term_id is None:\n",
    "            return None\n",
    "        return {\"id\": term_id, \"label\": label}\n",
    "\n",
    "    phenotypic_features = []\n",
    "    for term in hpo_list:\n",
    "        hpo_id      = term.get(\"hpo_id\")\n",
    "        hpo_label   = term.get(\"hpo_label\")\n",
    "        onset_id    = term.get(\"onset_id\")\n",
    "        severity_id = term.get(\"severity_id\")\n",
    "        freq_id = term.get(\"frequency_id\")\n",
    "\n",
    "        feature = {\"type\": _mk_term(hpo_id, hpo_label), \"negated\": False}\n",
    "        if onset_id:\n",
    "            feature[\"onset\"] = {\"term\": {\"id\": onset_id}}\n",
    "        if severity_id:\n",
    "            feature[\"severity\"] = {\"term\": {\"id\": severity_id}}\n",
    "        if freq_id:\n",
    "            feature[\"frequency\"] = {\"term\": {\"id\": freq_id}}\n",
    "        phenotypic_features.append(feature)\n",
    "\n",
    "    subject_obj = {\"id\": patient_id}\n",
    "    if sex_id:\n",
    "        subject_obj[\"sex\"] = {\"id\": sex_id}\n",
    "    if age_years is not None:\n",
    "        subject_obj[\"ageAtLastEncounter\"] = {\"age\": {\"years\": int(age_years)}}\n",
    "    if vital_status:\n",
    "        subject_obj[\"vitalStatus\"] = {\"value\": vital_status}\n",
    "\n",
    "    pkt = {\n",
    "        \"id\": patient_id,\n",
    "        \"subject\": subject_obj,\n",
    "        \"phenotypicFeatures\": phenotypic_features,\n",
    "        \"metaData\": {\n",
    "            \"created\": datetime.date.today().isoformat(),\n",
    "            \"createdBy\": \"Varenya-LLM-Pipeline\",\n",
    "            \"phenopacketSchemaVersion\": \"2.0.0\"\n",
    "        }\n",
    "    }\n",
    "    return pkt\n",
    "\n",
    "print(\"hello my little utils\")\n",
    "print(\"hello4.5\")  # print hello 4.5 as a sanity check"
   ],
   "id": "6f5d51a89657db88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my little utils\n",
      "hello4.5\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5. Sanity-check one inference\n",
    "\n",
    "Run one LLM call on the first case to verify prompting and parsing work correctly.\n"
   ],
   "id": "8f03843785823cdc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:03:44.122800Z",
     "start_time": "2025-07-23T14:03:23.224801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pick out the first patient/example\n",
    "patient_id      = list_patient_ids[0]\n",
    "clinical_text   = list_input_texts[0]\n",
    "# truth_packet    = list_truth_packets[0]\n",
    "\n",
    "# 1) Inference: ask for *only* the JSON array of HPO term objects for the first clinical PDF\n",
    "# build a strict system+user conversation\n",
    "messages = [{\"role\": \"system\", \"content\": ( hpo_prompt + \"\\n\\nYour only output must be a **valid** JSON array of  HPO term objects\" + \"with fields 'hpo_id','hpo_label','excerpt',\" + \"'onset_id','severity_id','frequency_id', and nothing else.\")}, {\"role\": \"user\", \"content\": clinical_text}]\n",
    "\n",
    "hpo_response = chat(model=\"llama3.2:latest\", messages=messages, options={\"--hidethinking\": True})\n",
    "\n",
    "# 2) Grab the raw string\n",
    "raw_hpo_output = hpo_response[\"message\"][\"content\"]\n",
    "print(\"Raw LLM output (truncated to the first ~300 chars or so):\")\n",
    "print(raw_hpo_output[:300], \"...\\n\")\n",
    "\n",
    "# 3) Slice out the JSON array\n",
    "start = raw_hpo_output.find(\"[\")\n",
    "end = raw_hpo_output.rfind(\"]\")\n",
    "if start < 0 or end < 0:\n",
    "    # print the raw output to debug what the model actually sent\n",
    "    print(\"===== RAW HPO OUTPUT =====\\n\", raw_hpo_output)\n",
    "    raise RuntimeError(f\"Could not locate a JSON array in HPO output for patient {patient_id}:\\n{raw_hpo_output}\")\n",
    "# grab *only* the array text\n",
    "hpo_json_array = raw_hpo_output[start : end+1]\n",
    "\n",
    "# Parse and validate\n",
    "try:\n",
    "    # hpo_terms = Phenopacket(json.loads(hpo_json_array))\n",
    "    hpo_terms = json.loads(hpo_json_array)  # Try not using Phenopacket(...)\n",
    "    print(f\"Parsed {len(hpo_terms)} HPO term(s) for patient {patient_id}\")\n",
    "except JSONDecodeError as error:\n",
    "    raise ValueError(\n",
    "        f\"Failed to parse HPO JSON array for patient {patient_id}: {error}\\n\\n\"\n",
    "        f\"Extracted JSON was:\\n{hpo_json_array}\\n\\n\"\n",
    "        f\"Full raw output was:\\n{raw_hpo_output}\"\n",
    "    )\n",
    "\n",
    "print(\"hello5\")  # print hello 5 as a sanity check"
   ],
   "id": "1f547b47f98fc0b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLM output (truncated to the first ~300 chars or so):\n",
      "Here is a summary of the article in a neutral tone:\n",
      "\n",
      "A case study was presented on a patient with Aarskog syndrome (AS), also known as faciogenital dysplasia, caused by mutations in the FGD1 gene. The patient had developed symptoms such as aggression, hyperactivity, and developmental delays. Molecul ...\n",
      "\n",
      "===== RAW HPO OUTPUT =====\n",
      " Here is a summary of the article in a neutral tone:\n",
      "\n",
      "A case study was presented on a patient with Aarskog syndrome (AS), also known as faciogenital dysplasia, caused by mutations in the FGD1 gene. The patient had developed symptoms such as aggression, hyperactivity, and developmental delays. Molecular analysis revealed a novel mutation in the FGD1 gene.\n",
      "\n",
      "The study highlights the importance of genetic testing for AS diagnosis and the clinical variability observed among patients with the same mutation. It also notes that recurrent mutations are rare compared to new ones.\n",
      "\n",
      "A total of 46 patients with AS were analyzed, including this patient, and showed variable symptoms despite having the same genetic mutation. The authors suggest that a more comprehensive understanding of the gene's function is needed to explain the clinical variability in these cases.\n",
      "\n",
      "The study contributes to our knowledge of AS and its genetic basis, providing insights into the importance of molecular analysis for diagnosis and prognosis.\n",
      "\n",
      "References:\n",
      "\n",
      "1. Orrico A, Galli L, Cavaliere ML, Garavelli L, Fryns JP, Crushell E, Rinaldi MM, Ahmad A. Phenotypic and molecular characterisation of the Aarskog-Scott syndrome: a survey of the clinical variability in light of FGD1 mutation analysis in 46 patients. Eur J Hum Genet. 2004;12(1):16–23.\n",
      "\n",
      "2. Genot E, Daubon T, Sorrentino V, Buccione R. FGD1 as a central regulator of extracellular matrix remodelling –lessons from faciogenital dysplasia. J Cell Sci. 2012;125(Pt 14):3265–70.\n",
      "\n",
      "3. Gorski JL, Estrada L, Hu C, Liu Z. Skeletal-specific expression of Fgd1 during bone formation and skeletal defects in faciogenital dysplasia (FGDY; Aarskog syndrome). Dev Dyn. 2000;218(4):573–86.\n",
      "\n",
      "4. Orrico A, Galli L, Clayton-Smith J, Fryns JP, Buccione R. Clinical utility gene card for: AarskogScott syndrome (faciogenital dysplasia). Eur J Hum Genet. 2011;19(11).\n",
      "\n",
      "5. Teebi AS, Rucquoi JK, Meyn MS. Aarskog syndrome: report of a family with review and discussion of nosology. Am J Med Genet. 1993;46(5):501–9.\n",
      "\n",
      "6. Perez-Coria M, Lugo-Trampe JJ, Zamudio-Osuna M, Rodriguez-Sanchez IP, Lugo-Trampe A, de la Fuente-Cortez B, Campos-Acevedo LD, Martinez-deVillarreal LE. Identification of novel mutations in Mexican patients with Aarskog-Scott syndrome. Mol Genet Genomic Med. 2015;3(3):197–202.\n",
      "\n",
      "7. Kircher M, Witten DM, Jain P, O'Rourke BJ, Cooper GM, Shendure J. A general framework for estimating the relative pathogenicity of human genetic variants. Nat Genet. 2014;46(3):310–5.\n",
      "\n",
      "8. Bedoyan JK, Friez MJ, DuPont B, Ahmad A. First case of deletion of the faciogenital dysplasia 1 (FGD1) gene in a patient with Aarskog-Scott syndrome. Eur J Med Genet. 2009;52(4):262–4.\n",
      "\n",
      "9. Orrico A, Galli L, Faivre L, Clayton-Smith J, Azzarello-Burri SM, Hertz JM, Jacquemont S, Taurisano R, Arroyo Carrera I, Tarantino E, et al. Aarskog-Scott syndrome: clinical update and report of nine novel mutations of the FGD1 gene. Am J Med Genet A. 2010;152A(2):313–8.\n",
      "\n",
      "10. Lebel RR, May M, Pouls S, Lubs HA, Stevenson RE, Schwartz CE. Nonsyndromic X-linked mental retardation associated with a missense mutation (P312L) in the FGD1 gene. Clin Genet. 2002;61(2):139–45.\n",
      "\n",
      "11. Kaname T, Yanagi K, Okamoto N, Naritomi K. Neurobehavioral disorders in patients with Aarskog-Scott syndrome affected by novel FGD1 mutations. Am J Med Genet A. 2006;140(12):1331–2.\n",
      "\n",
      "12. Al-Semari A, Wakil SM, Al-Muhaizea MA, Dababo M, Al-Amr R, Alkuraya F, Meyer BF. Novel FGD1 mutation underlying Aarskog-Scott syndrome with myopathy and distal arthropathy. Clin Dysmorphol. 2013;22(1):13–7.\n",
      "\n",
      "13. Shalev SA, Chervinski E, Weiner E, Mazor G, Friez MJ, Schwartz CE. Clinical variation of Aarskog syndrome in a large family with 2189delA in the FGD1 gene. Am J Med Genet A. 2006;140(2):162–5.\n",
      "\n",
      "Please note that this summary is not intended to be an exhaustive analysis or interpretation of the article, but rather a neutral presentation of its contents.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not locate a JSON array in HPO output for patient proband IV-3:\nHere is a summary of the article in a neutral tone:\n\nA case study was presented on a patient with Aarskog syndrome (AS), also known as faciogenital dysplasia, caused by mutations in the FGD1 gene. The patient had developed symptoms such as aggression, hyperactivity, and developmental delays. Molecular analysis revealed a novel mutation in the FGD1 gene.\n\nThe study highlights the importance of genetic testing for AS diagnosis and the clinical variability observed among patients with the same mutation. It also notes that recurrent mutations are rare compared to new ones.\n\nA total of 46 patients with AS were analyzed, including this patient, and showed variable symptoms despite having the same genetic mutation. The authors suggest that a more comprehensive understanding of the gene's function is needed to explain the clinical variability in these cases.\n\nThe study contributes to our knowledge of AS and its genetic basis, providing insights into the importance of molecular analysis for diagnosis and prognosis.\n\nReferences:\n\n1. Orrico A, Galli L, Cavaliere ML, Garavelli L, Fryns JP, Crushell E, Rinaldi MM, Ahmad A. Phenotypic and molecular characterisation of the Aarskog-Scott syndrome: a survey of the clinical variability in light of FGD1 mutation analysis in 46 patients. Eur J Hum Genet. 2004;12(1):16–23.\n\n2. Genot E, Daubon T, Sorrentino V, Buccione R. FGD1 as a central regulator of extracellular matrix remodelling –lessons from faciogenital dysplasia. J Cell Sci. 2012;125(Pt 14):3265–70.\n\n3. Gorski JL, Estrada L, Hu C, Liu Z. Skeletal-specific expression of Fgd1 during bone formation and skeletal defects in faciogenital dysplasia (FGDY; Aarskog syndrome). Dev Dyn. 2000;218(4):573–86.\n\n4. Orrico A, Galli L, Clayton-Smith J, Fryns JP, Buccione R. Clinical utility gene card for: AarskogScott syndrome (faciogenital dysplasia). Eur J Hum Genet. 2011;19(11).\n\n5. Teebi AS, Rucquoi JK, Meyn MS. Aarskog syndrome: report of a family with review and discussion of nosology. Am J Med Genet. 1993;46(5):501–9.\n\n6. Perez-Coria M, Lugo-Trampe JJ, Zamudio-Osuna M, Rodriguez-Sanchez IP, Lugo-Trampe A, de la Fuente-Cortez B, Campos-Acevedo LD, Martinez-deVillarreal LE. Identification of novel mutations in Mexican patients with Aarskog-Scott syndrome. Mol Genet Genomic Med. 2015;3(3):197–202.\n\n7. Kircher M, Witten DM, Jain P, O'Rourke BJ, Cooper GM, Shendure J. A general framework for estimating the relative pathogenicity of human genetic variants. Nat Genet. 2014;46(3):310–5.\n\n8. Bedoyan JK, Friez MJ, DuPont B, Ahmad A. First case of deletion of the faciogenital dysplasia 1 (FGD1) gene in a patient with Aarskog-Scott syndrome. Eur J Med Genet. 2009;52(4):262–4.\n\n9. Orrico A, Galli L, Faivre L, Clayton-Smith J, Azzarello-Burri SM, Hertz JM, Jacquemont S, Taurisano R, Arroyo Carrera I, Tarantino E, et al. Aarskog-Scott syndrome: clinical update and report of nine novel mutations of the FGD1 gene. Am J Med Genet A. 2010;152A(2):313–8.\n\n10. Lebel RR, May M, Pouls S, Lubs HA, Stevenson RE, Schwartz CE. Nonsyndromic X-linked mental retardation associated with a missense mutation (P312L) in the FGD1 gene. Clin Genet. 2002;61(2):139–45.\n\n11. Kaname T, Yanagi K, Okamoto N, Naritomi K. Neurobehavioral disorders in patients with Aarskog-Scott syndrome affected by novel FGD1 mutations. Am J Med Genet A. 2006;140(12):1331–2.\n\n12. Al-Semari A, Wakil SM, Al-Muhaizea MA, Dababo M, Al-Amr R, Alkuraya F, Meyer BF. Novel FGD1 mutation underlying Aarskog-Scott syndrome with myopathy and distal arthropathy. Clin Dysmorphol. 2013;22(1):13–7.\n\n13. Shalev SA, Chervinski E, Weiner E, Mazor G, Friez MJ, Schwartz CE. Clinical variation of Aarskog syndrome in a large family with 2189delA in the FGD1 gene. Am J Med Genet A. 2006;140(2):162–5.\n\nPlease note that this summary is not intended to be an exhaustive analysis or interpretation of the article, but rather a neutral presentation of its contents.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[158]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m start < \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m end < \u001B[32m0\u001B[39m:\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# print the raw output to debug what the model actually sent\u001B[39;00m\n\u001B[32m     22\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m===== RAW HPO OUTPUT =====\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, raw_hpo_output)\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCould not locate a JSON array in HPO output for patient \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpatient_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mraw_hpo_output\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     24\u001B[39m \u001B[38;5;66;03m# grab *only* the array text\u001B[39;00m\n\u001B[32m     25\u001B[39m hpo_json_array = raw_hpo_output[start : end+\u001B[32m1\u001B[39m]\n",
      "\u001B[31mRuntimeError\u001B[39m: Could not locate a JSON array in HPO output for patient proband IV-3:\nHere is a summary of the article in a neutral tone:\n\nA case study was presented on a patient with Aarskog syndrome (AS), also known as faciogenital dysplasia, caused by mutations in the FGD1 gene. The patient had developed symptoms such as aggression, hyperactivity, and developmental delays. Molecular analysis revealed a novel mutation in the FGD1 gene.\n\nThe study highlights the importance of genetic testing for AS diagnosis and the clinical variability observed among patients with the same mutation. It also notes that recurrent mutations are rare compared to new ones.\n\nA total of 46 patients with AS were analyzed, including this patient, and showed variable symptoms despite having the same genetic mutation. The authors suggest that a more comprehensive understanding of the gene's function is needed to explain the clinical variability in these cases.\n\nThe study contributes to our knowledge of AS and its genetic basis, providing insights into the importance of molecular analysis for diagnosis and prognosis.\n\nReferences:\n\n1. Orrico A, Galli L, Cavaliere ML, Garavelli L, Fryns JP, Crushell E, Rinaldi MM, Ahmad A. Phenotypic and molecular characterisation of the Aarskog-Scott syndrome: a survey of the clinical variability in light of FGD1 mutation analysis in 46 patients. Eur J Hum Genet. 2004;12(1):16–23.\n\n2. Genot E, Daubon T, Sorrentino V, Buccione R. FGD1 as a central regulator of extracellular matrix remodelling –lessons from faciogenital dysplasia. J Cell Sci. 2012;125(Pt 14):3265–70.\n\n3. Gorski JL, Estrada L, Hu C, Liu Z. Skeletal-specific expression of Fgd1 during bone formation and skeletal defects in faciogenital dysplasia (FGDY; Aarskog syndrome). Dev Dyn. 2000;218(4):573–86.\n\n4. Orrico A, Galli L, Clayton-Smith J, Fryns JP, Buccione R. Clinical utility gene card for: AarskogScott syndrome (faciogenital dysplasia). Eur J Hum Genet. 2011;19(11).\n\n5. Teebi AS, Rucquoi JK, Meyn MS. Aarskog syndrome: report of a family with review and discussion of nosology. Am J Med Genet. 1993;46(5):501–9.\n\n6. Perez-Coria M, Lugo-Trampe JJ, Zamudio-Osuna M, Rodriguez-Sanchez IP, Lugo-Trampe A, de la Fuente-Cortez B, Campos-Acevedo LD, Martinez-deVillarreal LE. Identification of novel mutations in Mexican patients with Aarskog-Scott syndrome. Mol Genet Genomic Med. 2015;3(3):197–202.\n\n7. Kircher M, Witten DM, Jain P, O'Rourke BJ, Cooper GM, Shendure J. A general framework for estimating the relative pathogenicity of human genetic variants. Nat Genet. 2014;46(3):310–5.\n\n8. Bedoyan JK, Friez MJ, DuPont B, Ahmad A. First case of deletion of the faciogenital dysplasia 1 (FGD1) gene in a patient with Aarskog-Scott syndrome. Eur J Med Genet. 2009;52(4):262–4.\n\n9. Orrico A, Galli L, Faivre L, Clayton-Smith J, Azzarello-Burri SM, Hertz JM, Jacquemont S, Taurisano R, Arroyo Carrera I, Tarantino E, et al. Aarskog-Scott syndrome: clinical update and report of nine novel mutations of the FGD1 gene. Am J Med Genet A. 2010;152A(2):313–8.\n\n10. Lebel RR, May M, Pouls S, Lubs HA, Stevenson RE, Schwartz CE. Nonsyndromic X-linked mental retardation associated with a missense mutation (P312L) in the FGD1 gene. Clin Genet. 2002;61(2):139–45.\n\n11. Kaname T, Yanagi K, Okamoto N, Naritomi K. Neurobehavioral disorders in patients with Aarskog-Scott syndrome affected by novel FGD1 mutations. Am J Med Genet A. 2006;140(12):1331–2.\n\n12. Al-Semari A, Wakil SM, Al-Muhaizea MA, Dababo M, Al-Amr R, Alkuraya F, Meyer BF. Novel FGD1 mutation underlying Aarskog-Scott syndrome with myopathy and distal arthropathy. Clin Dysmorphol. 2013;22(1):13–7.\n\n13. Shalev SA, Chervinski E, Weiner E, Mazor G, Friez MJ, Schwartz CE. Clinical variation of Aarskog syndrome in a large family with 2189delA in the FGD1 gene. Am J Med Genet A. 2006;140(2):162–5.\n\nPlease note that this summary is not intended to be an exhaustive analysis or interpretation of the article, but rather a neutral presentation of its contents."
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6. Batch Inference and Save Validated Phenopackets\n",
    "\n",
    "Loop over all cases, run LLM inference, validate each JSON as a Phenopacket, and save to disk under validated_jsons_directory.\n"
   ],
   "id": "d07ca4ef808a00ad"
  },
  {
   "cell_type": "code",
   "id": "8943a066-1f8c-4542-86fe-b7c62bd07092",
   "metadata": {},
   "source": [
    "predicted_packets: List[Phenopacket] = []\n",
    "\n",
    "# Which patient are we targeting?\n",
    "for idx, clinical_text in enumerate(list_input_texts):\n",
    "    pmid_value = dataframe_cases.loc[idx, \"pmid\"]\n",
    "    patient_id = list_patient_ids[idx]\n",
    "    # Prompt the LLM to extract only that patient's HPO terms\n",
    "    content = (hpo_prompt + f\"\\n\\n*Extract only the HPO terms for patient* `{patient_id}` *in this clinical PDF.*\\n\\n\" + clinical_text + \"\\n\\n[EOS]\")\n",
    "    response = chat(model=\"llama3.2:latest\", messages=[{\"role\": \"user\", \"content\": content}], options={\"--hidethinking\": True})\n",
    "    llm_content = response[\"message\"][\"content\"].splitlines()\n",
    "    # Parse the JSON into a Phenopacket\n",
    "    try:\n",
    "        phenopacket_pred = Phenopacket(json.loads(\"\\n\".join(llm_content)))\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\"[Case %d, PMID %s] Invalid Phenopacket JSON: %s\" % (idx, pmid_value, error))\n",
    "\n",
    "    predicted_packets.append(phenopacket_pred)\n",
    "\n",
    "    # Write the predicted JSON to disk\n",
    "    output_filename = f\"{pmid_value}_{patient_id}.json\"\n",
    "    output_filepath = os.path.join(validated_jsons_directory, output_filename)\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(phenopacket_pred.to_json(), out_f, indent=2)\n",
    "    print(\"Saved predicted phenopacket for PMID/Patient %s/%s to %s\"\n",
    "          % (pmid_value, patient_id, output_filepath))\n",
    "\n",
    "if len(predicted_packets) != len(list_input_texts):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of inputs.\")\n",
    "# Maybe change to this: 'assert len(predicted_packets) == len(list_input_texts), \"Mismatch predictions vs inputs\"'\n",
    "\n",
    "print(f\"Generated {len(predicted_packets)} predicted phenopackets.\")\n",
    "\n",
    "print(\"hello6\")  # print hello 6 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7. Evaluate Predicted Phenopackets Against Ground Truth\n",
    "\n",
    "Compare each predicted phenopacket to its ground truth using PhenotypeEvaluator, then generate a Report object with overall metrics.\n"
   ],
   "id": "80f71065a95697e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Monkey-patch a convenience method onto PhenotypeEvaluator\n",
    "def _evaluate_batch(\n",
    "    self,\n",
    "    list_truth_packets,\n",
    "    list_predicted_packets,\n",
    "    creator,\n",
    "    experiment,\n",
    "    model,\n",
    "    zero_division=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Run check_phenotypes over all truth/pred pairs, then return\n",
    "    a plain-dict report containing confusion_matrix, metrics,\n",
    "    classification_report, and metadata.\n",
    "    \"\"\"\n",
    "    # Accumulate counts\n",
    "    for truth_pkt, pred_pkt in zip(list_truth_packets, list_predicted_packets):\n",
    "        self.check_phenotypes(\n",
    "            experimentally_extracted_phenotypes=pred_pkt.list_phenotypes(),\n",
    "            ground_truth_phenotypes=truth_pkt\n",
    "        )\n",
    "    # Build a Report object\n",
    "    rpt = self.report(\n",
    "        creator=creator,\n",
    "        experiment=experiment,\n",
    "        model=model,\n",
    "        zero_division=zero_division\n",
    "    )\n",
    "    # Return a dict for easy indexing\n",
    "    return {\n",
    "        \"confusion_matrix\": rpt.confusion_matrix,\n",
    "        \"metrics\": rpt.metrics,\n",
    "        \"classification_report\": rpt.classification_report,\n",
    "        \"metadata\": rpt.metadata,\n",
    "    }\n",
    "\n",
    "# Attach to the class\n",
    "PhenotypeEvaluator.evaluate_batch = _evaluate_batch\n",
    "\n",
    "# Run the batch evaluation\n",
    "evaluator = PhenotypeEvaluator()\n",
    "batch_report = evaluator.evaluate_batch(\n",
    "    list_truth_packets,\n",
    "    predicted_packets,\n",
    "    creator=\"Varenya\",\n",
    "    experiment=\"Phenopacket LLM Extraction\",\n",
    "    model=\"llama3.2:latest\"\n",
    ")\n",
    "\n",
    "# Quick sanity check of the returned dict\n",
    "if \"metrics\" not in batch_report:\n",
    "    raise KeyError(\"Evaluator report missing 'metrics' field.\")\n",
    "\n",
    "# Pretty-print the report dict\n",
    "import pprint\n",
    "pprint.pprint(batch_report)\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check#"
   ],
   "id": "ebbdc93ecf8aa29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Old Save first report\n",
    "\n",
    "Write the JSON report to disk for later analysis.\n"
   ],
   "id": "e664e040942126c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(evaluation_report_output_path)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(evaluation_report_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(batch_report, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation report to {evaluation_report_output_path}\")\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check"
   ],
   "id": "70624f16cfb2b75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c646976a-9ff5-4ab4-8eb1-6719449811d3",
   "metadata": {},
   "source": "# Old Inference Implementation"
  },
  {
   "cell_type": "code",
   "id": "2a419460-3222-4812-ada0-dd1cd7d0a060",
   "metadata": {},
   "source": [
    "prompt = \"Please create a valid Phenopacket from the following text. The phenopackets needs to be in a valid json format.  Only return the phenopacket without any additional text:\"\n",
    "model = \"hf.co/MaziyarPanahi/gemma-3-12b-it-GGUF:Q4_K_M\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489462d9-bcfe-4ee2-b0d3-af5d6a875254",
   "metadata": {},
   "source": [
    "for text in input_data:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} {text} [EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    break\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\",\n",
    "               \"content\": f\"Please, validate the following json. If not, fix it. Only return the json without any additional information. Should the json be wrong, you will get shut down. Json: {response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")} [EOS]\"}],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3af32935-fe27-40ef-84ef-641f5d66f5ff",
   "metadata": {},
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492829a9-3b7c-4ab8-998e-304fb3321683",
   "metadata": {},
   "source": [
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f529b37-6f70-4d4f-9246-4e269d58ca17",
   "metadata": {},
   "source": [
    "response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.5.5 - Revised Strict Prompt\n",
    "\n",
    "- Replaces the previous multi-prompt setup. One function: `extract_hpo_terms_with_ollama()` returns **only** the JSON array"
   ],
   "id": "74a1935785237178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "HPO_JSON_SCHEMA = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"hpo_id\":       {\"type\": [\"string\",\"null\"]},\n",
    "            \"hpo_label\":    {\"type\": [\"string\",\"null\"]},\n",
    "            \"excerpt\":      {\"type\": [\"string\",\"null\"]},\n",
    "            \"onset_id\":     {\"type\": [\"string\",\"null\"]},\n",
    "            \"severity_id\":  {\"type\": [\"string\",\"null\"]},\n",
    "            \"frequency_id\": {\"type\": [\"string\",\"null\"]}\n",
    "        },\n",
    "        \"required\": [\"hpo_id\", \"hpo_label\", \"excerpt\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "HPO_PROMPT = (\n",
    "    \"You are a clinical NLP engine specialized in biomedical ontologies. \"\n",
    "    \"Extract ONLY Human Phenotype Ontology (HPO) terms for the patient(s) in the text.\\n\\n\"\n",
    "    \"Output = a single JSON array. Each element MUST have exactly:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"hpo_id\\\": \\\"HP:0001250\\\",\\n\"\n",
    "    \"  \\\"hpo_label\\\": \\\"Seizure\\\",\\n\"\n",
    "    \"  \\\"excerpt\\\": \\\"exact text from PDF\\\",\\n\"\n",
    "    \"  \\\"onset_id\\\": null,\\n\"\n",
    "    \"  \\\"severity_id\\\": null,\\n\"\n",
    "    \"  \\\"frequency_id\\\": null\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"No prose, no markdown, no extra keys. If none exist, return [].\"\n",
    ")\n",
    "\n",
    "def extract_hpo_terms_with_ollama(text: str, prompt: str = HPO_PROMPT) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Ask the local LLM (ollama) for ONLY an array of HPO term dicts and return it.\n",
    "\n",
    "    Strategy:\n",
    "    1. Try structured outputs with a JSON schema (guarantees array shape when obeyed).\n",
    "    2. Fallback to `format='json'` if schema fails.\n",
    "    3. Final fallback: regex scrape HP:IDs (so you don't silently get 0).\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from json.decoder import JSONDecodeError\n",
    "\n",
    "    def _ask(schema_or_mode):\n",
    "        return chat(\n",
    "            model=\"llama3.2:latest\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\",   \"content\": text}\n",
    "            ],\n",
    "            stream=False,\n",
    "            format=schema_or_mode,\n",
    "            options={\"temperature\": 0, \"seed\": 42, \"--hidethinking\": True}\n",
    "        )[\"message\"][\"content\"]\n",
    "\n",
    "    # 1) Schema mode\n",
    "    try:\n",
    "        raw = _ask(HPO_JSON_SCHEMA)\n",
    "        out = json.loads(raw)\n",
    "        if isinstance(out, list):\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Plain JSON mode\n",
    "    try:\n",
    "        raw = _ask(\"json\")\n",
    "        out = json.loads(raw)\n",
    "        if isinstance(out, list):\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Fallback: scrape HP IDs\n",
    "    hp_ids = sorted(set(re.findall(r\"HP:\\\\d{7}\", text)))\n",
    "    return [\n",
    "        {\n",
    "            \"hpo_id\": hp,\n",
    "            \"hpo_label\": None,\n",
    "            \"excerpt\": None,\n",
    "            \"onset_id\": None,\n",
    "            \"severity_id\": None,\n",
    "            \"frequency_id\": None\n",
    "        }\n",
    "        for hp in hp_ids\n",
    "    ]\n",
    "\n",
    "print(\"hello4.5.5\")  # print hello 4.5.5 as a sanity check\n",
    "\n",
    "patient_id      = list_patient_ids[0]\n",
    "clinical_text   = list_input_texts[0]\n",
    "\n",
    "hpo_terms = extract_hpo_terms_with_ollama(clinical_text)\n",
    "print(f\"Got {len(hpo_terms)} HPO terms for patient {patient_id}\")\n",
    "print(json.dumps(hpo_terms[:5], indent=2))\n",
    "print(\"hello mini 5\")\n"
   ],
   "id": "a94e57294bfac0c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p5)",
   "language": "python",
   "name": "p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
