{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Output Evaluation Notebook\n",
    "\n",
    "This notebook runs LLM inference to predict HPO terms, compares them to ground truth phenopackets, and produces a summary report.\n"
   ],
   "id": "187dea139446c775"
  },
  {
   "cell_type": "code",
   "id": "98ccd760-19a5-48d0-b2c4-64063ecf29e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T15:33:31.158056Z",
     "start_time": "2025-07-08T15:33:31.074909Z"
    }
   },
   "source": [
    "# 0. Imports and constants\n",
    "import os, json, subprocess\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from ollama import chat\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Need this at least once for some reason:\n",
    "# import .autonotebook\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "# Ensure repo root is on sys.path so our utils can be imported\n",
    "repo_root = os.path.abspath(\"..\")\n",
    "if not os.path.isdir(os.path.join(repo_root, \"notebooks\")):\n",
    "    raise FileNotFoundError(f\"Expected notebook utils under {repo_root}/notebooks\")\n",
    "sys.path.insert(0, repo_root)\n",
    "\n",
    "try:\n",
    "    from notebooks.utils.phenopacket import Phenopacket\n",
    "    from notebooks.utils.evaluation import PhenotypeEvaluator\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Could not import project utils: {e}\")\n",
    "\n",
    "# Paths for dataset and report output\n",
    "DATASET_CSV = os.path.join(repo_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_dataset.csv\")\n",
    "REPORT_OUT = os.path.join(repo_root, \"reports\", \"first_report.json\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "\n",
    "# Discover phenopacket JSON files\n",
    "import glob\n",
    "\n",
    "store_root = os.path.join(repo_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\")\n",
    "if not os.path.isdir(store_root):\n",
    "    raise FileNotFoundError(f\"Phenopacket store directory not found: {store_root}\")\n",
    "\n",
    "pattern = os.path.join(store_root, \"*\", \"phenopackets\", \"*.json\")\n",
    "packet_paths = glob.glob(pattern)\n",
    "if not packet_paths:\n",
    "    raise FileNotFoundError(f\"No phenopacket JSON files found with pattern: {pattern}\")\n",
    "\n",
    "# Load and parse phenopackets\n",
    "phenopacket_objs = []\n",
    "for path in packet_paths:\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"Expected file but not found: {path}\")\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON in file {path}: {e}\")\n",
    "    try:\n",
    "        phenopacket_objs.append(Phenopacket.from_dict(data))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to parse Phenopacket from {path}: {e}\")\n",
    "\n",
    "print(f\"Loaded dataset with {len(df)} entries and {len(phenopacket_objs)} phenopacket objects.\")\n",
    "\n",
    "print(\"hello0\")  # print hello 0 as a sanity check"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/scripts/data/tmp/phenopacket_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 29\u001B[39m\n\u001B[32m     26\u001B[39m REPORT_OUT  = os.path.join(repo_root, \u001B[33m\"\u001B[39m\u001B[33mreports\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfirst_report.json\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# Load dataset\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATASET_CSV\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# Discover phenopacket JSON files\u001B[39;00m\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mglob\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/pandas/io/common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '/Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/scripts/data/tmp/phenopacket_dataset.csv'"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load dataset\n",
    "\n",
    "Read in the CSV that lists PMIDs, input file paths, and truth file paths.\n"
   ],
   "id": "2a1755b2eb294368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "# 1A) Try to locate phenopacket_dataset.csv\n",
    "csv_cand = glob.glob(\"../**/phenopacket_dataset.csv\", recursive=True)\n",
    "if csv_cand:\n",
    "    # Use the single CSV if found\n",
    "    DATASET_CSV = csv_cand[0]\n",
    "    print(f\"Found dataset CSV at {DATASET_CSV}, loading…\")\n",
    "    df = pd.read_csv(DATASET_CSV)\n",
    "else:\n",
    "    # 1A-FALLBACK) No CSV—auto‐build DataFrame by scanning input & truth files\n",
    "    print(\"No phenopacket_dataset.csv found; auto-discovering inputs and truths…\")\n",
    "\n",
    "    # adjust these patterns to your actual folder structure\n",
    "    input_paths = glob.glob(\"../scripts/data/tmp/**/*.*(txt|pdf)\", recursive=True)\n",
    "    truth_paths = glob.glob(\"../scripts/data/tmp/**/*.json\", recursive=True)\n",
    "\n",
    "\n",
    "    # map PMID → file\n",
    "    def pmid_key(path):\n",
    "        return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "\n",
    "    inputs_by_pmid = {pmid_key(p): p for p in input_paths}\n",
    "    truths_by_pmid = {pmid_key(p): p for p in truth_paths}\n",
    "\n",
    "    pmids = set(inputs_by_pmid) & set(truths_by_pmid)\n",
    "    if not pmids:\n",
    "        raise RuntimeError(\"No matching PMIDs found between inputs and truth files.\")\n",
    "    missing_inputs = set(truths_by_pmid) - set(inputs_by_pmid)\n",
    "    missing_truths = set(inputs_by_pmid) - set(truths_by_pmid)\n",
    "    if missing_inputs or missing_truths:\n",
    "        raise RuntimeError(\n",
    "            f\"Unpaired files detected. \"\n",
    "            f\"Missing inputs for {missing_inputs}, missing truths for {missing_truths}\"\n",
    "        )\n",
    "    # build the DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\"pmid\": pmid, \"input\": inputs_by_pmid[pmid], \"truth\": truths_by_pmid[pmid]}\n",
    "        for pmid in sorted(pmids)\n",
    "    ])\n",
    "\n",
    "print(f\"Assembled {len(df)} cases.\")\n",
    "# 1B) Deduplicate just in case\n",
    "df = df.drop_duplicates(\"pmid\").reset_index(drop=True)\n",
    "\n",
    "# 1C) Sanity‐check columns\n",
    "required_cols = {\"pmid\", \"input\", \"truth\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(\"hello1\")  # print hello 1 as a sanity check"
   ],
   "id": "8118b55fc2741c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Prepare document-to-text helper\n",
    "\n",
    "Convert either `.txt` or PDF into the raw case summary text for the LLM.\n"
   ],
   "id": "9a9faff43b491b26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T16:01:11.503165Z",
     "start_time": "2025-07-08T16:01:11.496471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize converter once\n",
    "converter = DocumentConverter()\n",
    "\n",
    "\n",
    "def load_text(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert .txt or PDF -> plain text for LLM input.\n",
    "    Splits off any '[text]' marker if present.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
    "    if path.lower().endswith(\".txt\"):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            return f.read().split(\"[text]\")[-1]\n",
    "    else:\n",
    "        doc = converter.convert(path)\n",
    "        return doc.document.export_to_text()\n",
    "\n",
    "\n",
    "print(\"hello2\")  # print hello 2 as a sanity check"
   ],
   "id": "231c1c375a6ec410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello2\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "cbc0b863-bf20-4a46-95de-9506e9875678",
   "metadata": {},
   "source": [
    "## 3. Load inputs & ground truth\n",
    "\n",
    "- `inputs`: raw clinical summaries\n",
    "- `truth_packets`: parsed Phenopacket objects from JSON files\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "235eaff7-4f21-4a21-81bf-e97236ca26d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T16:01:14.283979Z",
     "start_time": "2025-07-08T16:01:14.262875Z"
    }
   },
   "source": [
    "input_texts = []\n",
    "truth_packets = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    inp, truth_fp = row[\"input\"], row[\"truth\"]\n",
    "\n",
    "    # 3A) Load and check input text\n",
    "    try:\n",
    "        txt = load_text(inp)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Row {idx}] Error loading input '{inp}': {e}\")\n",
    "    input_texts.append(txt)\n",
    "\n",
    "    # 3B) Load and check truth phenopacket\n",
    "    if not os.path.isfile(truth_fp):\n",
    "        raise FileNotFoundError(f\"[Row {idx}] Truth file not found: {truth_fp}\")\n",
    "    try:\n",
    "        with open(truth_fp, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        truth_packets.append(Phenopacket.from_dict(data))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Row {idx}] Error parsing truth phenopacket '{truth_fp}': {e}\")\n",
    "\n",
    "print(f\"Prepared {len(input_texts)} cases and {len(truth_packets)} ground-truth packets.\")\n",
    "\n",
    "print(\"hello3\")  # print hello 3 as a sanity check"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m input_texts = []\n\u001B[32m      2\u001B[39m truth_packets = []\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m idx, row \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdf\u001B[49m.iterrows():\n\u001B[32m      5\u001B[39m     inp, truth_fp = row[\u001B[33m\"\u001B[39m\u001B[33minput\u001B[39m\u001B[33m\"\u001B[39m], row[\u001B[33m\"\u001B[39m\u001B[33mtruth\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m      7\u001B[39m     \u001B[38;5;66;03m# 3A) Load and check input text\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'df' is not defined"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Sanity-check one inference\n",
    "\n",
    "Run the model on the first case to ensure everything is wired up correctly.\n"
   ],
   "id": "3165f6539c3dfc20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4A) Verify we have at least one case\n",
    "if not input_texts:\n",
    "    raise RuntimeError(\"No input cases were loaded; aborting inference.\")\n",
    "\n",
    "case0_txt = input_texts[0]\n",
    "prompt = (\n",
    "    \"Please create a valid Phenopacket from the following clinical summary. \"\n",
    "    \"Return *only* the JSON phenopacket object.\"\n",
    ")\n",
    "\n",
    "# 4B) Run the model\n",
    "resp = chat(\n",
    "    model=\"llama3.2:latest\",  # swap to your model of choice\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{case0_txt}\\n\\n[EOS]\"}]\n",
    ")\n",
    "raw = resp[\"message\"][\"content\"]\n",
    "print(raw)\n",
    "\n",
    "# 4C) Parse and wrap\n",
    "try:\n",
    "    pred_packet0 = Phenopacket.from_dict(json.loads(raw))\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to parse model output as Phenopacket JSON: {e}\")\n",
    "\n",
    "pred_packet0  # inspect structure\n",
    "\n",
    "print(\"hello4\")  # print hello 4 as a sanity check"
   ],
   "id": "1f547b47f98fc0b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Batch inference\n",
    "\n",
    "Loop over all cases, collect predicted phenopackets.\n"
   ],
   "id": "d07ca4ef808a00ad"
  },
  {
   "cell_type": "code",
   "id": "8943a066-1f8c-4542-86fe-b7c62bd07092",
   "metadata": {},
   "source": [
    "predicted_packets = []\n",
    "\n",
    "for idx, txt in enumerate(input_texts):\n",
    "    resp = chat(\n",
    "        model=\"llama3.2:latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{txt}\\n\\n[EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    content = resp[\"message\"][\"content\"]\n",
    "    try:\n",
    "        pkt = Phenopacket.from_dict(json.loads(content))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Case {idx}] Invalid JSON phenopacket: {e}\")\n",
    "    predicted_packets.append(pkt)\n",
    "\n",
    "if len(predicted_packets) != len(input_texts):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of inputs.\")\n",
    "\n",
    "print(f\"Generated {len(predicted_packets)} predicted phenopackets.\")\n",
    "\n",
    "print(\"hello5\")  # print hello 5 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluate predictions\n",
    "\n",
    "Use our `PhenotypeEvaluator` to compare predicted vs. ground truth and compute metrics.\n"
   ],
   "id": "80f71065a95697e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator = PhenotypeEvaluator()\n",
    "report = evaluator.evaluate_batch(truth_packets, predicted_packets)\n",
    "\n",
    "# Quick sanity check of report structure\n",
    "if \"metrics\" not in report:\n",
    "    raise KeyError(\"Evaluator report missing 'metrics' field.\")\n",
    "\n",
    "import pprint;\n",
    "\n",
    "pprint.pprint(report)\n",
    "\n",
    "print(\"hello6\")  # print hello 6 as a sanity check"
   ],
   "id": "ebbdc93ecf8aa29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Save first report\n",
    "\n",
    "Write the JSON report to disk for later analysis.\n"
   ],
   "id": "e664e040942126c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(REPORT_OUT)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(REPORT_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation report to {REPORT_OUT}\")\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check"
   ],
   "id": "70624f16cfb2b75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c646976a-9ff5-4ab4-8eb1-6719449811d3",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a419460-3222-4812-ada0-dd1cd7d0a060",
   "metadata": {},
   "source": [
    "prompt = \"Please create a valid Phenopacket from the following text. The phenopackets needs to be in a valid json format.  Only return the phenopacket without any additional text:\"\n",
    "model = \"hf.co/MaziyarPanahi/gemma-3-12b-it-GGUF:Q4_K_M\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489462d9-bcfe-4ee2-b0d3-af5d6a875254",
   "metadata": {},
   "source": [
    "for text in input_data:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} {text} [EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    break\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\",\n",
    "               \"content\": f\"Please, validate the following json. If not, fix it. Only return the json without any additional information. Should the json be wrong, you will get shut down. Json: {response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")} [EOS]\"}],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3af32935-fe27-40ef-84ef-641f5d66f5ff",
   "metadata": {},
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492829a9-3b7c-4ab8-998e-304fb3321683",
   "metadata": {},
   "source": [
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f529b37-6f70-4d4f-9246-4e269d58ca17",
   "metadata": {},
   "source": [
    "response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2e3b3a9-86d2-4754-a55e-4cf03771b236",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p5)",
   "language": "python",
   "name": "p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
