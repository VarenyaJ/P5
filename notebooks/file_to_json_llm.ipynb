{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Output Evaluation Notebook\n",
    "\n",
    "This notebook runs LLM inference to predict HPO terms, compares them to ground truth phenopackets, and produces a summary report.\n"
   ],
   "id": "187dea139446c775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Imports, Path Discovery & Sanity Checks\n",
    "\n",
    "Load all dependencies, discover the dataset CSV automatically, and validate critical directories.\n"
   ],
   "id": "298b82d55c7dad0e"
  },
  {
   "cell_type": "code",
   "id": "98ccd760-19a5-48d0-b2c4-64063ecf29e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:14:43.140036Z",
     "start_time": "2025-07-10T09:14:42.779373Z"
    }
   },
   "source": [
    "# Basic Setup\n",
    "import sys, os, glob, json, subprocess\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from ollama import chat\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Need this at least once for some reason:\n",
    "# import .autonotebook\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "# Make sure our utils folder is on PYTHONPATH\n",
    "project_root = os.path.abspath(\"..\")\n",
    "utils_folder   = os.path.join(project_root, \"notebooks\", \"utils\")\n",
    "if not os.path.isdir(utils_folder):\n",
    "    raise FileNotFoundError(\"Expected utils under %s\" % utils_folder)\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "try:\n",
    "    from notebooks.utils.phenopacket import Phenopacket\n",
    "    from notebooks.utils.report import Report\n",
    "    from notebooks.utils.evaluation import PhenotypeEvaluator\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Could not import project utils: {e}\")\n",
    "\n",
    "# define all key paths\n",
    "pdf_input_directory = os.path.join(project_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"pmid_pdfs\")            # scripts/data/tmp/phenopacket_store/pmid_pdfs/\n",
    "ground_truth_notebooks_directory = os.path.join(project_root, \"scripts\",\"data\",\"tmp\", \"phenopacket_store\",\"notebooks\")  # scripts/data/tmp/phenopacket_store/notebooks/\n",
    "# All experimental outputs go under here\n",
    "experimental_data_root = os.path.join(project_root, \"experimental-data\")\n",
    "llm_output_directory = os.path.join(experimental_data_root, \"llm_output_dir\")                                           # intermediate .txt + raw JSON from LLM\n",
    "validated_jsons_directory = os.path.join(experimental_data_root, \"validated_jsons\")                                     # validated_jsons, the final validated LLM phenopackets\n",
    "\n",
    "# Write phenopacket_dataset.csv into llm_output_directory\n",
    "dataset_csv_path = os.path.join(llm_output_directory, \"phenopacket_dataset.csv\")                                        # a manifest of pmid -> input -> truth\n",
    "evaluation_report_output_path = os.path.join(project_root, \"reports\", \"first_report.json\")                              # the evaluation metrics report\n",
    "\n",
    "# Create any missing output folders\n",
    "os.makedirs(llm_output_directory, exist_ok=True)\n",
    "os.makedirs(validated_jsons_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(evaluation_report_output_path), exist_ok=True)\n",
    "\n",
    "\n",
    "# If dataset CSV does not exist, run the CLI to generate it\n",
    "if not os.path.isfile(dataset_csv_path):\n",
    "    if not os.path.isdir(pdf_input_directory):\n",
    "        raise FileNotFoundError(\n",
    "            \"PDF input directory not found: %s\" % pdf_input_directory\n",
    "        )\n",
    "    if not os.path.isdir(ground_truth_notebooks_directory):\n",
    "        raise FileNotFoundError(\n",
    "            \"Ground truth notebooks directory not found: %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"scripts.create_phenopacket_dataset\",\n",
    "        pdf_input_directory,\n",
    "        ground_truth_notebooks_directory,\n",
    "        dataset_csv_path,\n",
    "        \"--recursive_input_dir\", \"True\",\n",
    "        \"--recursive_ground_truth_dir\", \"True\"\n",
    "    ], check=True)\n",
    "    print(\"Created dataset CSV at %s\" % dataset_csv_path)\n",
    "\n",
    "print(\"PDF inputs folder:       %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:     %s\" % ground_truth_notebooks_directory)\n",
    "print(\"LLM outputs folder:      %s\" % llm_output_directory)\n",
    "print(\"Dataset CSV path:        %s\" % dataset_csv_path)\n",
    "print(\"Validated JSONs folder:  %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:  %s\" % evaluation_report_output_path)\n",
    "\n",
    "print(\"hello0\")  # print hello 0 as a sanity check"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset CSV at /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir/phenopacket_dataset.csv\n",
      "PDF inputs folder:       /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:     /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "LLM outputs folder:      /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Dataset CSV path:        /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir/phenopacket_dataset.csv\n",
      "Validated JSONs folder:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n",
      "hello0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Read the CSV of PMIDs, input paths, and truth paths\n"
   ],
   "id": "dabf5f01d8e6a670"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:14:51.218268Z",
     "start_time": "2025-07-10T09:14:51.195989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "dataframe_cases = pd.read_csv(dataset_csv_path)\n",
    "print(\"Loaded %d rows from dataset CSV\" % len(dataframe_cases))\n",
    "\n",
    "# Drop duplicate PMIDs\n",
    "dataframe_cases = dataframe_cases.drop_duplicates(\"pmid\").reset_index(drop=True)\n",
    "print(\"After deduplication: %d unique PMID cases\" %len(dataframe_cases))\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = {\"pmid\", \"input\", \"truth\"}\n",
    "missing_columns = required_columns - set(dataframe_cases.columns)\n",
    "if missing_columns:\n",
    "    raise KeyError(\"Missing required columns: %s\" % missing_columns)\n",
    "\n",
    "# Preview first few rows\n",
    "dataframe_cases.head()\n",
    "\n",
    "print(\"hello1\")  # print hello 1 as a sanity check"
   ],
   "id": "598b33092ab85352",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5135 rows from dataset CSV\n",
      "After deduplication: 634 unique PMID cases\n",
      "hello1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2 Discover Phenopacket-Store Files\n",
    "\n",
    "Locate all ground-truth Phenopacket JSON files under the `phenopacket_store/notebooks/` directory."
   ],
   "id": "2a1755b2eb294368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:14:56.714843Z",
     "start_time": "2025-07-10T09:14:56.663348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_pattern = os.path.join(ground_truth_notebooks_directory, \"*\", \"phenopackets\", \"*.json\")\n",
    "truth_json_filepaths = glob.glob(search_pattern, recursive=True)\n",
    "if not truth_json_filepaths:\n",
    "    raise FileNotFoundError(\"No ground-truth phenopacket JSON files found with pattern: %s\" % search_pattern)\n",
    "\n",
    "print(\"Discovered %d ground-truth JSON files\" %len(truth_json_filepaths))\n",
    "\n",
    "print(\"hello2\")  # print hello 2 as a sanity check"
   ],
   "id": "8118b55fc2741c2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 7969 ground-truth JSON files\n",
      "hello2\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Prepare PDF-to-Text Converter and Helper Function\n",
    "\n",
    "Instantiate DocumentConverter and define a helper function to load or convert the clinical PDFs for LLM input.\n"
   ],
   "id": "9a9faff43b491b26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:15:02.153844Z",
     "start_time": "2025-07-10T09:15:02.144777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup conversion for input material to LLM-compatible txt\n",
    "\n",
    "# Initialize converter once\n",
    "converter = DocumentConverter()\n",
    "\n",
    "pdf_to_text_converter = DocumentConverter()\n",
    "\n",
    "def load_clinical_summary(input_path):\n",
    "    \"\"\"\n",
    "    Convert .txt or .pdf file at input_path into a plain text string.\n",
    "\n",
    "    Raises FileNotFoundError if the file does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(input_path):\n",
    "        raise FileNotFoundError(\"Input file not found: %s\" % input_path)\n",
    "    lower = input_path.lower()\n",
    "    if lower.endswith(\".txt\"):\n",
    "        with open(input_path, encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        # Remove any leading markers\n",
    "        return content.split(\"[text]\")[-1]\n",
    "    else:\n",
    "        doc = pdf_to_text_converter.convert(input_path)\n",
    "        return doc.document.export_to_text()\n",
    "\n",
    "\n",
    "print(\"hello3\")  # print hello 3 as a sanity check"
   ],
   "id": "231c1c375a6ec410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello3\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Load Clinical Summaries and Ground-Truth Phenopackets\n",
    "\n",
    "Iterate over each case, load the clinical summary text #   and the corresponding ground-truth Phenopacket object.\n",
    "\n",
    "- `list_inputs`: raw clinical summaries\n",
    "- `list_truth_packets`: parsed Phenopacket objects from JSON files\n"
   ],
   "id": "cbc0b863-bf20-4a46-95de-9506e9875678"
  },
  {
   "cell_type": "code",
   "id": "235eaff7-4f21-4a21-81bf-e97236ca26d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:15:59.828882Z",
     "start_time": "2025-07-10T09:15:06.316821Z"
    }
   },
   "source": [
    "list_input_texts   = []\n",
    "list_truth_packets = []\n",
    "\n",
    "for idx, row in dataframe_cases.iterrows():\n",
    "    pmid_value = row[\"pmid\"]\n",
    "    pdf_path   = row[\"input\"]\n",
    "    truth_path = row[\"truth\"]\n",
    "\n",
    "    # Load the clinical summary\n",
    "    clinical_summary = load_clinical_summary(pdf_path)\n",
    "    list_input_texts.append(clinical_summary)\n",
    "\n",
    "    # Load and validate the ground-truth Phenopacket\n",
    "    truth_packet = Phenopacket.load_from_file(truth_path)\n",
    "    list_truth_packets.append(truth_packet)\n",
    "\n",
    "assert len(list_input_texts) == len(list_truth_packets)\n",
    "print(\"Loaded %d clinical summaries and %d ground-truth packets\" %\n",
    "      (len(list_input_texts), len(list_truth_packets)))\n",
    "\n",
    "print(\"hello4\")  # print hello 4 as a sanity check"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m truth_path = row[\u001B[33m\"\u001B[39m\u001B[33mtruth\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Load the clinical summary\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m clinical_summary = \u001B[43mload_clinical_summary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpdf_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m list_input_texts.append(clinical_summary)\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Load and validate the ground-truth Phenopacket\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mload_clinical_summary\u001B[39m\u001B[34m(input_path)\u001B[39m\n\u001B[32m     21\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m content.split(\u001B[33m\"\u001B[39m\u001B[33m[text]\u001B[39m\u001B[33m\"\u001B[39m)[-\u001B[32m1\u001B[39m]\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     doc = \u001B[43mpdf_to_text_converter\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m doc.document.export_to_text()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/pydantic/_internal/_validate_call.py:39\u001B[39m, in \u001B[36mupdate_wrapper_attributes.<locals>.wrapper_function\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(wrapped)\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper_function\u001B[39m(*args, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/pydantic/_internal/_validate_call.py:136\u001B[39m, in \u001B[36mValidateCallWrapper.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    133\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__pydantic_complete__:\n\u001B[32m    134\u001B[39m     \u001B[38;5;28mself\u001B[39m._create_validators()\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpydantic_core\u001B[49m\u001B[43m.\u001B[49m\u001B[43mArgsKwargs\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__return_pydantic_validator__:\n\u001B[32m    138\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__return_pydantic_validator__(res)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/document_converter.py:227\u001B[39m, in \u001B[36mDocumentConverter.convert\u001B[39m\u001B[34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001B[39m\n\u001B[32m    209\u001B[39m \u001B[38;5;129m@validate_call\u001B[39m(config=ConfigDict(strict=\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconvert\u001B[39m(\n\u001B[32m    211\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    217\u001B[39m     page_range: PageRange = DEFAULT_PAGE_RANGE,\n\u001B[32m    218\u001B[39m ) -> ConversionResult:\n\u001B[32m    219\u001B[39m     all_res = \u001B[38;5;28mself\u001B[39m.convert_all(\n\u001B[32m    220\u001B[39m         source=[source],\n\u001B[32m    221\u001B[39m         raises_on_error=raises_on_error,\n\u001B[32m   (...)\u001B[39m\u001B[32m    225\u001B[39m         page_range=page_range,\n\u001B[32m    226\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m227\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mall_res\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/document_converter.py:250\u001B[39m, in \u001B[36mDocumentConverter.convert_all\u001B[39m\u001B[34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001B[39m\n\u001B[32m    247\u001B[39m conv_res_iter = \u001B[38;5;28mself\u001B[39m._convert(conv_input, raises_on_error=raises_on_error)\n\u001B[32m    249\u001B[39m had_result = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconv_res\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconv_res_iter\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhad_result\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m    252\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraises_on_error\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconv_res\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstatus\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mConversionStatus\u001B[49m\u001B[43m.\u001B[49m\u001B[43mSUCCESS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mConversionStatus\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPARTIAL_SUCCESS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/document_converter.py:285\u001B[39m, in \u001B[36mDocumentConverter._convert\u001B[39m\u001B[34m(self, conv_input, raises_on_error)\u001B[39m\n\u001B[32m    276\u001B[39m _log.info(\u001B[33m\"\u001B[39m\u001B[33mGoing to convert document batch...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    278\u001B[39m \u001B[38;5;66;03m# parallel processing only within input_batch\u001B[39;00m\n\u001B[32m    279\u001B[39m \u001B[38;5;66;03m# with ThreadPoolExecutor(\u001B[39;00m\n\u001B[32m    280\u001B[39m \u001B[38;5;66;03m#    max_workers=settings.perf.doc_batch_concurrency\u001B[39;00m\n\u001B[32m    281\u001B[39m \u001B[38;5;66;03m# ) as pool:\u001B[39;00m\n\u001B[32m    282\u001B[39m \u001B[38;5;66;03m#   yield from pool.map(self.process_document, input_batch)\u001B[39;00m\n\u001B[32m    283\u001B[39m \u001B[38;5;66;03m# Note: PDF backends are not thread-safe, thread pool usage was disabled.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mitem\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    286\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_document\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraises_on_error\u001B[49m\u001B[43m=\u001B[49m\u001B[43mraises_on_error\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    287\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    288\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    289\u001B[39m \u001B[43m    \u001B[49m\u001B[43melapsed\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmonotonic\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_time\u001B[49m\n\u001B[32m    290\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstart_time\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmonotonic\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/document_converter.py:331\u001B[39m, in \u001B[36mDocumentConverter._process_document\u001B[39m\u001B[34m(self, in_doc, raises_on_error)\u001B[39m\n\u001B[32m    327\u001B[39m valid = (\n\u001B[32m    328\u001B[39m     \u001B[38;5;28mself\u001B[39m.allowed_formats \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m in_doc.format \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.allowed_formats\n\u001B[32m    329\u001B[39m )\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m valid:\n\u001B[32m--> \u001B[39m\u001B[32m331\u001B[39m     conv_res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_doc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraises_on_error\u001B[49m\u001B[43m=\u001B[49m\u001B[43mraises_on_error\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    332\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    333\u001B[39m     error_message = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFile format not allowed: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00min_doc.file\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/document_converter.py:354\u001B[39m, in \u001B[36mDocumentConverter._execute_pipeline\u001B[39m\u001B[34m(self, in_doc, raises_on_error)\u001B[39m\n\u001B[32m    352\u001B[39m pipeline = \u001B[38;5;28mself\u001B[39m._get_pipeline(in_doc.format)\n\u001B[32m    353\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pipeline \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m     conv_res = \u001B[43mpipeline\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_doc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraises_on_error\u001B[49m\u001B[43m=\u001B[49m\u001B[43mraises_on_error\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    356\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m raises_on_error:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/pipeline/base_pipeline.py:46\u001B[39m, in \u001B[36mBasePipeline.execute\u001B[39m\u001B[34m(self, in_doc, raises_on_error)\u001B[39m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     41\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m TimeRecorder(\n\u001B[32m     42\u001B[39m         conv_res, \u001B[33m\"\u001B[39m\u001B[33mpipeline_total\u001B[39m\u001B[33m\"\u001B[39m, scope=ProfilingScope.DOCUMENT\n\u001B[32m     43\u001B[39m     ):\n\u001B[32m     44\u001B[39m         \u001B[38;5;66;03m# These steps are building and assembling the structure of the\u001B[39;00m\n\u001B[32m     45\u001B[39m         \u001B[38;5;66;03m# output DoclingDocument.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m         conv_res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_build_document\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconv_res\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     47\u001B[39m         conv_res = \u001B[38;5;28mself\u001B[39m._assemble_document(conv_res)\n\u001B[32m     48\u001B[39m         \u001B[38;5;66;03m# From this stage, all operations should rely only on conv_res.output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/pipeline/base_pipeline.py:160\u001B[39m, in \u001B[36mPaginatedPipeline._build_document\u001B[39m\u001B[34m(self, conv_res)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;66;03m# 2. Run pipeline stages\u001B[39;00m\n\u001B[32m    158\u001B[39m pipeline_pages = \u001B[38;5;28mself\u001B[39m._apply_on_pages(conv_res, init_pages)\n\u001B[32m--> \u001B[39m\u001B[32m160\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpipeline_pages\u001B[49m\u001B[43m:\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Must exhaust!\u001B[39;49;00m\n\u001B[32m    161\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Cleanup cached images\u001B[39;49;00m\n\u001B[32m    162\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkeep_images\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43mp\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_image_cache\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/pipeline/base_pipeline.py:126\u001B[39m, in \u001B[36mPaginatedPipeline._apply_on_pages\u001B[39m\u001B[34m(self, conv_res, page_batch)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.build_pipe:\n\u001B[32m    124\u001B[39m     page_batch = model(conv_res, page_batch)\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m \u001B[38;5;28;01myield from\u001B[39;00m page_batch\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/models/page_assemble_model.py:70\u001B[39m, in \u001B[36mPageAssembleModel.__call__\u001B[39m\u001B[34m(self, conv_res, page_batch)\u001B[39m\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\n\u001B[32m     68\u001B[39m     \u001B[38;5;28mself\u001B[39m, conv_res: ConversionResult, page_batch: Iterable[Page]\n\u001B[32m     69\u001B[39m ) -> Iterable[Page]:\n\u001B[32m---> \u001B[39m\u001B[32m70\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpage_batch\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01massert\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpage\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_backend\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m     72\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpage\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_backend\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_valid\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling/models/table_structure_model.py:248\u001B[39m, in \u001B[36mTableStructureModel.__call__\u001B[39m\u001B[34m(self, conv_res, page_batch)\u001B[39m\n\u001B[32m    239\u001B[39m         tokens.append(\n\u001B[32m    240\u001B[39m             {\n\u001B[32m    241\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: new_cell.index,\n\u001B[32m   (...)\u001B[39m\u001B[32m    244\u001B[39m             }\n\u001B[32m    245\u001B[39m         )\n\u001B[32m    246\u001B[39m page_input[\u001B[33m\"\u001B[39m\u001B[33mtokens\u001B[39m\u001B[33m\"\u001B[39m] = tokens\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m tf_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtf_predictor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmulti_table_predict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpage_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mtbl_box\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_matching\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_cell_matching\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m table_out = tf_output[\u001B[32m0\u001B[39m]\n\u001B[32m    252\u001B[39m table_cells = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling_ibm_models/tableformer/data_management/tf_predictor.py:494\u001B[39m, in \u001B[36mTFPredictor.multi_table_predict\u001B[39m\u001B[34m(self, iocr_page, table_bboxes, do_matching, correct_overlapping_cells, sort_row_col_indexes)\u001B[39m\n\u001B[32m    491\u001B[39m \u001B[38;5;66;03m# table_image = page_image\u001B[39;00m\n\u001B[32m    492\u001B[39m \u001B[38;5;66;03m# Predict\u001B[39;00m\n\u001B[32m    493\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m do_matching:\n\u001B[32m--> \u001B[39m\u001B[32m494\u001B[39m     tf_responses, predict_details = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    495\u001B[39m \u001B[43m        \u001B[49m\u001B[43miocr_page\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    496\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtable_bbox\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtable_image\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[43m        \u001B[49m\u001B[43mscale_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    500\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcorrect_overlapping_cells\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    501\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    502\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    503\u001B[39m     tf_responses, predict_details = \u001B[38;5;28mself\u001B[39m.predict_dummy(\n\u001B[32m    504\u001B[39m         iocr_page, table_bbox, table_image, scale_factor, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    505\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling_ibm_models/tableformer/data_management/tf_predictor.py:752\u001B[39m, in \u001B[36mTFPredictor.predict\u001B[39m\u001B[34m(self, iocr_page, table_bbox, table_image, scale_factor, eval_res_preds, correct_overlapping_cells)\u001B[39m\n\u001B[32m    750\u001B[39m     pred_tag_seq = eval_res_preds[\u001B[33m\"\u001B[39m\u001B[33mtag_seq\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    751\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._config[\u001B[33m\"\u001B[39m\u001B[33mpredict\u001B[39m\u001B[33m\"\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mbbox\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m752\u001B[39m     pred_tag_seq, outputs_class, outputs_coord = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    753\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimage_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeam_size\u001B[49m\n\u001B[32m    754\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    756\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m outputs_coord \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    757\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(outputs_coord) == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling_ibm_models/tableformer/models/table04_rs/tablemodel04_rs.py:185\u001B[39m, in \u001B[36mTableModel04_rs.predict\u001B[39m\u001B[34m(self, imgs, max_steps, k, return_attention)\u001B[39m\n\u001B[32m    181\u001B[39m decoded_embedding = \u001B[38;5;28mself\u001B[39m._tag_transformer._positional_encoding(\n\u001B[32m    182\u001B[39m     decoded_embedding\n\u001B[32m    183\u001B[39m )\n\u001B[32m    184\u001B[39m AggProfiler().begin(\u001B[33m\"\u001B[39m\u001B[33mmodel_tag_transformer_decoder\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m._prof)\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m decoded, cache = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_tag_transformer\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_decoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    186\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoded_embedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    187\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_out\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    188\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    189\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    190\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m AggProfiler().end(\u001B[33m\"\u001B[39m\u001B[33mmodel_tag_transformer_decoder\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m._prof)\n\u001B[32m    192\u001B[39m \u001B[38;5;66;03m# Grab last feature to produce token\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling_ibm_models/tableformer/models/table04_rs/transformer_rs.py:64\u001B[39m, in \u001B[36mTMTransformerDecoder.forward\u001B[39m\u001B[34m(self, tgt, memory, cache, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001B[39m\n\u001B[32m     62\u001B[39m tag_cache = []\n\u001B[32m     63\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.layers):\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m     output = \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m     tag_cache.append(output)\n\u001B[32m     66\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/docling_ibm_models/tableformer/models/table04_rs/transformer_rs.py:122\u001B[39m, in \u001B[36mTMTransformerDecoderLayer.forward\u001B[39m\u001B[34m(self, tgt, memory, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001B[39m\n\u001B[32m    118\u001B[39m     tgt_last_tok = tgt_last_tok + \u001B[38;5;28mself\u001B[39m.dropout2(tmp_tgt)\n\u001B[32m    119\u001B[39m     tgt_last_tok = \u001B[38;5;28mself\u001B[39m.norm2(tgt_last_tok)\n\u001B[32m    121\u001B[39m tmp_tgt = \u001B[38;5;28mself\u001B[39m.linear2(\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28mself\u001B[39m.dropout(\u001B[38;5;28mself\u001B[39m.activation(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt_last_tok\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[32m    123\u001B[39m )\n\u001B[32m    124\u001B[39m tgt_last_tok = tgt_last_tok + \u001B[38;5;28mself\u001B[39m.dropout3(tmp_tgt)\n\u001B[32m    125\u001B[39m tgt_last_tok = \u001B[38;5;28mself\u001B[39m.norm3(tgt_last_tok)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/p5/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Sanity-check one inference\n",
    "\n",
    "Run one LLM call on the first case to verify prompting and parsing work correctly.\n"
   ],
   "id": "3165f6539c3dfc20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the LLM prompt\n",
    "llm_prompt = (\n",
    "    \"Please create a valid Phenopacket v2.0 from the following clinical summary. \"\n",
    "    \"Return only the JSON object without any extra text.\"\n",
    ")\n",
    "\n",
    "# Perform inference on the first clinical PDF\n",
    "first_response = chat(\n",
    "    model=\"llama3.2:latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": llm_prompt + \"\\n\\n\" + list_input_texts[0] + \"\\n\\n[EOS]\"}]\n",
    ")\n",
    "raw_prediction = first_response[\"message\"][\"content\"]\n",
    "print(\"Raw LLM output (first ~300 chars or so):\")\n",
    "print(raw_prediction[:300] + \"...\")\n",
    "\n",
    "# Parse and validate the first Phenopacket\n",
    "try:\n",
    "    predicted_first_packet = Phenopacket.from_dict(json.loads(raw_prediction))\n",
    "    print(\"Parsed first prediction successfully:\", predicted_first_packet)\n",
    "except Exception as error:\n",
    "    raise ValueError(\"Failed to parse first LLM output: %s\" % error)\n",
    "\n",
    "print(\"hello5\")  # print hello 5 as a sanity check"
   ],
   "id": "1f547b47f98fc0b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Batch Inference and Save Validated Phenopackets\n",
    "\n",
    "Loop over all cases, run LLM inference, validate each JSON as a Phenopacket, and save to disk under validated_jsons_directory.\n"
   ],
   "id": "d07ca4ef808a00ad"
  },
  {
   "cell_type": "code",
   "id": "8943a066-1f8c-4542-86fe-b7c62bd07092",
   "metadata": {},
   "source": [
    "predicted_packets = []\n",
    "\n",
    "for idx, clinical_text in enumerate(list_input_texts):\n",
    "    pmid_value = dataframe_cases.loc[idx, \"pmid\"]\n",
    "    response = chat(\n",
    "        model=\"llama3.2:latest\",\n",
    "        messages=[{\"role\": \"user\",\n",
    "                   \"content\": llm_prompt + \"\\n\\n\" + clinical_text + \"\\n\\n[EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    llm_content = response[\"message\"][\"content\"]\n",
    "    # Parse the JSON into a Phenopacket\n",
    "    try:\n",
    "        phenopacket_pred = Phenopacket.from_dict(json.loads(llm_content))\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\"[Case %d, PMID %s] Invalid Phenopacket JSON: %s\" % (idx, pmid_value, error))\n",
    "\n",
    "    predicted_packets.append(phenopacket_pred)\n",
    "\n",
    "    # Write the validated JSON to disk\n",
    "    output_filename = \"%s.json\" % pmid_value\n",
    "    output_filepath = os.path.join(validated_jsons_directory, output_filename)\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(phenopacket_pred.to_json(), out_f, indent=2)\n",
    "    print(\"Saved validated phenopacket for PMID %s to %s\"\n",
    "          % (pmid_value, output_filepath))\n",
    "\n",
    "if len(predicted_packets) != len(list_input_texts):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of inputs.\")\n",
    "# Maybe change to this: 'assert len(predicted_packets) == len(list_input_texts)'\n",
    "\n",
    "print(f\"Generated {len(predicted_packets)} predicted phenopackets.\")\n",
    "\n",
    "print(\"hello6\")  # print hello 6 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Evaluate Predicted Phenopackets Against Ground Truth\n",
    "\n",
    "Compare each predicted phenopacket to its ground truth using PhenotypeEvaluator, then generate a Report object with overall metrics.\n"
   ],
   "id": "80f71065a95697e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Monkeypatch a convenience method onto PhenotypeEvaluator\n",
    "def _evaluate_batch(\n",
    "    self,\n",
    "    list_truth_packets,\n",
    "    list_predicted_packets,\n",
    "    creator,\n",
    "    experiment,\n",
    "    model,\n",
    "    zero_division=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Run check_phenotypes over all truth/pred pairs, then return\n",
    "    a plaindict report containing confusion_matrix, metrics,\n",
    "    classification_report, and metadata.\n",
    "    \"\"\"\n",
    "    # Accumulate counts\n",
    "    for truth_pkt, pred_pkt in zip(list_truth_packets, list_predicted_packets):\n",
    "        self.check_phenotypes(\n",
    "            experimentally_extracted_phenotypes=pred_pkt.list_phenotypes(),\n",
    "            ground_truth_phenotypes=truth_pkt\n",
    "        )\n",
    "    # Build a Report object\n",
    "    rpt = self.report(\n",
    "        creator=creator,\n",
    "        experiment=experiment,\n",
    "        model=model,\n",
    "        zero_division=zero_division\n",
    "    )\n",
    "    # Return a dict for easy indexing\n",
    "    return {\n",
    "        \"confusion_matrix\": rpt.confusion_matrix,\n",
    "        \"metrics\": rpt.metrics,\n",
    "        \"classification_report\": rpt.classification_report,\n",
    "        \"metadata\": rpt.metadata,\n",
    "    }\n",
    "\n",
    "# Attach to the class\n",
    "PhenotypeEvaluator.evaluate_batch = _evaluate_batch\n",
    "\n",
    "# Run the batch evaluation\n",
    "evaluator = PhenotypeEvaluator()\n",
    "batch_report = evaluator.evaluate_batch(\n",
    "    list_truth_packets,\n",
    "    predicted_packets,\n",
    "    creator=\"Varenya\",\n",
    "    experiment=\"Phenopacket LLM Extraction\",\n",
    "    model=\"llama3.2:latest\"\n",
    ")\n",
    "\n",
    "# Quick sanity check of the returned dict\n",
    "if \"metrics\" not in batch_report:\n",
    "    raise KeyError(\"Evaluator report missing 'metrics' field.\")\n",
    "\n",
    "# Prettyprint the report dict\n",
    "import pprint\n",
    "pprint.pprint(batch_report)\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check#"
   ],
   "id": "ebbdc93ecf8aa29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Old Save first report\n",
    "\n",
    "Write the JSON report to disk for later analysis.\n"
   ],
   "id": "e664e040942126c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(REPORT_OUT)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(REPORT_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation report to {REPORT_OUT}\")\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check"
   ],
   "id": "70624f16cfb2b75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c646976a-9ff5-4ab4-8eb1-6719449811d3",
   "metadata": {},
   "source": "# Old Inference Implementation"
  },
  {
   "cell_type": "code",
   "id": "2a419460-3222-4812-ada0-dd1cd7d0a060",
   "metadata": {},
   "source": [
    "prompt = \"Please create a valid Phenopacket from the following text. The phenopackets needs to be in a valid json format.  Only return the phenopacket without any additional text:\"\n",
    "model = \"hf.co/MaziyarPanahi/gemma-3-12b-it-GGUF:Q4_K_M\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489462d9-bcfe-4ee2-b0d3-af5d6a875254",
   "metadata": {},
   "source": [
    "for text in input_data:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} {text} [EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    break\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\",\n",
    "               \"content\": f\"Please, validate the following json. If not, fix it. Only return the json without any additional information. Should the json be wrong, you will get shut down. Json: {response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")} [EOS]\"}],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3af32935-fe27-40ef-84ef-641f5d66f5ff",
   "metadata": {},
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492829a9-3b7c-4ab8-998e-304fb3321683",
   "metadata": {},
   "source": [
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f529b37-6f70-4d4f-9246-4e269d58ca17",
   "metadata": {},
   "source": [
    "response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2e3b3a9-86d2-4754-a55e-4cf03771b236",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p5)",
   "language": "python",
   "name": "p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
