{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Output Evaluation Notebook\n",
    "\n",
    "This notebook runs LLM inference to predict HPO terms, compares them to ground truth phenopackets, and produces a summary report.\n"
   ],
   "id": "187dea139446c775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Imports, Path Discovery & Sanity Checks\n",
    "\n",
    "Load all dependencies, discover the dataset CSV automatically, and validate critical directories.\n"
   ],
   "id": "298b82d55c7dad0e"
  },
  {
   "cell_type": "code",
   "id": "98ccd760-19a5-48d0-b2c4-64063ecf29e2",
   "metadata": {},
   "source": [
    "# Basic Setup\n",
    "import sys, os, glob, json, subprocess\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from ollama import chat\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Need this at least once for some reason:\n",
    "# import .autonotebook\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "# Make sure our utils folder is on PYTHONPATH\n",
    "project_root = os.path.abspath(\"..\")\n",
    "utils_folder   = os.path.join(project_root, \"notebooks\", \"utils\")\n",
    "if not os.path.isdir(utils_folder):\n",
    "    raise FileNotFoundError(\"Expected utils under %s\" % utils_folder)\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "try:\n",
    "    from notebooks.utils.phenopacket import Phenopacket\n",
    "    from notebooks.utils.report import Report\n",
    "    from notebooks.utils.evaluation import PhenotypeEvaluator\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Could not import project utils: {e}\")\n",
    "\n",
    "# define all key paths\n",
    "pdf_input_directory = os.path.join(project_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"pmid_pdfs\")            # scripts/data/tmp/phenopacket_store/pmid_pdfs/\n",
    "ground_truth_notebooks_directory = os.path.join(project_root, \"scripts\",\"data\",\"tmp\", \"phenopacket_store\",\"notebooks\")  # scripts/data/tmp/phenopacket_store/notebooks/\n",
    "# All experimental outputs go under here\n",
    "experimental_data_root = os.path.join(project_root, \"experimental-data\")\n",
    "llm_output_directory = os.path.join(experimental_data_root, \"llm_output_dir\")                                           # intermediate .txt + raw JSON from LLM\n",
    "dataset_csv_path = os.path.join(experimental_data_root, \"phenopacket_dataset.csv\")                                      # phenopacket_dataset.csv, manifest of pmid -> input -> truth\n",
    "validated_jsons_directory = os.path.join(experimental_data_root, \"validated_jsons\")                                     # validated_jsons, the final validated LLM phenopackets\n",
    "evaluation_report_output_path = os.path.join(project_root, \"reports\", \"first_report.json\")                              # the evaluation metrics report\n",
    "\n",
    "# Create any missing output folders\n",
    "os.makedirs(llm_output_directory, exist_ok=True)\n",
    "os.makedirs(validated_jsons_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(evaluation_report_output_path), exist_ok=True)\n",
    "\n",
    "\n",
    "# If dataset CSV does not exist, run the CLI to generate it\n",
    "if not os.path.isfile(dataset_csv_path):\n",
    "    if not os.path.isdir(pdf_input_directory):\n",
    "        raise FileNotFoundError(\n",
    "            \"PDF input directory not found: %s\" % pdf_input_directory\n",
    "        )\n",
    "    if not os.path.isdir(ground_truth_notebooks_directory):\n",
    "        raise FileNotFoundError(\n",
    "            \"Ground truth notebooks directory not found: %s\"\n",
    "            % ground_truth_notebooks_directory\n",
    "        )\n",
    "\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"scripts.create_phenopacket_dataset\",\n",
    "        pdf_input_directory,\n",
    "        ground_truth_notebooks_directory,\n",
    "        llm_output_directory,\n",
    "        dataset_csv_path,\n",
    "        \"--recursive_input_dir\", \"True\",\n",
    "        \"--recursive_ground_truth_dir\", \"True\"\n",
    "    ], check=True)\n",
    "    print(\"Created dataset CSV at %s\" % dataset_csv_path)\n",
    "\n",
    "print(\"PDF inputs folder:       %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:     %s\" % ground_truth_notebooks_directory)\n",
    "print(\"LLM outputs folder:      %s\" % llm_output_directory)\n",
    "print(\"Dataset CSV path:        %s\" % dataset_csv_path)\n",
    "print(\"Validated JSONs folder:  %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:  %s\" % evaluation_report_output_path)\n",
    "\n",
    "print(\"hello0\")  # print hello 0 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Read the CSV of PMIDs, input paths, and truth paths\n"
   ],
   "id": "dabf5f01d8e6a670"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "dataframe_cases = pd.read_csv(dataset_csv_path)\n",
    "print(\"Loaded %d rows from dataset CSV\" % len(dataframe_cases))\n",
    "\n",
    "# Drop duplicate PMIDs\n",
    "dataframe_cases = dataframe_cases.drop_duplicates(\"pmid\").reset_index(drop=True)\n",
    "print(\"After deduplication: %d unique PMID cases\" %len(dataframe_cases))\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = {\"pmid\", \"input\", \"truth\"}\n",
    "missing_columns = required_columns - set(dataframe_cases.columns)\n",
    "if missing_columns:\n",
    "    raise KeyError(\"Missing required columns: %s\" % missing_columns)\n",
    "\n",
    "# Preview first few rows\n",
    "dataframe_cases.head()\n",
    "\n",
    "print(\"hello1\")  # print hello 1 as a sanity check"
   ],
   "id": "598b33092ab85352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2 Discover Phenopacket-Store Files\n",
    "\n",
    "Locate all ground-truth Phenopacket JSON files under the `phenopacket_store/notebooks/` directory."
   ],
   "id": "2a1755b2eb294368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_pattern = os.path.join(ground_truth_notebooks_directory, \"*\", \"phenopackets\", \"*.json\")\n",
    "truth_json_filepaths = glob.glob(search_pattern, recursive=True)\n",
    "if not truth_json_filepaths:\n",
    "    raise FileNotFoundError(\"No ground-truth phenopacket JSON files found with pattern: %s\" % search_pattern)\n",
    "\n",
    "print(\"Discovered %d ground-truth JSON files\" %len(truth_json_filepaths))\n",
    "\n",
    "print(\"hello2\")  # print hello 2 as a sanity check"
   ],
   "id": "8118b55fc2741c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Prepare PDF-to-Text Converter and Helper Function\n",
    "\n",
    "Instantiate DocumentConverter and define a helper function to load or convert the clinical PDFs for LLM input.\n"
   ],
   "id": "9a9faff43b491b26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup conversion for input material to LLM-compatible txt\n",
    "\n",
    "# Initialize converter once\n",
    "converter = DocumentConverter()\n",
    "\n",
    "pdf_to_text_converter = DocumentConverter()\n",
    "\n",
    "def load_clinical_summary(input_path):\n",
    "    \"\"\"\n",
    "    Convert .txt or .pdf file at input_path into a plain text string.\n",
    "\n",
    "    Raises FileNotFoundError if the file does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(input_path):\n",
    "        raise FileNotFoundError(\"Input file not found: %s\" % input_path)\n",
    "    lower = input_path.lower()\n",
    "    if lower.endswith(\".txt\"):\n",
    "        with open(input_path, encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        # Remove any leading markers\n",
    "        return content.split(\"[text]\")[-1]\n",
    "    else:\n",
    "        doc = pdf_to_text_converter.convert(input_path)\n",
    "        return doc.document.export_to_text()\n",
    "\n",
    "\n",
    "print(\"hello2\")  # print hello 3 as a sanity check"
   ],
   "id": "231c1c375a6ec410",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Load Clinical Summaries and Ground-Truth Phenopackets\n",
    "\n",
    "Iterate over each case, load the clinical summary text #   and the corresponding ground-truth Phenopacket object.\n",
    "\n",
    "- `list_inputs`: raw clinical summaries\n",
    "- `list_truth_packets`: parsed Phenopacket objects from JSON files\n"
   ],
   "id": "cbc0b863-bf20-4a46-95de-9506e9875678"
  },
  {
   "cell_type": "code",
   "id": "235eaff7-4f21-4a21-81bf-e97236ca26d1",
   "metadata": {},
   "source": [
    "list_input_texts   = []\n",
    "list_truth_packets = []\n",
    "\n",
    "for idx, row in dataframe_cases.iterrows():\n",
    "    pmid_value = row[\"pmid\"]\n",
    "    pdf_path   = row[\"input\"]\n",
    "    truth_path = row[\"truth\"]\n",
    "\n",
    "    # Load the clinical summary\n",
    "    clinical_summary = load_clinical_summary(pdf_path)\n",
    "    list_input_texts.append(clinical_summary)\n",
    "\n",
    "    # Load and validate the ground-truth Phenopacket\n",
    "    truth_packet = Phenopacket.load_from_file(truth_path)\n",
    "    list_truth_packets.append(truth_packet)\n",
    "\n",
    "assert len(list_input_texts) == len(list_truth_packets)\n",
    "print(\"Loaded %d clinical summaries and %d ground-truth packets\" %\n",
    "      (len(list_input_texts), len(list_truth_packets)))\n",
    "\n",
    "print(\"hello3\")  # print hello 3 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Sanity-check one inference\n",
    "\n",
    "Run the model on the first case to ensure everything is wired up correctly.\n"
   ],
   "id": "3165f6539c3dfc20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4A) Verify we have at least one case\n",
    "if not input_texts:\n",
    "    raise RuntimeError(\"No input cases were loaded; aborting inference.\")\n",
    "\n",
    "case0_txt = input_texts[0]\n",
    "prompt = (\n",
    "    \"Please create a valid Phenopacket v2.0 from the following clinical summary. \"\n",
    "    \"Return *only* the JSON phenopacket object, ensuring correct HPO terms, IDs, and definitions\"\n",
    ")\n",
    "\n",
    "# 4B) Run the model\n",
    "resp = chat(\n",
    "    model=\"llama3.2:latest\",  # swap to your model of choice\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{case0_txt}\\n\\n[EOS]\"}]\n",
    ")\n",
    "raw = resp[\"message\"][\"content\"]\n",
    "print(raw)\n",
    "\n",
    "# 4C) Parse and wrap\n",
    "try:\n",
    "    pred_packet0 = Phenopacket.from_dict(json.loads(raw))\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to parse model output as Phenopacket JSON: {e}\")\n",
    "\n",
    "pred_packet0  # inspect structure\n",
    "\n",
    "print(\"hello4\")  # print hello 4 as a sanity check"
   ],
   "id": "1f547b47f98fc0b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Batch inference\n",
    "\n",
    "Loop over all cases, collect predicted phenopackets.\n"
   ],
   "id": "d07ca4ef808a00ad"
  },
  {
   "cell_type": "code",
   "id": "8943a066-1f8c-4542-86fe-b7c62bd07092",
   "metadata": {},
   "source": [
    "predicted_packets = []\n",
    "\n",
    "for idx, txt in enumerate(input_texts):\n",
    "    resp = chat(\n",
    "        model=\"llama3.2:latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{txt}\\n\\n[EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    content = resp[\"message\"][\"content\"]\n",
    "    try:\n",
    "        pkt = Phenopacket.from_dict(json.loads(content))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Case {idx}] Invalid JSON phenopacket: {e}\")\n",
    "    predicted_packets.append(pkt)\n",
    "\n",
    "if len(predicted_packets) != len(input_texts):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of inputs.\")\n",
    "\n",
    "print(f\"Generated {len(predicted_packets)} predicted phenopackets.\")\n",
    "\n",
    "print(\"hello5\")  # print hello 5 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluate predictions\n",
    "\n",
    "Use our `PhenotypeEvaluator` to compare predicted vs. ground truth and compute metrics.\n"
   ],
   "id": "80f71065a95697e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator = PhenotypeEvaluator()\n",
    "report = evaluator.evaluate_batch(truth_packets, predicted_packets)\n",
    "\n",
    "# Quick sanity check of report structure\n",
    "if \"metrics\" not in report:\n",
    "    raise KeyError(\"Evaluator report missing 'metrics' field.\")\n",
    "\n",
    "import pprint; pprint.pprint(report)\n",
    "\n",
    "print(\"hello6\")  # print hello 6 as a sanity check"
   ],
   "id": "ebbdc93ecf8aa29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Save first report\n",
    "\n",
    "Write the JSON report to disk for later analysis.\n"
   ],
   "id": "e664e040942126c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(REPORT_OUT)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(REPORT_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation report to {REPORT_OUT}\")\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check"
   ],
   "id": "70624f16cfb2b75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c646976a-9ff5-4ab4-8eb1-6719449811d3",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a419460-3222-4812-ada0-dd1cd7d0a060",
   "metadata": {},
   "source": [
    "prompt = \"Please create a valid Phenopacket from the following text. The phenopackets needs to be in a valid json format.  Only return the phenopacket without any additional text:\"\n",
    "model = \"hf.co/MaziyarPanahi/gemma-3-12b-it-GGUF:Q4_K_M\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489462d9-bcfe-4ee2-b0d3-af5d6a875254",
   "metadata": {},
   "source": [
    "for text in input_data:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} {text} [EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    break\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\",\n",
    "               \"content\": f\"Please, validate the following json. If not, fix it. Only return the json without any additional information. Should the json be wrong, you will get shut down. Json: {response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")} [EOS]\"}],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3af32935-fe27-40ef-84ef-641f5d66f5ff",
   "metadata": {},
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492829a9-3b7c-4ab8-998e-304fb3321683",
   "metadata": {},
   "source": [
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f529b37-6f70-4d4f-9246-4e269d58ca17",
   "metadata": {},
   "source": [
    "response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2e3b3a9-86d2-4754-a55e-4cf03771b236",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p5)",
   "language": "python",
   "name": "p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
