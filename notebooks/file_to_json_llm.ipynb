{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Output Evaluation Notebook\n",
    "\n",
    "This notebook runs LLM inference to predict HPO terms, compares them to ground truth phenopackets, and produces a summary report.\n"
   ],
   "id": "187dea139446c775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Imports, Path Discovery & Sanity Checks\n",
    "\n",
    "Load all dependencies, discover the dataset CSV automatically, and validate critical directories.\n"
   ],
   "id": "298b82d55c7dad0e"
  },
  {
   "cell_type": "code",
   "id": "98ccd760-19a5-48d0-b2c4-64063ecf29e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T12:46:02.086509Z",
     "start_time": "2025-07-09T12:46:02.059869Z"
    }
   },
   "source": [
    "# 0. Basic Setup\n",
    "import sys, os, glob, json, subprocess\n",
    "import pandas as pd\n",
    "from ollama import chat\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Need this at least once for some reason:\n",
    "# import .autonotebook\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "# ensure project utils are importable\n",
    "repo_root = os.path.abspath(\"..\")\n",
    "if not os.path.isdir(os.path.join(repo_root, \"notebooks\")):\n",
    "    raise FileNotFoundError(f\"Expected notebook utils under {repo_root}/notebooks\")\n",
    "sys.path.insert(0, repo_root)\n",
    "try:\n",
    "    from notebooks.utils.phenopacket import Phenopacket\n",
    "    from notebooks.utils.evaluation import PhenotypeEvaluator\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Could not import project utils: {e}\")\n",
    "\n",
    "# define all key paths\n",
    "LLM_OUT_DIR = os.path.join(repo_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"llm_output_dir\")\n",
    "GT_NOTEBKS  = os.path.join(repo_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\")\n",
    "PMID_PDFS    = os.path.join(repo_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"pmid_pdfs\")\n",
    "CSV_PATH     = os.path.join(repo_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"phenopacket_dataset.csv\")\n",
    "REPORT_OUT   = os.path.join(repo_root, \"reports\", \"first_report.json\")\n",
    "os.makedirs(os.path.dirname(REPORT_OUT), exist_ok=True)\n",
    "\n",
    "\n",
    "# If CSV doesn't exist, auto-generate it\n",
    "if not os.path.isfile(CSV_PATH):\n",
    "    if not os.path.isdir(LLM_OUT_DIR):\n",
    "        raise FileNotFoundError(f\"LLM output directory not found: {LLM_OUT_DIR}\")\n",
    "    if not os.path.isdir(GT_NOTEBKS):\n",
    "        raise FileNotFoundError(f\"Ground-truth notebooks dir not found: {GT_NOTEBKS}\")\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"scripts.create_phenopacket_dataset\",\n",
    "        LLM_OUT_DIR,\n",
    "        GT_NOTEBKS,\n",
    "        CSV_PATH,\n",
    "        \"--recursive_input_dir\", \"True\",\n",
    "        \"--recursive_ground_truth_dir\", \"True\"\n",
    "    ], check=True)\n",
    "    print(f\"Created dataset CSV at {CSV_PATH}\")\n",
    "\n",
    "\n",
    "print(f\"Using DATASET_CSV = {DATASET_CSV}\")\n",
    "print(f\"Will write report to {REPORT_OUT}\")\n",
    "\n",
    "print(\"hello0\")  # print hello 0 as a sanity check"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "LLM output directory not found: /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/scripts/data/tmp/phenopacket_store/llm_output_dir",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.isfile(CSV_PATH):\n\u001B[32m     30\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.isdir(LLM_OUT_DIR):\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLLM output directory not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mLLM_OUT_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.isdir(GT_NOTEBKS):\n\u001B[32m     33\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGround-truth notebooks dir not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mGT_NOTEBKS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: LLM output directory not found: /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/scripts/data/tmp/phenopacket_store/llm_output_dir"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Read the CSV of PMIDs, input paths, and truth paths into a DataFrame, dedupe, and verify columns.\n"
   ],
   "id": "dabf5f01d8e6a670"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## 1. Load datasets\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "print(f\"Loaded {len(df)} rows from {DATASET_CSV}\")\n",
    "\n",
    "# Deduplicate and sanity-check columns\n",
    "df = df.drop_duplicates(\"pmid\").reset_index(drop=True)\n",
    "required = {\"pmid\", \"input\", \"truth\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns in dataset CSV: {missing}\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(\"hello1\")  # print hello 1 as a sanity check"
   ],
   "id": "598b33092ab85352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.5 Discover Phenopacket-Store Files\n",
    "\n",
    "Locate all JSON phenopacket files in the `phenopacket_store` folder.\n"
   ],
   "id": "2a1755b2eb294368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.5. Find the ground truth phenopackets\n",
    "store_root = os.path.join(repo_root, \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\")\n",
    "if not os.path.isdir(store_root):\n",
    "    raise FileNotFoundError(f\"Phenopacket store directory not found: {store_root}\")\n",
    "\n",
    "pattern = os.path.join(store_root, \"*\", \"phenopackets\", \"*.json\")\n",
    "packet_paths = glob.glob(pattern)\n",
    "if not packet_paths:\n",
    "    raise FileNotFoundError(f\"No phenopacket JSON files found with pattern: {pattern}\")\n",
    "\n",
    "store_packets = []\n",
    "for p in packet_paths:\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    store_packets.append(Phenopacket.from_dict(data))\n",
    "\n",
    "print(f\"Discovered {len(store_packets)} phenopackets in store.\")\n",
    "\n",
    "print(\"hello1.5\")  # print hello 1.5 as a sanity check"
   ],
   "id": "8118b55fc2741c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Prepare document-to-text helper\n",
    "\n",
    "Convert either `.txt` or PDF into the raw case summary text for the LLM.\n"
   ],
   "id": "9a9faff43b491b26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup conversion for input material to LLM-compatible txt\n",
    "\n",
    "# Initialize converter once\n",
    "converter = DocumentConverter()\n",
    "\n",
    "\n",
    "def load_text(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert .txt or PDF -> plain text for LLM input.\n",
    "    Splits off any '[text]' marker if present.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
    "    if path.lower().endswith(\".txt\"):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            return f.read().split(\"[text]\")[-1]\n",
    "    else:\n",
    "        doc = converter.convert(path)\n",
    "        return doc.document.export_to_text()\n",
    "\n",
    "\n",
    "print(\"hello2\")  # print hello 2 as a sanity check"
   ],
   "id": "231c1c375a6ec410",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cbc0b863-bf20-4a46-95de-9506e9875678",
   "metadata": {},
   "source": [
    "## 3. Load Inputs & Ground Truth\n",
    "\n",
    "Iterate the DataFrame, load each input summary and corresponding truth phenopacket.\n",
    "\n",
    "- `inputs`: raw clinical summaries\n",
    "- `truth_packets`: parsed Phenopacket objects from JSON files\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "235eaff7-4f21-4a21-81bf-e97236ca26d1",
   "metadata": {},
   "source": [
    "input_texts = []\n",
    "truth_packets = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    inp, truth_fp = row[\"input\"], row[\"truth\"]\n",
    "\n",
    "    # 3A) Load and check input text\n",
    "    try:\n",
    "        txt = load_text(inp)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Row {idx}] Error loading input '{inp}': {e}\")\n",
    "    input_texts.append(txt)\n",
    "\n",
    "    # 3B) Load and check truth phenopacket\n",
    "    if not os.path.isfile(truth_fp):\n",
    "        raise FileNotFoundError(f\"[Row {idx}] Truth file not found: {truth_fp}\")\n",
    "    try:\n",
    "        with open(truth_fp, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        truth_packets.append(Phenopacket.from_dict(data))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Row {idx}] Error parsing truth phenopacket '{truth_fp}': {e}\")\n",
    "\n",
    "print(f\"Prepared {len(input_texts)} cases and {len(truth_packets)} ground-truth packets.\")\n",
    "\n",
    "print(\"hello3\")  # print hello 3 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Sanity-check one inference\n",
    "\n",
    "Run the model on the first case to ensure everything is wired up correctly.\n"
   ],
   "id": "3165f6539c3dfc20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4A) Verify we have at least one case\n",
    "if not input_texts:\n",
    "    raise RuntimeError(\"No input cases were loaded; aborting inference.\")\n",
    "\n",
    "case0_txt = input_texts[0]\n",
    "prompt = (\n",
    "    \"Please create a valid Phenopacket v2.0 from the following clinical summary. \"\n",
    "    \"Return *only* the JSON phenopacket object, ensuring correct HPO terms, IDs, and definitions\"\n",
    ")\n",
    "\n",
    "# 4B) Run the model\n",
    "resp = chat(\n",
    "    model=\"llama3.2:latest\",  # swap to your model of choice\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{case0_txt}\\n\\n[EOS]\"}]\n",
    ")\n",
    "raw = resp[\"message\"][\"content\"]\n",
    "print(raw)\n",
    "\n",
    "# 4C) Parse and wrap\n",
    "try:\n",
    "    pred_packet0 = Phenopacket.from_dict(json.loads(raw))\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to parse model output as Phenopacket JSON: {e}\")\n",
    "\n",
    "pred_packet0  # inspect structure\n",
    "\n",
    "print(\"hello4\")  # print hello 4 as a sanity check"
   ],
   "id": "1f547b47f98fc0b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Batch inference\n",
    "\n",
    "Loop over all cases, collect predicted phenopackets.\n"
   ],
   "id": "d07ca4ef808a00ad"
  },
  {
   "cell_type": "code",
   "id": "8943a066-1f8c-4542-86fe-b7c62bd07092",
   "metadata": {},
   "source": [
    "predicted_packets = []\n",
    "\n",
    "for idx, txt in enumerate(input_texts):\n",
    "    resp = chat(\n",
    "        model=\"llama3.2:latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{txt}\\n\\n[EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    content = resp[\"message\"][\"content\"]\n",
    "    try:\n",
    "        pkt = Phenopacket.from_dict(json.loads(content))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Case {idx}] Invalid JSON phenopacket: {e}\")\n",
    "    predicted_packets.append(pkt)\n",
    "\n",
    "if len(predicted_packets) != len(input_texts):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of inputs.\")\n",
    "\n",
    "print(f\"Generated {len(predicted_packets)} predicted phenopackets.\")\n",
    "\n",
    "print(\"hello5\")  # print hello 5 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluate predictions\n",
    "\n",
    "Use our `PhenotypeEvaluator` to compare predicted vs. ground truth and compute metrics.\n"
   ],
   "id": "80f71065a95697e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator = PhenotypeEvaluator()\n",
    "report = evaluator.evaluate_batch(truth_packets, predicted_packets)\n",
    "\n",
    "# Quick sanity check of report structure\n",
    "if \"metrics\" not in report:\n",
    "    raise KeyError(\"Evaluator report missing 'metrics' field.\")\n",
    "\n",
    "import pprint; pprint.pprint(report)\n",
    "\n",
    "print(\"hello6\")  # print hello 6 as a sanity check"
   ],
   "id": "ebbdc93ecf8aa29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Save first report\n",
    "\n",
    "Write the JSON report to disk for later analysis.\n"
   ],
   "id": "e664e040942126c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(REPORT_OUT)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(REPORT_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation report to {REPORT_OUT}\")\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check"
   ],
   "id": "70624f16cfb2b75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c646976a-9ff5-4ab4-8eb1-6719449811d3",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a419460-3222-4812-ada0-dd1cd7d0a060",
   "metadata": {},
   "source": [
    "prompt = \"Please create a valid Phenopacket from the following text. The phenopackets needs to be in a valid json format.  Only return the phenopacket without any additional text:\"\n",
    "model = \"hf.co/MaziyarPanahi/gemma-3-12b-it-GGUF:Q4_K_M\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489462d9-bcfe-4ee2-b0d3-af5d6a875254",
   "metadata": {},
   "source": [
    "for text in input_data:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} {text} [EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    break\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\",\n",
    "               \"content\": f\"Please, validate the following json. If not, fix it. Only return the json without any additional information. Should the json be wrong, you will get shut down. Json: {response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")} [EOS]\"}],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3af32935-fe27-40ef-84ef-641f5d66f5ff",
   "metadata": {},
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492829a9-3b7c-4ab8-998e-304fb3321683",
   "metadata": {},
   "source": [
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f529b37-6f70-4d4f-9246-4e269d58ca17",
   "metadata": {},
   "source": [
    "response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2e3b3a9-86d2-4754-a55e-4cf03771b236",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p5)",
   "language": "python",
   "name": "p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
