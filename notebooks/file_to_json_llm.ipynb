{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Output Evaluation Notebook\n",
    "\n",
    "This notebook runs LLM inference to predict HPO terms, compares them to ground truth phenopackets, and produces a summary report.\n"
   ],
   "id": "187dea139446c775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 0. Imports, Path Discovery & Sanity Checks\n",
    "\n",
    "Load all dependencies, discover the dataset CSV automatically, and validate critical directories.\n"
   ],
   "id": "298b82d55c7dad0e"
  },
  {
   "cell_type": "code",
   "id": "98ccd760-19a5-48d0-b2c4-64063ecf29e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:37:47.494829Z",
     "start_time": "2025-07-21T09:36:56.973453Z"
    }
   },
   "source": [
    "# Basic Setup\n",
    "import sys, os, glob, json, subprocess, pickle\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from ollama import chat\n",
    "from docling.document_converter import DocumentConverter, ConversionError\n",
    "from pypdfium2._helpers.misc import PdfiumError\n",
    "import warnings\n",
    "from google.protobuf.json_format import ParseDict, ParseError\n",
    "from phenopackets import Phenopacket as ProtoPhenopacket\n",
    "import hashlib\n",
    "import json\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# Need this at least once for some reason:\n",
    "# import .autonotebook\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "# Make sure our utils folder is on PYTHONPATH\n",
    "project_root        = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "src_folder          = os.path.join(project_root, \"src\")\n",
    "utils_folder        = os.path.join(project_root, \"notebooks\", \"utils\")\n",
    "\n",
    "print(\"Project Start:       %s\" % project_root)\n",
    "print(\"Source Folder:       %s\" % src_folder)\n",
    "print(\"Utilities Folder:    %s\" % utils_folder)\n",
    "\n",
    "for path in (src_folder, utils_folder):\n",
    "    if not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"Expected folder on PYTHOPATH : {path}\")\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "print(\"PYTHONPATH patched with:\", src_folder, utils_folder)\n",
    "\n",
    "try:\n",
    "    from phenopacket import Phenopacket\n",
    "    from report import Report\n",
    "    from evaluation import PhenotypeEvaluator\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Could not import project utils: {e}\")\n",
    "\n",
    "# define all key paths\n",
    "pdf_input_directory                 = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"pmid_pdfs\")            # scripts/data/tmp/phenopacket_store/pmid_pdfs/\n",
    "ground_truth_notebooks_directory    = os.path.join(src_folder, \"P5\", \"scripts\", \"data\",\"tmp\", \"phenopacket_store\",\"notebooks\")              # scripts/data/tmp/phenopacket_store/notebooks/\n",
    "dataset_csv_path                    = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\")\n",
    "\n",
    "# All experimental outputs go under here\n",
    "experimental_data_root              = os.path.join(project_root, \"experimental-data\")\n",
    "llm_output_directory                = os.path.join(experimental_data_root, \"llm_output_dir\")                                                # intermediate .txt + raw JSON from LLM\n",
    "validated_jsons_directory           = os.path.join(experimental_data_root, \"validated_jsons\")                                               # validated_jsons, the final validated LLM phenopackets\n",
    "evaluation_report_output_path       = os.path.join(project_root, \"reports\", \"first_report.json\")                                            # the evaluation metrics report\n",
    "\n",
    "# Create any missing output folders\n",
    "os.makedirs(pdf_input_directory, exist_ok=True)\n",
    "os.makedirs(ground_truth_notebooks_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(dataset_csv_path), exist_ok=True)\n",
    "os.makedirs(llm_output_directory, exist_ok=True)\n",
    "os.makedirs(validated_jsons_directory, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(evaluation_report_output_path), exist_ok=True)\n",
    "\n",
    "# Create the PMIDs pickle file path\n",
    "pmid_pkl_path = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"pmids.pkl\")\n",
    "\n",
    "# TODO: Figure out why deleting the `ground_truth_notebooks_directory` after creating it works. Maybe because git doesn't let me just overwrite a directory with a clone request\n",
    "# Before the git pull operation\n",
    "import shutil\n",
    "\n",
    "# Clean up existing directory if it exists\n",
    "target_dir = os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\")\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "\n",
    "# 1. Now run the git pull to clone the \"phenopacket-store\" GitHub repo into scripts/data/tmp/phenopacket_store\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.pull_git_files\",\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\"),\n",
    "    \"https://github.com/monarch-initiative/phenopacket-store.git\",\n",
    "    \"notebooks\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 1 Complete, Produced %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "# 2. Scan the just-pulled notebooks for PMID_##### files\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.create_pmid_pkl\",\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"phenopacket_store\", \"notebooks\"),\n",
    "    os.path.join(src_folder, \"P5\", \"scripts\", \"data\", \"tmp\", \"pmids.pkl\"),\n",
    "    \"--recursive_dir_search\",\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 2 Complete\")\n",
    "\n",
    "# 3. Download *all* PDFs for those PMIDs (0 = unlimited)\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"P5.scripts.pmid_downloader\", pmid_pkl_path, pdf_input_directory, \"10\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Stage 3 Complete\")\n",
    "\n",
    "# 4. Finally, build THE CSV mapping PDFs to the ground-truth JSONs\n",
    "if not os.path.isfile(dataset_csv_path):\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"P5.scripts.create_phenopacket_dataset\",\n",
    "        pdf_input_directory,\n",
    "        ground_truth_notebooks_directory,\n",
    "        dataset_csv_path,\n",
    "        \"--recursive_ground_truth_dir\", \"True\"\n",
    "    ], check=True)\n",
    "    print(f\"Created dataset CSV at {dataset_csv_path}\")\n",
    "\n",
    "    print(\"Stage 4 Complete\")\n",
    "\n",
    "    if not os.path.isdir(pdf_input_directory):\n",
    "        raise FileNotFoundError(\"PDF input directory not found: %s\" % pdf_input_directory)\n",
    "    if not os.path.isdir(ground_truth_notebooks_directory):\n",
    "        raise FileNotFoundError(\"Ground truth notebooks directory not found: %s\" % ground_truth_notebooks_directory)\n",
    "\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)\n",
    "\n",
    "print(\"hello0\")  # print hello 0 as a sanity check"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Start:       /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5\n",
      "Source Folder:       /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src\n",
      "Utilities Folder:    /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/notebooks/utils\n",
      "PYTHONPATH patched with: /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/notebooks/utils\n",
      "Stage 1 Complete, Produced /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "1234 PMIDs found within /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Stage 2 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_33791923:   0%|                         | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PMCID found for PMID_33791923.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_31449058:  10%|█▋               | 1/10 [00:00<00:06,  1.47it/s]An error occurred when downloading PMID_31449058 = PMC_6763221: No connection adapters were found for 'data:,'\n",
      "Processing PMID_9012405:  20%|███▌              | 2/10 [00:07<00:33,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_9012405 was successfully downloaded. PMCID=1712398.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_34612517:  30%|█████            | 3/10 [00:13<00:36,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PMCID found for PMID_34612517.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_35308163:  40%|██████▊          | 4/10 [00:14<00:20,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_35308163 was successfully downloaded. PMCID=8931749.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_28966547:  50%|████████▌        | 5/10 [00:20<00:22,  4.49s/it]An error occurred when downloading PMID_28966547 = PMC_5610811: No connection adapters were found for 'data:,'\n",
      "Processing PMID_36233161:  60%|██████████▏      | 6/10 [00:26<00:19,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_36233161 was successfully downloaded. PMCID=9570320.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_12949970:  70%|███████████▉     | 7/10 [00:33<00:16,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PMCID found for PMID_12949970.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_24190800:  80%|█████████████▌   | 8/10 [00:34<00:08,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_24190800 was successfully downloaded. PMCID=4110326.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMID_24498630:  90%|███████████████▎ | 9/10 [00:40<00:04,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PDF for PMID_24498630 was successfully downloaded. PMCID=3907912.\n",
      "Stage 3 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing of 10 PMIDs complete. 5 PDFs successfully downloaded.: 100%|█| 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset CSV at /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Stage 4 Complete\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n",
      "hello0\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1. Load Dataset\n",
    "\n",
    "Read the CSV of PMIDs, input paths, and truth paths\n"
   ],
   "id": "dabf5f01d8e6a670"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:38:00.095601Z",
     "start_time": "2025-07-21T09:38:00.085951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "dataframe_cases = pd.read_csv(dataset_csv_path)\n",
    "print(f\"Loaded {len(dataframe_cases)} rows from dataset CSV\")\n",
    "# Load cases & deduplicate PMIDs, with start/end counts\n",
    "orig_count = len(dataframe_cases)\n",
    "print(f\"Before deduplication: {orig_count} total cases\")\n",
    "\n",
    "# Debug: verify that every `input` path actually exists\n",
    "print(\"Checking existence of input PDFs:\")\n",
    "for pdf_path in dataframe_cases[\"input\"]:\n",
    "    status = \"FOUND\" if os.path.isfile(pdf_path) else \"MISSING\"\n",
    "    print(f\"  • {pdf_path}: {status}\")\n",
    "\n",
    "# Drop duplicate PMIDs\n",
    "dataframe_cases = dataframe_cases.drop_duplicates(subset=\"pmid\", keep=\"first\").reset_index(drop=True) # This may be too aggressive and I need to check if this is a good approach\n",
    "removed = orig_count - len(dataframe_cases)\n",
    "print(f\"{removed} duplicates removed (now {len(dataframe_cases)} unique PMIDs)\")\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = {\"pmid\", \"input\", \"truth\"}\n",
    "missing_columns = required_columns - set(dataframe_cases.columns)\n",
    "if missing_columns:\n",
    "    raise KeyError(\"Missing required columns: %s\" % missing_columns)\n",
    "\n",
    "# Preview first few rows\n",
    "dataframe_cases.head()\n",
    "\n",
    "print(\"hello1\")  # print hello 1 as a sanity check\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)"
   ],
   "id": "598b33092ab85352",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19 rows from dataset CSV\n",
      "Before deduplication: 19 total cases\n",
      "Checking existence of input PDFs:\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24190800.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_36233161.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_36233161.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_36233161.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_9012405.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_9012405.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24498630.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24498630.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_24498630.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "  • /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_35308163.pdf: FOUND\n",
      "14 duplicates removed (now 5 unique PMIDs)\n",
      "hello1\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2 Discover Phenopacket-Store Files\n",
    "\n",
    "Locate all ground-truth Phenopacket JSON files under the `phenopacket_store/notebooks/` directory."
   ],
   "id": "2a1755b2eb294368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:38:04.765773Z",
     "start_time": "2025-07-21T09:38:04.704080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare phenopacket JSON cache, skip logic, and cap how many PDFs to process\n",
    "\n",
    "# Set number of cases to process in this run\n",
    "MAX_LOAD = 10\n",
    "\n",
    "# Persist cache of already-parsed phenopacket cases\n",
    "phenopacket_cache_path = os.path.join(experimental_data_root, \"phenopacket_cache.json\")\n",
    "\n",
    "# Load existing cache is present\n",
    "try:\n",
    "    with open(phenopacket_cache_path, \"r\") as f:\n",
    "        phenopacket_cache: Dict[str, Any] = json.load(f)\n",
    "        print(f\"Loaded phenopacket cache with {len(phenopacket_cache)} entries\")\n",
    "except FileNotFoundError:\n",
    "    phenopacket_cache = {}\n",
    "    print(\"Initialized empty phenopacket cache\")\n",
    "\n",
    "# Lookup parsed PDFs - by MD5 hash?\n",
    "# TODO: figure out how to use SHA-256 or SHA-512\n",
    "existing_hashes = {info[\"hash\"] for info in phenopacket_cache.values()}\n",
    "\n",
    "# Prepare to record skipped cases due to failures, as well as the reasons why\n",
    "skipped_pdfs: List[Dict[str, str]] = []\n",
    "loaded_count = 0\n",
    "\n",
    "# Finding all ground-truth phenopacket JSON files\n",
    "search_pattern = os.path.join(ground_truth_notebooks_directory, \"*\", \"phenopackets\", \"*.json\")\n",
    "truth_json_filepaths = glob.glob(search_pattern, recursive=True)\n",
    "if not truth_json_filepaths:\n",
    "    raise FileNotFoundError(\"No ground-truth phenopacket JSON files found with pattern: %s\" % search_pattern)\n",
    "\n",
    "print(\"Discovered %d ground-truth JSON files\" %len(truth_json_filepaths))\n",
    "\n",
    "print(\"hello2\")  # print hello 2 as a sanity check\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)"
   ],
   "id": "8118b55fc2741c2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty phenopacket cache\n",
      "Discovered 7969 ground-truth JSON files\n",
      "hello2\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3. Subset PMIDs, Prepare PDF-to-Text Converter, and Helper Functions\n",
    "\n",
    "- Randomly pick N unique PMIDs from the CSV (from Step 1) to keep runs fast and reproducible.\n",
    "- Instantiate DocumentConverter and define a helper function to load or convert the clinical PDFs for LLM input.\n"
   ],
   "id": "9a9faff43b491b26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:38:07.047481Z",
     "start_time": "2025-07-21T09:38:07.038915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload the deduplicated CSV from Step 1\n",
    "full_df = pd.read_csv(dataset_csv_path).drop_duplicates(subset=\"pmid\").reset_index(drop=True)\n",
    "\n",
    "# Choose how many cases to sample\n",
    "N = 10\n",
    "subset_df = full_df.sample(n=N, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Sampling {N} PMIDs:\", subset_df[\"pmid\"].tolist())\n",
    "\n",
    "\n",
    "# Setup conversion for input material to LLM-compatible txt now\n",
    "\n",
    "# Initialize converter once\n",
    "pdf_to_text_converter = DocumentConverter()\n",
    "\n",
    "# Path to persistent cache of PDF text\n",
    "text_cache_path = os.path.join(experimental_data_root, \"text_cache.pkl\")\n",
    "# Load or initialize cache\n",
    "if os.path.exists(text_cache_path):\n",
    "    with open(text_cache_path, \"rb\") as f:\n",
    "        _text_cache: Dict[str, str] = pickle.load(f)\n",
    "    print(f\"Loaded text cache with {len(_text_cache)} entries\")\n",
    "else:\n",
    "    _text_cache: Dict[str, str] = {}\n",
    "    print(\"Initialized empty text cache\")\n",
    "\n",
    "def load_clinical_pdf(input_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert .txt or .pdf file at \"input_path\" into a plain text string.\n",
    "    Use the in-memory cache first; write new text back to the cache only when the cache is explicitly saved at the end of the pipeline.\n",
    "    Raise FileNotFoundError or ConversionError if the file does not exist.\n",
    "    \"\"\"\n",
    "    # Return cache if it exists in memory\n",
    "    if input_path in _text_cache:\n",
    "        return _text_cache[input_path]\n",
    "\n",
    "    # Ensure the files exists before we continue\n",
    "    if not os.path.isfile(input_path):\n",
    "        raise FileNotFoundError(\"Input file not found: %s\" % input_path)\n",
    "\n",
    "    # If it's already plain text, read and strip any header\n",
    "    if input_path.lower().endswith(\".txt\"):\n",
    "        content = open(input_path, encoding=\"utf-8\").read()\n",
    "        # Remove any leading markers\n",
    "        return content.split(\"[text]\")[-1]\n",
    "    else:\n",
    "        try:\n",
    "            # Convert PDF to text and handle conversion failures\n",
    "            doc = pdf_to_text_converter.convert(input_path)\n",
    "            content = doc.document.export_to_text()\n",
    "        except ConversionError as e:\n",
    "            raise ConversionError(f\"Could not convert {os.path.basename(input_path)}: {e}\")\n",
    "\n",
    "\n",
    "    # Save new text in memory and write updated cache to disk later\n",
    "    _text_cache[input_path] = content\n",
    "    return content\n",
    "\n",
    "print(\"hello3\")  # print hello 3 as a sanity check\n",
    "print(\"PDF inputs folder:               %s\" % pdf_input_directory)\n",
    "print(\"Ground truth folder:             %s\" % ground_truth_notebooks_directory)\n",
    "print(\"Dataset CSV path:                %s\" % dataset_csv_path)\n",
    "print(\"Experimentally generated files:  %s\" % experimental_data_root)\n",
    "print(\"LLM outputs folder:              %s\" % llm_output_directory)\n",
    "print(\"Validated JSONs folder:          %s\" % validated_jsons_directory)\n",
    "print(\"Evaluation report path:          %s\" % evaluation_report_output_path)"
   ],
   "id": "231c1c375a6ec410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty text cache\n",
      "hello3\n",
      "PDF inputs folder:               /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/pmid_pdfs\n",
      "Ground truth folder:             /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/phenopacket_store/notebooks\n",
      "Dataset CSV path:                /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/src/P5/scripts/data/tmp/PMID_PDF_Phenopacket_list_in_phenopacket_store.csv\n",
      "Experimentally generated files:  /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data\n",
      "LLM outputs folder:              /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/llm_output_dir\n",
      "Validated JSONs folder:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/validated_jsons\n",
      "Evaluation report path:          /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/reports/first_report.json\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4. Load Clinical PDFs and Ground-Truth Phenopackets\n",
    "\n",
    "Iterate over each case, load the clinical PDF text and the corresponding ground-truth Phenopacket object.\n",
    "\n",
    "- `list_inputs_texts`: raw clinical PDFs\n",
    "- `list_truth_packets`: parsed Phenopacket objects from JSON files\n",
    "- `list_patient_ids`: PMID patient identifiers\n"
   ],
   "id": "cbc0b863-bf20-4a46-95de-9506e9875678"
  },
  {
   "cell_type": "code",
   "id": "235eaff7-4f21-4a21-81bf-e97236ca26d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:39:49.390763Z",
     "start_time": "2025-07-21T09:38:09.773020Z"
    }
   },
   "source": [
    "# Load Clinical PDFs and Ground-Truth Phenopackets (with full skip‐tracking & deduplication)\n",
    "# Watch for any \"bad\" PDFs like \"scripts/data/tmp/phenopacket_store/pmid_pdfs/PMID_32325141.pdf\", which made `load_clinical_pdf` raise a ``ConversionError causing the whole cell to fail\n",
    "\n",
    "def compute_pdf_hash(pdf_path: str) -> str:\n",
    "    \"\"\"Compute MD5 hash of a PDF file.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def save_caches():\n",
    "    \"\"\"Persist both phenopacket_cache (JSON) and text_cache (pickle).\"\"\"\n",
    "    with open(phenopacket_cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(phenopacket_cache, f, indent=2)\n",
    "    with open(text_cache_path, \"wb\") as f:\n",
    "        pickle.dump(_text_cache, f)\n",
    "\n",
    "# Now iterate over rows, should lookup only once\n",
    "list_input_texts    = []\n",
    "list_truth_packets  = []\n",
    "list_patient_ids    = []\n",
    "\n",
    "\n",
    "for case in dataframe_cases.itertuples(index=False):\n",
    "    pmid_value = case.pmid\n",
    "    pdf_path   = case.input\n",
    "    truth_path = case.truth\n",
    "\n",
    "    # 1) Compute MD5 of the PDF to detect reruns\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            pdf_bytes = f.read()\n",
    "        pdf_hash = hashlib.md5(pdf_bytes).hexdigest()\n",
    "    except Exception as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"pdf\": pdf_path, \"reason\": f\"I/O error computing hash: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # 2) Skip if already processed\n",
    "    if pdf_hash in existing_hashes:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"pdf\": pdf_path, \"reason\": \"already in cache\"})\n",
    "        continue\n",
    "\n",
    "    # 3) Enforce MAX_LOAD\n",
    "    if loaded_count >= MAX_LOAD:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"pdf\": pdf_path, \"reason\": \"max load reached\"})\n",
    "        continue\n",
    "\n",
    "    # 4) Convert PDF to text\n",
    "    try:\n",
    "        clinical_text = load_clinical_pdf(pdf_path)\n",
    "    except (ConversionError, PdfiumError) as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"pdf\": pdf_path, \"reason\": f\"conversion error: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # 5) Load raw JSON and validate with ignore_unknown_fields\n",
    "    try:\n",
    "        raw_true_packet = json.load(open(truth_path, \"r\", encoding=\"utf-8\"))\n",
    "        proto = ProtoPhenopacket()\n",
    "        ParseDict(raw_true_packet, proto, ignore_unknown_fields=True)\n",
    "    except (ParseError, json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"truth\": truth_path, \"reason\": f\"schema parse error: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # 6) Wrap in util Phenopacket to ensure phenotypicFeatures exists\n",
    "    try:\n",
    "        truth_packet = Phenopacket(raw_true_packet)\n",
    "    except InvalidPhenopacketError as e:\n",
    "        skipped_pdfs.append({\"pmid\": pmid_value, \"truth\": truth_path, \"reason\": f\"phenopacket invalid: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # 7) Record in cache & lists\n",
    "    phenopacket_cache[str(pmid_value)] = {\"hash\": pdf_hash, \"json\": raw_true_packet}\n",
    "    existing_hashes.add(pdf_hash)\n",
    "    save_caches()\n",
    "\n",
    "    list_input_texts.append(clinical_text)\n",
    "    list_truth_packets.append(truth_packet)\n",
    "    list_patient_ids.append(truth_packet.to_json()[\"subject\"][\"id\"])\n",
    "\n",
    "    loaded_count += 1\n",
    "    print(f\"Loaded {loaded_count} new cases, skipped {len(skipped_pdfs)} so far\")\n",
    "\n",
    "if not list_input_texts:\n",
    "        raise RuntimeError(\"No clinical texts were loaded, please check that the dataset CSV `input` paths match files in `pdf_input_directory`\")\n",
    "\n",
    "assert len(list_input_texts) == len(list_truth_packets) == len(list_patient_ids)\n",
    "print(\"Loaded %d clinical texts and %d ground-truth packets for %d unique patients\" % (len(list_input_texts), len(list_truth_packets), len(list_patient_ids)))\n",
    "\n",
    "print(\"hello4\")  # print hello 4 as a sanity check"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 new cases, skipped 0 so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 new cases, skipped 0 so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 new cases, skipped 0 so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/varenya/miniconda3/envs/p5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 new cases, skipped 0 so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 new cases, skipped 0 so far\n",
      "Loaded 5 clinical texts and 5 ground-truth packets for 5 unique patients\n",
      "hello4\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4.5. Define LLM Prompts\n",
    "\n",
    "Create one prompt for just HPO terms and another one for the full phenopacket extraction\n"
   ],
   "id": "5b74dfdf270a5845"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:00:46.096186Z",
     "start_time": "2025-07-21T10:00:46.089110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save cache to disk\n",
    "with open(text_cache_path,\"wb\") as f:\n",
    "    pickle.dump(_text_cache, f)\n",
    "print(f\"Saved text cache with {len(_text_cache)} entries to {text_cache_path}\")\n",
    "\n",
    "# 1) Prompt for just HPO labels\n",
    "hpo_prompt = (\n",
    "    \"You are a clinical NLP engine specialized in biomedical ontologies. Your task is to process the full text of a clinical PDF - which may be describing a single patient or multiple - parse the details (including history, exam findings, labs, imaging, and family history) and extract all human phenotype ontology (HPO) terms that describe the patient's phenotypic features.\"\n",
    "    \"Instructions:\"\n",
    "    \"1. Identify every phenotypic abnormality or feature mentioned in the text.\"\n",
    "    \"2. For each feature, map it to the correct HPO identifier (e.g. 'HP:0001250'), label (e.g. 'Seizure'), and descriptor value (e.g. 'A seizure is an intermittent abnormality of nervous system physiology characterized by a transient occurrence of signs and/or symptoms due to abnormal excessive or synchronous neuronal activity in the brain.').\"\n",
    "    \"3. Capture relevant qualifiers when present:\"\n",
    "        \"- Onset: map to HPO onset terms (e.g. 'HP:0011463' for 'Childhood onset').\"\n",
    "        \"- Severity: map to HPO severity terms (e.g. 'HP:0012829' for 'Profound').\"\n",
    "        \"- Temporal pattern: include if specified (e.g. 'HP:0031796' for 'Recurrent', map to HPO frequency terms if available).\"\n",
    "    \"4. For each term, include the exact text excerpt where it appears.\"\n",
    "    \"5. Output exclusively a JSON array. Each element must be an object with the following fields:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'hpo_id': 'HP:000____',\"\n",
    "        \"'hpo_label': 'Term label',\"\n",
    "        \"'excerpt': 'Exact text from the PDF',\"\n",
    "        \"'onset_id': 'HP:0XXXXX or null',\"\n",
    "        \"'severity_id': 'HP:0XXXXX or null',\"\n",
    "        \"'frequency_id': 'HP:0XXXXX or null'\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"Do not include any explanatory text, only the JSON array.\"\n",
    "\n",
    "    \"Your output **MUST** be exactly a JSON array/object and nothing else.\"\n",
    "\n",
    "    \"If you cannot comply, output exactly: {'error': 'cannot extract JSON'}\"\n",
    "    )\n",
    "\n",
    "# 2) Prompt for full phenopacket\n",
    "full_pp_prompt = (\n",
    "    \"You are a biomedical data curation assistant. Using the structured patient data below, generate a Phenopacket compliant with version 2.0 of the GA4GH Phenopacket schema. Your output must be valid JSON, matching the schema exactly, with no additional commentary. Here are the minimum expected output criteria:\"\n",
    "\n",
    "    \"Inputs:\"\n",
    "    \"patient_id: '{{patient_id}}'\"\n",
    "    \"sex: '{{sex}}'              // 'male' or 'female'\"\n",
    "    \"age_years: {{age_in_years}} // integer\"\n",
    "    \"v  ital_status: '{{vital_status}}' // 'alive' or 'deceased'\"\n",
    "    \"phenotypic_features: {{phenotypic_features_json}} // JSON array from the HPO extraction prompt\"\n",
    "    \"diseases: {{diseases_json}}         // optional, array of disease objects with MONDO or OMIM IDs\"\n",
    "    \"measurements: {{measurements_json}} // optional, array of quantitative trait measurements\"\n",
    "    \"metadata: {\"\n",
    "        \"'created_by': '{{your_name_or_tool}}',\"\n",
    "        \"'created_on': '{{YYYY-MM-DD}}'\"\n",
    "    \"}\"\n",
    "\n",
    "    \"Requirements:\"\n",
    "    \"Top-level fields:\"\n",
    "    \"'id': patient_id\"\n",
    "    \"'subject': object with:\"\n",
    "        \"'id': patient_id\"\n",
    "        \"'sex': { 'id': 'PATO:0000383' or 'PATO:0000384', 'label': sex }\"\n",
    "        \"'ageAtLastEncounter': { 'age': { 'years': age_years } }\"\n",
    "        \"'vitalStatus': { 'value': vital_status }\"\n",
    "        \"'phenotypicFeatures': use the phenotypic_features input; for each feature, map:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'type': { 'id': hpo_id, 'label': hpo_label },\"\n",
    "        \"'negated': false,\"\n",
    "        \"'onset': { 'term': { 'id': onset_id, 'label': (look up label) } },\"\n",
    "        \"'severity': { 'term': { 'id': severity_id, 'label': (look up label) } },\"\n",
    "        \"'frequency': { 'term': { 'id': frequency_id, 'label': (look up label) } }\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"Include 'diseases' and 'measurements' only if provided, following the GA4GH schema.\"\n",
    "    \"'metadata' must include:\"\n",
    "    \"```json\"\n",
    "    \"{\"\n",
    "        \"'phenopacketSchemaVersion': '2.0.0',\"\n",
    "        \"'created': '{{YYYY-MM-DD}}',\"\n",
    "        \"'createdBy': '{{your_name_or_tool}}'\"\n",
    "    \"}\"\n",
    "    \"```\"\n",
    "    \"'Do not add any extra fields. Output must be purely the JSON object.'\"\n",
    "\n",
    "    \"Do not include any explanatory text, only the JSON array.\"\n",
    "\n",
    "    \"Your output **MUST** be exactly a JSON array/object and nothing else.\"\n",
    "\n",
    "    \"If you cannot comply, output exactly: {'error': 'cannot extract JSON'}\"\n",
    ")"
   ],
   "id": "589eb0d8cac4d8ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved text cache with 5 entries to /Users/varenya/Desktop/Illini+Uni/Personalized_Genomic_Medicine-Precision_Genomics_Laboratory/PreGen/P5/experimental-data/text_cache.pkl\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5. Sanity-check one inference\n",
    "\n",
    "Run one LLM call on the first case to verify prompting and parsing work correctly.\n"
   ],
   "id": "3165f6539c3dfc20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:01:00.249060Z",
     "start_time": "2025-07-21T10:00:50.677637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pick out the first patient/example\n",
    "patient_id      = list_patient_ids[0]\n",
    "clinical_text   = list_input_texts[0]\n",
    "# truth_packet    = list_truth_packets[0]\n",
    "\n",
    "# 1) Inference: ask for *only* the JSON array of HPO term objects for the first clinical PDF\n",
    "hpo_response = chat(\n",
    "    model=\"llama3.2:latest\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            hpo_prompt\n",
    "            + \"\\n\\n*Extract only the JSON array of **valid** HPO term objects for patient `{}`,*\\n\"\n",
    "            + \"*each with `hpo_id`, `hpo_label` and `descriptor`, and nothing else.*\\n\\n\"\n",
    "            .format(patient_id)\n",
    "            + clinical_text\n",
    "            + \"\\n\\n[EOS]\"\n",
    "        )\n",
    "    }],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n",
    "\n",
    "# 2) Grab the raw string\n",
    "raw_hpo_output = hpo_response[\"message\"][\"content\"]\n",
    "print(\"Raw LLM output (truncated to the first ~300 chars or so):\")\n",
    "print(raw_hpo_output[:300], \"...\\n\")\n",
    "\n",
    "# 3) Slice out the JSON array\n",
    "start_idx       = raw_hpo_output.find(\"[\")\n",
    "end_idx         = raw_hpo_output.rfind(\"]\")\n",
    "if start_idx < 0 or end_idx < 0:\n",
    "    raise RuntimeError(f\"Could not locate a JSON array in HPO output for patient {patient_id}:\\n{raw_hpo_output}\")\n",
    "hpo_json_array  = raw_hpo_output[start_idx : end_idx+1]\n",
    "\n",
    "# Parse and validate\n",
    "try:\n",
    "    hpo_terms = Phenopacket(json.loads(raw_hpo_output))\n",
    "    print(f\"Parsed {len(hpo_terms)} HPO term(s) for patient {patient_id}\")\n",
    "except JSONDecodeError as error:\n",
    "    raise ValueError(f\"Failed to parse HPO JSON for patient {patient_id}: {e}\\nRaw output:\\n{raw_hpo_output}\")\n",
    "\n",
    "print(\"hello5\")  # print hello 5 as a sanity check"
   ],
   "id": "1f547b47f98fc0b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLM output (truncated to the first ~300 chars or so):\n",
      "Here is the summary of the article:\n",
      "\n",
      "**Title:** Adult-Onset Mitochondrial Neurohepatopathy Caused by MPV17 Mutations: A Case Report and Literature Review\n",
      "\n",
      "**Summary:** Mitochondrial DNA depletion syndromes can present in adulthood with multisystem involvement, including amenorrhea and megaloblastic  ...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not locate a JSON array in HPO output for patient Case Report:\nHere is the summary of the article:\n\n**Title:** Adult-Onset Mitochondrial Neurohepatopathy Caused by MPV17 Mutations: A Case Report and Literature Review\n\n**Summary:** Mitochondrial DNA depletion syndromes can present in adulthood with multisystem involvement, including amenorrhea and megaloblastic anemia. The authors report a case of a female patient who presented with adult-onset hepatocerebral mitochondrial DNA depletion syndrome caused by a mutation in the MPV17 gene.\n\n**Main Points:**\n\n1. Mitochondrial DNA depletion syndromes can present in adulthood with multisystem involvement.\n2. MPV17 mutations are associated with infantile hepatic mitochondrial DNA depletion syndrome, but adult-onset forms have been reported.\n3. The authors report a case of a female patient who presented with amenorrhea and megaloblastic anemia due to an MPV17 mutation.\n4. The patient's liver and nervous system involvement were consistent with hepatocerebral mitochondrial DNA depletion syndrome.\n5. The diagnosis was confirmed by genetic testing for the MPV17 mutation.\n\n**Clinical Significance:** This case highlights the importance of considering mitochondrial DNA depletion syndromes in adults presenting with unexplained amenorrhea, megaloblastic anemia, or liver disease. Further research is needed to understand the molecular mechanisms underlying adult-onset MPV17 mutations and their clinical implications.\n\n**Conflict of Interest:** The authors disclose no conflicts of interest.\n\n**Ethics:** The study was conducted in accordance with the Helsinki Declaration of 1975 and institutional review board guidelines.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[100]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     30\u001B[39m end_idx         = raw_hpo_output.rfind(\u001B[33m\"\u001B[39m\u001B[33m]\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m start_idx < \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m end_idx < \u001B[32m0\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCould not locate a JSON array in HPO output for patient \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpatient_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mraw_hpo_output\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     33\u001B[39m hpo_json_array  = raw_hpo_output[start_idx : end_idx+\u001B[32m1\u001B[39m]\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# Parse and validate\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Could not locate a JSON array in HPO output for patient Case Report:\nHere is the summary of the article:\n\n**Title:** Adult-Onset Mitochondrial Neurohepatopathy Caused by MPV17 Mutations: A Case Report and Literature Review\n\n**Summary:** Mitochondrial DNA depletion syndromes can present in adulthood with multisystem involvement, including amenorrhea and megaloblastic anemia. The authors report a case of a female patient who presented with adult-onset hepatocerebral mitochondrial DNA depletion syndrome caused by a mutation in the MPV17 gene.\n\n**Main Points:**\n\n1. Mitochondrial DNA depletion syndromes can present in adulthood with multisystem involvement.\n2. MPV17 mutations are associated with infantile hepatic mitochondrial DNA depletion syndrome, but adult-onset forms have been reported.\n3. The authors report a case of a female patient who presented with amenorrhea and megaloblastic anemia due to an MPV17 mutation.\n4. The patient's liver and nervous system involvement were consistent with hepatocerebral mitochondrial DNA depletion syndrome.\n5. The diagnosis was confirmed by genetic testing for the MPV17 mutation.\n\n**Clinical Significance:** This case highlights the importance of considering mitochondrial DNA depletion syndromes in adults presenting with unexplained amenorrhea, megaloblastic anemia, or liver disease. Further research is needed to understand the molecular mechanisms underlying adult-onset MPV17 mutations and their clinical implications.\n\n**Conflict of Interest:** The authors disclose no conflicts of interest.\n\n**Ethics:** The study was conducted in accordance with the Helsinki Declaration of 1975 and institutional review board guidelines."
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6. Batch Inference and Save Validated Phenopackets\n",
    "\n",
    "Loop over all cases, run LLM inference, validate each JSON as a Phenopacket, and save to disk under validated_jsons_directory.\n"
   ],
   "id": "d07ca4ef808a00ad"
  },
  {
   "cell_type": "code",
   "id": "8943a066-1f8c-4542-86fe-b7c62bd07092",
   "metadata": {},
   "source": [
    "predicted_packets: List[Phenopacket] = []\n",
    "\n",
    "# Which patient are we targeting?\n",
    "for idx, clinical_text in enumerate(list_input_texts):\n",
    "    pmid_value = dataframe_cases.loc[idx, \"pmid\"]\n",
    "    patient_id = list_patient_ids[idx]\n",
    "    # Prompt the LLM to extract only that patient's HPO terms\n",
    "    content = (hpo_prompt + f\"\\n\\n*Extract only the HPO terms for patient* `{patient_id}` *in this clinical PDF.*\\n\\n\" + clinical_text + \"\\n\\n[EOS]\")\n",
    "    response = chat(model=\"llama3.2:latest\", messages=[{\"role\": \"user\", \"content\": content}], options={\"--hidethinking\": True})\n",
    "    llm_content = response[\"message\"][\"content\"].splitlines()\n",
    "    # Parse the JSON into a Phenopacket\n",
    "    try:\n",
    "        phenopacket_pred = Phenopacket(json.loads(\"\\n\".join(llm_content)))\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\"[Case %d, PMID %s] Invalid Phenopacket JSON: %s\" % (idx, pmid_value, error))\n",
    "\n",
    "    predicted_packets.append(phenopacket_pred)\n",
    "\n",
    "    # Write the predicted JSON to disk\n",
    "    output_filename = f\"{pmid_value}_{patient_id}.json\"\n",
    "    output_filepath = os.path.join(validated_jsons_directory, output_filename)\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(phenopacket_pred.to_json(), out_f, indent=2)\n",
    "    print(\"Saved predicted phenopacket for PMID/Patient %s/%s to %s\"\n",
    "          % (pmid_value, patient_id, output_filepath))\n",
    "\n",
    "if len(predicted_packets) != len(list_input_texts):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of inputs.\")\n",
    "# Maybe change to this: 'assert len(predicted_packets) == len(list_input_texts), \"Mismatch predictions vs inputs\"'\n",
    "\n",
    "print(f\"Generated {len(predicted_packets)} predicted phenopackets.\")\n",
    "\n",
    "print(\"hello6\")  # print hello 6 as a sanity check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7. Evaluate Predicted Phenopackets Against Ground Truth\n",
    "\n",
    "Compare each predicted phenopacket to its ground truth using PhenotypeEvaluator, then generate a Report object with overall metrics.\n"
   ],
   "id": "80f71065a95697e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Monkey‐patch a convenience method onto PhenotypeEvaluator\n",
    "def _evaluate_batch(\n",
    "    self,\n",
    "    list_truth_packets,\n",
    "    list_predicted_packets,\n",
    "    creator,\n",
    "    experiment,\n",
    "    model,\n",
    "    zero_division=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Run check_phenotypes over all truth/pred pairs, then return\n",
    "    a plain‐dict report containing confusion_matrix, metrics,\n",
    "    classification_report, and metadata.\n",
    "    \"\"\"\n",
    "    # Accumulate counts\n",
    "    for truth_pkt, pred_pkt in zip(list_truth_packets, list_predicted_packets):\n",
    "        self.check_phenotypes(\n",
    "            experimentally_extracted_phenotypes=pred_pkt.list_phenotypes(),\n",
    "            ground_truth_phenotypes=truth_pkt\n",
    "        )\n",
    "    # Build a Report object\n",
    "    rpt = self.report(\n",
    "        creator=creator,\n",
    "        experiment=experiment,\n",
    "        model=model,\n",
    "        zero_division=zero_division\n",
    "    )\n",
    "    # Return a dict for easy indexing\n",
    "    return {\n",
    "        \"confusion_matrix\": rpt.confusion_matrix,\n",
    "        \"metrics\": rpt.metrics,\n",
    "        \"classification_report\": rpt.classification_report,\n",
    "        \"metadata\": rpt.metadata,\n",
    "    }\n",
    "\n",
    "# Attach to the class\n",
    "PhenotypeEvaluator.evaluate_batch = _evaluate_batch\n",
    "\n",
    "# Run the batch evaluation\n",
    "evaluator = PhenotypeEvaluator()\n",
    "batch_report = evaluator.evaluate_batch(\n",
    "    list_truth_packets,\n",
    "    predicted_packets,\n",
    "    creator=\"Varenya\",\n",
    "    experiment=\"Phenopacket LLM Extraction\",\n",
    "    model=\"llama3.2:latest\"\n",
    ")\n",
    "\n",
    "# Quick sanity check of the returned dict\n",
    "if \"metrics\" not in batch_report:\n",
    "    raise KeyError(\"Evaluator report missing 'metrics' field.\")\n",
    "\n",
    "# Pretty‐print the report dict\n",
    "import pprint\n",
    "pprint.pprint(batch_report)\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check#"
   ],
   "id": "ebbdc93ecf8aa29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Old Save first report\n",
    "\n",
    "Write the JSON report to disk for later analysis.\n"
   ],
   "id": "e664e040942126c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(evaluation_report_output_path)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(evaluation_report_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(batch_report, f, indent=2)\n",
    "\n",
    "print(f\"Saved evaluation report to {evaluation_report_output_path}\")\n",
    "\n",
    "print(\"hello7\")  # print hello 7 as a sanity check"
   ],
   "id": "70624f16cfb2b75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c646976a-9ff5-4ab4-8eb1-6719449811d3",
   "metadata": {},
   "source": "# Old Inference Implementation"
  },
  {
   "cell_type": "code",
   "id": "2a419460-3222-4812-ada0-dd1cd7d0a060",
   "metadata": {},
   "source": [
    "prompt = \"Please create a valid Phenopacket from the following text. The phenopackets needs to be in a valid json format.  Only return the phenopacket without any additional text:\"\n",
    "model = \"hf.co/MaziyarPanahi/gemma-3-12b-it-GGUF:Q4_K_M\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489462d9-bcfe-4ee2-b0d3-af5d6a875254",
   "metadata": {},
   "source": [
    "for text in input_data:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} {text} [EOS]\"}],\n",
    "        options={\"--hidethinking\": True}\n",
    "    )\n",
    "    break\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\",\n",
    "               \"content\": f\"Please, validate the following json. If not, fix it. Only return the json without any additional information. Should the json be wrong, you will get shut down. Json: {response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")} [EOS]\"}],\n",
    "    options={\"--hidethinking\": True}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3af32935-fe27-40ef-84ef-641f5d66f5ff",
   "metadata": {},
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492829a9-3b7c-4ab8-998e-304fb3321683",
   "metadata": {},
   "source": [
    "JSON(response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f529b37-6f70-4d4f-9246-4e269d58ca17",
   "metadata": {},
   "source": [
    "response[\"message\"][\"content\"].split(\"</think>\")[-1].replace(\"```json\", \"\").replace(\"```\", \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2e3b3a9-86d2-4754-a55e-4cf03771b236",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p5)",
   "language": "python",
   "name": "p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
